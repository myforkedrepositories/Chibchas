{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Institulac "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "import re\n",
    "import time\n",
    "\n",
    "#requirements\n",
    "import json\n",
    "import pandas as pd\n",
    "import helium as h\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "\n",
    "pd.set_option(\"max_rows\",100)\n",
    "#pd.set_option(\"display.max_columns\",100)\n",
    "pd.set_option(\"max_colwidth\",1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DICT CAT-PRODS-TAB\n",
    "with open('dict_tables.json') as file_json:\n",
    "    dict_tables=json.loads(file_json.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(df,cod_gr):\n",
    "    \n",
    "    info= {\n",
    "        'Nombre_Grupo' : df['Nombre Grupo'].dropna().iloc[0],\n",
    "\n",
    "        'Nombre_Lider' : df['Nombre Líder'].dropna().iloc[0],\n",
    "\n",
    "        'CCRG Grupo'  : cod_gr\n",
    "    }\n",
    "    \n",
    "    dfi = pd.DataFrame(info, index=[0])\n",
    "  \n",
    "    \n",
    "    return dfi\n",
    "\n",
    "# extra headers by products\n",
    "DBEH = {\n",
    "    \n",
    "    'INFO_GROUP': 'TABLE',\n",
    "    'MEMBERS':['Identificación', 'Nacionalidad', 'Tiene afiliación con UdeA', 'Si no tiene afiliación UdeA diligencie el nombre de la Institución','Nro. Horas de dedicación semanales que avala el Coordinador de grupo'], # 2\n",
    "       \n",
    "    'NC_P': {'ART_IMP_P': {'ART_P_TABLE':['URL','DOI','Si no tiene URL o DOI agregue una evidencia en el repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "             'ART_ELE_P': {'ART_E_P_TABLE':['URL','DOI','Si no tiene URL o DOI agregue una evidencia en el repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "             'LIB_P':     {'LIB_P_TABLE':['Proyecto de investigación del cual se derivó el libro (Código-Título)','Financiador(es) del proyecto del cual se derivó el libro', 'Financiador(es) de la publicación','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "             'CAP_LIB_P': {'CAP_LIB_P_TABLE':['Proyecto de investigación del cual se derivó el libro que contiene el capítulo (Código-Título)','Financiador del proyecto del cual se derivó el libro que contiene el capítulo','Financiador de la publicación','Autores','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "             'NOT_CIE_P': {'NOT_CIE_P_TABLE':['URL','DOI','Si no tiene URL o DOI genere una evidencia en el repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "             'PAT_P':     {'PAT_P_TABLE':['Autores', 'Examen de fondo favorable','Examen preliminar internacional favorable','Adjunta opiniones escritas de la bUsqueda internacional','Contrato de explotación','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']}, #  1 2 3 -1\n",
    "             'PRD_INV_ART_P': {'PAAD_P_TABLE':['Autores','Tiene certificado institucional de la obra','Tiene certificado de la entidad que convoca al evento en el que participa','Tiene certificado de la entidad que convoca al premio en el que obtiene','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']}, # 1 2 3 -1\n",
    "             'VAR_VEG_P':     {'VV_P_TABLE':['Autores','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "             'VAR_ANI_P':     {'VA_P_TABLE':['Autores','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "             'RAZ_PEC_P':     {'RAZ_PEC_P_TABLE':['Autores','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "             'TRA_FIL_P': {'TRA_FIL_P_TABLE':['Proyecto de investigación del cual se derivó el libro (Código-Título)','Financiador(es) del proyecto del cual se derivó el libro','Financiador(es) de la publicación','Autores','Citas recibidas (si tiene)','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']}\n",
    "            },\n",
    "     'DTI_P': {'DIS_IND_P': {'DI_P_TABLE':['Autores','Contrato (si aplica)','Nombre comercial (si aplica)','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'CIR_INT_P': {'ECI_P_TABLE':['Autores','Contrato (si aplica)','Nombre comercial (si aplica)','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'SOFT_P': {'SF_P_TABLE':['Autores','Contrato (si aplica)','Nombre comercial (si aplica)','TRL','Agregue la evidencia verificada al repositorio digital y copie el link del archivo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'NUTRA_P': {'NUTRA_P_TABLE':['Autores','Agregue la evidencia verificada al repositorio digital y copie el link del archivo en este campo','¿El producto cumple con los requisitos para ser avalado?']}, # add\n",
    "              'COL_CIENT_P': {'COL_CIENT_P_TABLE':['Autores','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo', '¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'REG_CIENT_P': {'REG_CIENT_P_TABLE':['Autores','Contrato licenciamiento (si aplica)','Agregue las evidencias verificadas al repositorio digital y copie el link del archivo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'PLT_PIL_P': {'PP_P_TABLE':['Autores','Agregue la evidencia verificada al repositorio digital y copie el link del archivo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'PRT_IND_P': {'PI_P_TABLE':['Autores','Nombre comercial (si aplica)','TRL','Agregue la evidencia verificada al repositorio digital y copie el link del archivo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'SEC_IND_P': {'SE_P_TABLE':['Autores','Agregue la evidencia verificada al repositorio digital y copie el link del archivo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'PROT_VIG_EPID_P': {'PROT_VIG_EPID_P_TABLE':['Autores','Agregue la evidencia verificada al repositorio digital y copie el link del archivo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'EMP_BSE_TEC_P': {'EBT_P_TABLE':['Autores','Agregue la evidencia verificada al repositorio digital y copie el link del archivo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'EMP_CRE_CUL_P': {'ICC_P_TABLE':['Autores','Agregue la evidencia verificada al repositorio digital y copie el link del archivo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'INN_GES_EMP_P': {'IG_P_TABLE':['Autores','Contrato (si aplica)','Nombre comercial (si aplica)','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'INN_PROC_P': {'IPP_P_TABLE':['Autores','Contrato (si aplica)','Nombre comercial (si aplica)','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'REG_NORM_REGL_LEG_P': {'RNR_P_TABLE':['Autores','Contrato (si aplica)','Convenio (si aplica)','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'CONP_TEC_P': {'CONP_TEC_P_TABLE':['Autores','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'REG_AAD_P': {'AAAD_P_TABLE':['Autores','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'SIG_DIS_P': {'SD_P_TABLE':['Autores','Contrato licenciamiento (si aplica)','Agregue las evidencias verificadas al repositorio digital y copie el link del archivo en este campo','¿El producto cumple con los requisitos para ser avalado?']}\n",
    "              },\n",
    "    'ASC_P': {'GEN_CONT_IMP_P': {'GC_I_P_TABLE_5':['Autores','Citas recibidas (si tiene)','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'PASC_P': {'PASC_FOR_P_TABLE':['Proyecto/Código','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?'],\n",
    "               'PASC_TRA_P_TABLE':['Proyecto/Código','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?'],\n",
    "               'PASC_GEN_P_TABLE':['Proyecto/Código','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?'],\n",
    "               'PASC_CAD_P_TABLE':['Proyecto/Código','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'DC_P': {'DC_CD_P_TABLE':['Proyecto/Código','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?'],\n",
    "               'DC_CON_P_TABLE':['Medio de verificación','Proyecto/Código','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?'],\n",
    "               'DC_TRA_P_TABLE':['Medio de verificación','Proyecto/Código','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?'],\n",
    "               'DC_DES_P_TABLE':['Medio de verificación','Proyecto/Código','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']}},\n",
    "    \n",
    "    'FRH_P': {'TES_DOC_P': {'TD_P_TABLE':['Número de cédula del graduado','¿La fecha fin coincide con la fecha de grado del estudiante?','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},  # 1 -1\n",
    "              'TES_MAST_P': {'TM_P_TABLE':['Número de cédula del graduado','¿La fecha fin coincide con la fecha de grado del estudiante?','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']}, # 1 -1\n",
    "              'TES_PREG_P': {'TP_P_TABLE':['Número de cédula del graduado','¿La fecha fin coincide con la fecha de grado del estudiante?','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']}, # 1 -1\n",
    "              'ASE_PRG_ACA_P': {'APGA_P_TABLE':['Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'ASE_CRE_CUR_P': {'ACC_P_TABLE':['Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'ASE_PRG_ONDAS_P': {'APO_P_TABLE':['Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']}},\n",
    "    'NC' : {'LIB' : {'LIB_T_AVAL_TABLE': ['Proyecto de investigación del cual se derivó el libro (Código-Título)','Financiador(es) del proyecto del cual se derivó el libro', 'Financiador(es) de la publicación','Autores','Citas recibidas (si tiene)','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']}, \n",
    "            'CAP_LIB':{'CAP_LIB_T_AVAL_TABLE':['Proyecto de investigación del cual se derivó el libro que contiene el capítulo (Código-Título)','Financiador del proyecto del cual se derivó el libro que contiene el capítulo','Financiador de la publicación','Autores','Citas recibidas (si tiene)','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']}}\n",
    "}\n",
    "\n",
    "d = {\n",
    "                '1': 'C',\n",
    "                '2': 'D',\n",
    "                '3': 'E',\n",
    "                '4': 'F',\n",
    "                '5': 'G',\n",
    "                '6': 'H',\n",
    "                '7': 'I',\n",
    "                '8': 'J',\n",
    "                '9': 'K',\n",
    "                '10': 'L',\n",
    "                '11': 'M',\n",
    "                '12': 'N',\n",
    "                '13': 'O',\n",
    "                '14': 'P',\n",
    "                '15': 'Q',\n",
    "                '16': 'R',\n",
    "                '17': 'S',\n",
    "                '18': 'T',\n",
    "                '19': 'U',\n",
    "                '20': 'V'\n",
    "}\n",
    "\n",
    "def clean_df(df):\n",
    "    'remove innecesari collums'\n",
    "    c=[x for x in df.columns if x.find('Unnamed:') == -1 and  x.find('Revisar') == -1 and x.find('Avalar integrante') == -1]\n",
    "    dfc=df[c]\n",
    "    return dfc\n",
    "\n",
    "def rename_col(df,colr,colf):\n",
    "    df.rename(columns = {colr: colf,}, inplace = True)\n",
    "    return df\n",
    "\n",
    "# WORKSHEET 4 - 12.\n",
    "def format_df(df, sheet_name, start_row, writer,eh, veh = None):\n",
    "    'format headers'\n",
    "    \n",
    "    df.to_excel(writer,sheet_name, startrow = start_row+1, startcol=2,index = False)\n",
    "\n",
    "    # Get the xlsxwriter workbook and worksheet objects.\n",
    "    worksheet = writer.sheets[sheet_name]\n",
    "    \n",
    "         \n",
    "    merge_format = workbook.add_format({\n",
    "    'bold': 1,\n",
    "    'border':1,\n",
    "    'text_wrap': True,    \n",
    "    'align': 'center',\n",
    "    'valign': 'vcenter',\n",
    "    'font_color': 'blue'})\n",
    "    \n",
    "    #form merge cells\n",
    "    if not df.empty:\n",
    "        start,end = 1,df.shape[1]\n",
    "    else:\n",
    "        start,end = 1,1\n",
    "    \n",
    "\n",
    "    m_range = d.get(str(start)) + str(start_row + 1) + ':' + d.get(str(end)) + str(start_row +1)\n",
    "\n",
    "    worksheet.merge_range(m_range, 'Información suministrada por la Vicerrectoría de Investigación', merge_format)\n",
    "    \n",
    "    # for merge headers cells\n",
    "    _m_range = d.get(str(end+1)) + str(start_row +1) + ':' +  d.get(str(end+len(eh))) + str(start_row +1)\n",
    "    \n",
    "    worksheet.merge_range(_m_range, 'Validación del Centro, Instituto o Corporación', merge_format)\n",
    "        \n",
    "    worksheet.set_row_pixels(start_row+1, 120)\n",
    "    #worksheet.set_column('C:C',30,general)\n",
    "    \n",
    "    # SET COLUMS FORMAT BY SHEET\n",
    "    if sheet_name=='3.Integrantes grupo':\n",
    "        worksheet.set_column('A:A', 5)\n",
    "        worksheet.set_column('B:B', 2)\n",
    "        worksheet.set_column('D:K',15,general)\n",
    "    \n",
    "    if sheet_name=='4.ART y N':\n",
    "        worksheet.set_column('A:A', 5)\n",
    "        worksheet.set_column('B:B', 2)\n",
    "        worksheet.set_column('C:C',20,general)\n",
    "        worksheet.set_column('M:O',20, general)\n",
    "     \n",
    "\n",
    "    if sheet_name=='5.LIB y LIB_FOR':\n",
    "        worksheet.set_column('A:A', 5)\n",
    "        worksheet.set_column('B:B', 2)\n",
    "        worksheet.set_column('C:C',20,general)\n",
    "        worksheet.set_column('I:P',20,general)\n",
    "\n",
    "    if sheet_name=='6.CAP':\n",
    "        worksheet.set_column('A:A', 5)\n",
    "        worksheet.set_column('B:B', 2)\n",
    "        worksheet.set_column('C:C',20,general)\n",
    "        worksheet.set_column('D:H',10,general)\n",
    "        worksheet.set_column('I:K',18,general)\n",
    "        worksheet.set_column('J:P',20,general)\n",
    "\n",
    "    if sheet_name=='7.Patente_Variedades':\n",
    "        worksheet.set_column('A:A', 5)\n",
    "        worksheet.set_column('B:B', 2)\n",
    "        worksheet.set_column('C:C',20,general)\n",
    "        worksheet.set_column('D:I',10,general)\n",
    "        worksheet.set_column('J:K',20,general)\n",
    "        worksheet.set_column('L:S',20,general)\n",
    "\n",
    "    if sheet_name=='8.AAD':\n",
    "        worksheet.set_column('A:A', 5)\n",
    "        worksheet.set_column('B:B', 2)\n",
    "        worksheet.set_column('C:C',20,general)\n",
    "        worksheet.set_column('F:K',10,general)\n",
    "        worksheet.set_column('L:P',25,general)\n",
    "\n",
    "    if sheet_name=='9.Tecnológico':\n",
    "        worksheet.set_column('A:A', 5)\n",
    "        worksheet.set_column('B:B', 2)\n",
    "        worksheet.set_column('C:C',20,general)\n",
    "        worksheet.set_column('D:I',10,general)\n",
    "        worksheet.set_column('J:S',18,general)\n",
    "\n",
    "    if sheet_name=='10.Empresarial':\n",
    "        worksheet.set_column('A:A', 5)\n",
    "        worksheet.set_column('B:B', 2)\n",
    "        worksheet.set_column('C:C',20,general)\n",
    "        worksheet.set_column('D:H',10,general)\n",
    "        worksheet.set_column('I:N',20,general)\n",
    "\n",
    "    if sheet_name=='11.ASC y Divulgación':\n",
    "        worksheet.set_column('A:A', 5)\n",
    "        worksheet.set_column('B:B', 2)\n",
    "        worksheet.set_column('C:C',28,general)\n",
    "        worksheet.set_column('I:I',15,general)\n",
    "        worksheet.set_column('J:N',20,general)\n",
    "\n",
    "    if sheet_name=='12.Formación y programas':\n",
    "        worksheet.set_column('A:A', 5)\n",
    "        worksheet.set_column('B:B', 2)\n",
    "        worksheet.set_column('C:C',25,general)\n",
    "        worksheet.set_column('D:G',10,general)\n",
    "        worksheet.set_column('L:O',15,general)\n",
    "        worksheet.set_column('N:N',20,general)\n",
    "        \n",
    "    worksheet.write(start_row+1, 0, 'VoBo de VRI', merge_format)\n",
    "    # Add a header format.\n",
    "    \n",
    "    fmt_header = workbook.add_format({\n",
    "        'bold': True,\n",
    "        'align': 'center',    \n",
    "        'text_wrap': True,\n",
    "        'valign': 'vcenter',\n",
    "        'fg_color': '#33A584',\n",
    "        'font_color': '#FFFFFF',\n",
    "        'border': 1})\n",
    "    \n",
    "    # Write the column headers with the defined format.\n",
    "    for col_num, value in enumerate(df.columns.values):\n",
    "        worksheet.write(start_row+1, col_num + 2, value, fmt_header)\n",
    "        \n",
    "    # write extra headers\n",
    "    for col_num, value in enumerate(eh):\n",
    "        worksheet.write(start_row+1, col_num + df.shape[1] + 2, value, fmt_header)\n",
    "        \n",
    "    v_range = 'A' + str(start_row +3) + ':' + 'A' + str(df.shape[0] + start_row +2)\n",
    "    worksheet.data_validation(v_range,{'validate': 'list',\n",
    "                                  'source': ['Sí', 'No']})\n",
    "    \n",
    "    \n",
    "    if sheet_name !='3.Integrantes grupo':\n",
    "        \n",
    "        v_range = d.get(str(end+len(eh))) + str(start_row +3) + ':' + d.get(str(end+len(eh))) + str(df.shape[0] + start_row +2)\n",
    "        worksheet.data_validation(v_range,{'validate': 'list',\n",
    "                                  'source': ['Sí', 'No']})\n",
    "    \n",
    "    # Integrantes\n",
    "    if veh == 0:\n",
    "        v_range = d.get(str(end+len(eh)-2)) + str(start_row +3) + ':' + d.get(str(end+len(eh)-2)) + str(df.shape[0] + start_row +2)\n",
    "        worksheet.data_validation(v_range,{'validate': 'list',\n",
    "                                'source': ['Sí', 'No']})  \n",
    "    # patentes\n",
    "    if veh == 1 :\n",
    "        v_range = d.get(str(end+len(eh)-3)) + str(start_row +3) + ':' + d.get(str(end+len(eh)-3)) + str(df.shape[0] + start_row +2)\n",
    "        worksheet.data_validation(v_range,{'validate': 'list',\n",
    "                                  'source': ['Sí', 'No']})\n",
    "        v_range = d.get(str(end+len(eh)-4)) + str(start_row +3) + ':' + d.get(str(end+len(eh)-4)) + str(df.shape[0] + start_row +2)\n",
    "        worksheet.data_validation(v_range,{'validate': 'list',\n",
    "                                  'source': ['Sí', 'No']})\n",
    "        v_range = d.get(str(end+len(eh)-5)) + str(start_row +3) + ':' + d.get(str(end+len(eh)-5)) + str(df.shape[0] + start_row +2)\n",
    "        worksheet.data_validation(v_range,{'validate': 'list',\n",
    "                                  'source': ['Sí', 'No']})\n",
    "    if veh ==2:\n",
    "        v_range = d.get(str(end+len(eh)-2)) + str(start_row +3) + ':' + d.get(str(end+len(eh)-2)) + str(df.shape[0] + start_row +2)\n",
    "        worksheet.data_validation(v_range,{'validate': 'list',\n",
    "                                  'source': ['Sí', 'No']})\n",
    "        \n",
    "    if veh == 3:\n",
    "        v_range = d.get(str(end+len(eh)-2)) + str(start_row +3) + ':' + d.get(str(end+len(eh)-3)) + str(df.shape[0] + start_row +2)\n",
    "        worksheet.data_validation(v_range,{'validate': 'list',\n",
    "                                  'source': ['Sí', 'No']})\n",
    "        v_range = d.get(str(end+len(eh)-3)) + str(start_row +3) + ':' + d.get(str(end+len(eh)-4)) + str(df.shape[0] + start_row +2)\n",
    "        worksheet.data_validation(v_range,{'validate': 'list',\n",
    "                                  'source': ['Sí', 'No']})\n",
    "        v_range = d.get(str(end+len(eh)-4)) + str(start_row +3) + ':' + d.get(str(end+len(eh)-5)) + str(df.shape[0] + start_row +2)\n",
    "        worksheet.data_validation(v_range,{'validate': 'list',\n",
    "                                  'source': ['Sí', 'No']})\n",
    "        \n",
    "        \n",
    "##### WORKSHEET 2\n",
    "def format_info(df, writer, sheet_name):\n",
    "    \n",
    "    '''format worksheet'''\n",
    "    \n",
    "    workbook=writer.book\n",
    "    \n",
    "    normal=workbook.add_format({'font_size':12,'text_wrap':True})\n",
    "    \n",
    "    merge_format = workbook.add_format({\n",
    "    'bold': 1,\n",
    "    'border':1,\n",
    "    'text_wrap': True,    \n",
    "    'align': 'center',\n",
    "    'valign': 'vcenter',\n",
    "    'font_color': 'black'})\n",
    "    \n",
    "    fmt_header = workbook.add_format({\n",
    "        'align': 'center',    \n",
    "        'text_wrap': True,\n",
    "        'valign': 'top',\n",
    "        'fg_color': '#33A584',\n",
    "        'font_color': '#FFFFFF',\n",
    "        'border': 1})\n",
    "    \n",
    "    # write df\n",
    "    start_row = 6\n",
    "    start_col = 3\n",
    "    \n",
    "    df.to_excel(writer, sheet_name, startrow =start_row, startcol=start_col,index = False)\n",
    "\n",
    "    # get worksheet object\n",
    "    worksheet = writer.sheets[sheet_name]\n",
    "    \n",
    "    for col_num, value in enumerate(df.columns.values):\n",
    "        worksheet.write(start_row, col_num + 3, value, fmt_header)\n",
    "    \n",
    "    #Prepare image insertion: See → https://xlsxwriter.readthedocs.io/example_images.html\n",
    "    worksheet.set_column('A:A', 15)\n",
    "    worksheet.set_column('B:B', 15)\n",
    "    worksheet.insert_image('A1', 'img/udea.jpeg')\n",
    "    \n",
    "    # title 1 UNIVERSIDAD DE ANTIOQUIA\n",
    "    title = workbook.add_format({'font_size':16,'center_across':True})\n",
    "\n",
    "    # title 2 Vicerrectoria de Investigación\n",
    "    title2 = workbook.add_format({'font_size':16,'center_across':True})\n",
    "   \n",
    "    # sub title 2 datos identificacion contacto\n",
    "    title3 = workbook.add_format({'font_size':12,'center_across':True})\n",
    "    \n",
    "    # merge d1:f1\n",
    "    worksheet.merge_range('D1:F1', 'UNIVERSIDAD DE ANTIOQUIA', title)\n",
    "        \n",
    "    # merge d2:f2\n",
    "    worksheet.merge_range('D2:F2', ' Vicerrectoria de Investigación', title2)\n",
    "    \n",
    "    # merge d3:f3\n",
    "    worksheet.merge_range('D3:F3', ' Datos de identificación y contacto', title3)\n",
    "    \n",
    "    # D5: F5\n",
    "    worksheet.merge_range('D5:E5','Número inscripcion a la convocatoria:',merge_format)\n",
    "    worksheet.write('F5','#',merge_format)\n",
    "    \n",
    "    # d6:f6\n",
    "    worksheet.merge_range('D6:F6','Identificación del Grupo',merge_format)\n",
    "        \n",
    "    # d9:f9\n",
    "    worksheet.merge_range('D10:F10','Identificación del Centro de Investigación',merge_format)\n",
    "    # write \n",
    "    a='Nombre del Centro, Instituto o Corporación'\n",
    "    worksheet.write('D11',a, fmt_header)\n",
    "    worksheet.set_column('D11:D11',30, fmt_header)\n",
    "    \n",
    "    b='Nombre completo del Jefe de Centro, Instituto o Corporación'\n",
    "    worksheet.write('E11',b, fmt_header) \n",
    "    worksheet.set_column('E11:E11',30, fmt_header)\n",
    "    \n",
    "    c='Email'\n",
    "    worksheet.write('F11',c, fmt_header) \n",
    "    worksheet.set_column('F11:F11',30, fmt_header)\n",
    "    \n",
    "    # d13:f13\n",
    "    worksheet.merge_range('D13:F13','Identificación de quien diligencia el formato',merge_format)\n",
    "    a='Nombre completo del encargado de diligenciar el formato'\n",
    "    worksheet.write('D14',a, fmt_header)\n",
    "    worksheet.set_column('D14:D14',30, normal)\n",
    "    \n",
    "    b='Email'\n",
    "    worksheet.write('E14',b, fmt_header) \n",
    "    worksheet.set_column('E14:E14',30, normal)\n",
    "    \n",
    "    c='Teléfono de contacto'\n",
    "    worksheet.write('F14',c, fmt_header) \n",
    "    worksheet.set_column('F14:F14',30, normal)\n",
    "\n",
    "# WORKSHEET 1\n",
    "def format_ptt(workbook):\n",
    "    \n",
    "    #Global variables\n",
    "    abstract_text='VERIFICACIÓN DE INFORMACIÓN PARA OTORGAR AVAL A LOS GRUPOS DE INVESTIGACIÓN  E INVESTIGADORES PARA SU PARTICIPACIÓN EN LA CONVOCATORIA 894 DE 2021 DE MINCIENCIAS'\n",
    "    instructions='''Los grupos de investigación e investigadores de la Universidad de Antioquia que deseen participar en la Convocatoria Nacional para el reconocimiento y medición de grupos de investigación, desarrollo tecnológico o de innovación y para el reconocimiento de investigadores del Sistema Nacional de Ciencia, Tecnología e Innovación - SNCTI, 894 de 2021, deben presentar la información actualizada en las plataformas CvLAC y GrupLAC validada por el Centro de Investigación en el presente formato, y respaldada en el repositorio digital de evidencias dispuesto para este fin, para la obtención del aval institucional por parte de la Vicerrectoría de Investigación. \n",
    "\n",
    "    La información a validar corresponde a los años 2019-2020 y aquella que entra en la ventana de observación y debe ser modificada según el Modelo de medición de grupos. La validación comprende:\n",
    "\n",
    "    1. Verificación de la vinculación de los integrantes a la Universidad de Antioquia y al grupo de investigación.  Diligenciar los campos solicitados. \n",
    "\n",
    "    2. Verificación de la producción de GNC, DTeI, ASC y FRH, en los campos habilitados en cada hoja de este formato. Las evidencias requeridas para los productos deben ser anexadas al repositorio digital asignado al grupo y se deben enlazar a cada producto.  \n",
    "\n",
    "    Este documento debe ser diligenciado en línea.\n",
    "\n",
    "    De antemano, la Vicerrectoría de Investigación agradece su participación en este ejercicio, que resulta de vital importancia para llevar a buen término la Convocatoria de Reconocimiento y Medición de Grupos de Investigación\n",
    "    '''\n",
    "    #Final part of the first sheet\n",
    "    datos=clean_df(pd.read_excel('https://github.com/restrepo/InstituLAC/raw/main/data/template_data.xlsx'))\n",
    "\n",
    "    #Capture xlsxwriter object \n",
    "    # IMPORTANT → workbook is the same object used in the official document at https://xlsxwriter.readthedocs.io\n",
    "    #workbook=writer.book\n",
    "    #***************\n",
    "    #Styles as explained in https://xlsxwriter.readthedocs.io\n",
    "    title=workbook.add_format({'font_size':28,'center_across':True})\n",
    "    subtitle=workbook.add_format({'font_size':24,'center_across':True})\n",
    "    abstract=workbook.add_format({'font_size':20,'center_across':True,'text_wrap':True})\n",
    "    normal=workbook.add_format({'font_size':12,'text_wrap':True})\n",
    "\n",
    "    #***************\n",
    "    #Creates the first work-sheet\n",
    "    #IMPORTANT → worksheet is the same object  used in the official document at https://xlsxwriter.readthedocs.io\n",
    "    worksheet=workbook.add_worksheet(\"1.Presentación\")\n",
    "    #Prepare image insertion: See → https://xlsxwriter.readthedocs.io/example_images.html\n",
    "    worksheet.set_column('A:A', 15)\n",
    "    worksheet.set_column('B:B', 15)\n",
    "    worksheet.insert_image('A1', 'img/udea.jpeg')\n",
    "    #Prepare text insertion: See  → https://xlsxwriter.readthedocs.io/example_images.html\n",
    "    worksheet.set_column('C:C', 140,general)\n",
    "    worksheet.set_row_pixels(0, 60)\n",
    "    #Texts\n",
    "    worksheet.write('C1', 'UNIVERSIDAD DE ANTIOQUIA',title)\n",
    "    worksheet.set_row_pixels(2, 60)\n",
    "    worksheet.write('C3', 'VICERRECTORÍA DE INVESTIGACIÓN',subtitle)\n",
    "    worksheet.set_row_pixels(5, 100)\n",
    "    worksheet.write('C6', abstract_text,abstract)\n",
    "    worksheet.set_row_pixels(8, 40)\n",
    "    worksheet.write('C9','PRESENTACIÓN DEL EJERCICIO',\n",
    "                    workbook.add_format({'font_size':18,'center_across':True}) )\n",
    "    worksheet.set_row_pixels(10, 320)\n",
    "    worksheet.write('C11',instructions,normal)\n",
    "    #*** ADD PANDAS DATAFRAME IN SPECIFIC POSITION ****\n",
    "    #Add a data Frame in some specific position. See → https://stackoverflow.com/a/43510881/2268280\n",
    "    #                                       See also → https://xlsxwriter.readthedocs.io/working_with_pandas.html\n",
    "    writer.sheets[\"1.Presentación\"]=worksheet\n",
    "    datos.to_excel(writer,sheet_name=\"1.Presentación\",startrow=12,startcol=2,index=False)\n",
    "    #**************************************************\n",
    "    #Fix columns heights for long text\n",
    "    worksheet.set_row_pixels(17, 40)\n",
    "    worksheet.set_row_pixels(18, 40)\n",
    "    worksheet.set_row_pixels(19, 40)\n",
    "    worksheet.set_row_pixels(20, 40)\n",
    "    worksheet.set_row_pixels(22, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN CODE\n",
    "\n",
    "# login =\n",
    "# name_ins =\n",
    "# usser =\n",
    "# passw=\n",
    "\n",
    "# login\n",
    "browser = h.start_firefox('https://scienti.minciencias.gov.co/institulac2-war/')\n",
    "sleep=0.8\n",
    "\n",
    "#browser = h.start_firefox('https://scienti.minciencias.gov.co/institulac2-war/')\n",
    "time.sleep(sleep)\n",
    "h.click('Consulte Aquí')\n",
    "\n",
    "time.sleep(sleep)\n",
    "h.write('UNIVERSIDAD DE ANTIOQUIA',into='Digite el nombre de la Institución') # name ins\n",
    "\n",
    "time.sleep(sleep)\n",
    "h.click('Buscar')\n",
    "\n",
    "time.sleep(sleep)\n",
    "h.click(browser.find_element_by_id('list_instituciones'))\n",
    "\n",
    "time.sleep(sleep)\n",
    "\n",
    "time.sleep(sleep)\n",
    "h.select('seleccione una','UNIVERSIDAD DE ANTIOQUIA') # name_ins\n",
    "\n",
    "time.sleep(sleep)\n",
    "h.write('annyarango',into='Usuario')                  # user\n",
    "\n",
    "time.sleep(sleep)\n",
    "h.write('1@Silver', into='Contraseña')                # passw\n",
    "\n",
    "time.sleep(sleep)\n",
    "h.click(h.Button('Ingresar'))\n",
    "\n",
    "# cookie injection\n",
    "time.sleep(sleep)\n",
    "# implementation cookie injection\n",
    "\n",
    "# get current cookie and store\n",
    "new_cookie=browser.get_cookies()[0]\n",
    "    \n",
    "# create new_cookie with time_expire\n",
    "time_expire = (datetime(2022,1,1) - datetime(1970,1,1)).total_seconds()\n",
    "new_cookie['expiry'] = int(time_expire)\n",
    "    \n",
    "# delete cookie sites\n",
    "browser.delete_all_cookies()\n",
    "    \n",
    "# add new cookie\n",
    "browser.add_cookie(new_cookie)\n",
    "\n",
    "# navigation 1\n",
    "time.sleep(sleep)\n",
    "h.click('Aval')\n",
    "\n",
    "time.sleep(sleep)\n",
    "h.click('Avalar grupos')\n",
    "\n",
    "time.sleep(sleep)\n",
    "h.click('Grupos Avalados')\n",
    "\n",
    "# -- end login --\n",
    "\n",
    "# list of total groups\n",
    "#select max results per page\n",
    "h.wait_until(h.Text('Ver Reporte').exists)\n",
    "h.click(browser.find_element_by_xpath('//table[@id=\"grupos_avalados\"]//select[@name=\"maxRows\"]'))\n",
    "\n",
    "time.sleep(sleep)\n",
    "h.select(browser.find_element_by_xpath('//table[@id=\"grupos_avalados\"]//select[@name=\"maxRows\"]'),'100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Nombre del grupo', 'Nombre del líder', 'COL Grupo', 'Revisar'], dtype='object') (100, 4)\n",
      "Index(['Nombre del grupo', 'Nombre del líder', 'COL Grupo', 'Revisar'], dtype='object') (100, 4)\n",
      "Index(['Nombre del grupo', 'Nombre del líder', 'COL Grupo', 'Revisar'], dtype='object') (100, 4)\n",
      "Index(['Nombre del grupo', 'Nombre del líder', 'COL Grupo', 'Revisar'], dtype='object') (24, 4)\n",
      "Message: Unable to locate element: //table[@id=\"grupos_avalados\"]//tr/td[3]/a\n",
      "\n",
      "out of cicle\n"
     ]
    }
   ],
   "source": [
    "# catch 1: groups info [name, lider, cod,  link to producs]  \n",
    "# schema\n",
    "# empty df\n",
    "# select max items per page\n",
    "# while until end\n",
    "# try:\n",
    "    # catch table\n",
    "    # preproces table\n",
    "    # catch urls\n",
    "    # add url colums\n",
    "    # add df\n",
    "    # click next page -> raise error\n",
    "# except Nosuchelement:\n",
    "    # break\n",
    "    \n",
    "# catch 1: list of groups\n",
    "dfg=pd.DataFrame()\n",
    "cont=True\n",
    "\n",
    "while cont:\n",
    "    \n",
    "    try:\n",
    "        # catch source\n",
    "        time.sleep(sleep)\n",
    "        source_g=browser.page_source\n",
    "        \n",
    "        # catch table\n",
    "        time.sleep(sleep)\n",
    "        df=pd.read_html(source_g, attrs={\"id\":\"grupos_avalados\"}, header=2)[0]\n",
    "        \n",
    "        # and preprocces it\n",
    "        c=[x for x in df.columns if x.find('Unnamed:') == -1]\n",
    "        dfgp=df[c][1:-1]\n",
    "        print(dfgp.columns,dfgp.shape)\n",
    "        \n",
    "        # catch urls\n",
    "        url=[a.get_attribute('href') for a in browser.find_elements_by_xpath('//table[@id=\"grupos_avalados\"]//td[5]/a')]\n",
    "        dfgp['Revisar'] = url\n",
    "        dfg=dfg.append(dfgp)\n",
    "        \n",
    "        # click next page. this instruction rise error of the end. \n",
    "        h.click(browser.find_element_by_xpath('//table[@id=\"grupos_avalados\"]//tr/td[3]/a'))\n",
    "        \n",
    "    except NoSuchElementException as e:\n",
    "        \n",
    "        print(e)\n",
    "        print('out of cicle')\n",
    "        break\n",
    "        \n",
    "    time.sleep(sleep)\n",
    "    time.sleep(sleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = dfg.reset_index(drop=True)\n",
    "assert dfg.shape[0] == 324\n",
    "\n",
    "time.sleep(sleep*2)\n",
    "\n",
    "DB = [] # \n",
    "LP = []\n",
    "LR = [] \n",
    "for idx in dfg.index[:1]:       # TEST\n",
    "    \n",
    "    # create db for store things related to group\n",
    "    DBG = {}\n",
    "    \n",
    "    # part info group\n",
    "    print(dfg.loc[idx,'Nombre del grupo'])\n",
    "\n",
    "    # specific group url\n",
    "    time.sleep(sleep)\n",
    "    url_group = dfg.loc[idx,'Revisar']\n",
    "\n",
    "    # go to url group\n",
    "    time.sleep(sleep)\n",
    "    browser.get(url_group)\n",
    "\n",
    "    # catch two tables: info grupo and  members\n",
    "    source=browser.page_source\n",
    "\n",
    "    #info\n",
    "    l_info=pd.read_html(source, match='Nombre Grupo')\n",
    "    info_g=l_info[3].pivot(columns=0,values=1)\n",
    "    \n",
    "    # STORE INFO_GROUP\n",
    "    DBG['Info_group'] = info_g\n",
    "\n",
    "    # members\n",
    "    l_int = pd.read_html(source,attrs={'id':'tblIntegrantes'},header=2)\n",
    "    mem_g=l_int[0]\n",
    "    \n",
    "    # STORE_MEMBERS\n",
    "    DBG['Members'] =  mem_g\n",
    "\n",
    "    # Products\n",
    "\n",
    "    #time.sleep(sleep*5) # time time time !!!\n",
    "    h.wait_until(lambda: browser.find_element_by_xpath('//td[@id=\"bodyPrincipal\"]//a[text()=\"Ver productos\"]') is not None)\n",
    "    h.click(browser.find_element_by_xpath('//td[@id=\"bodyPrincipal\"]//a[text()=\"Ver productos\"]'))\n",
    "\n",
    "    # products by belongs to  # time time time\n",
    "    #time.sleep(sleep*7)       # time time time\n",
    "    h.wait_until(lambda: browser.find_element_by_xpath('//*[@id=\"ProdsPertenecia\"]') is not None)\n",
    "    h.click(browser.find_element_by_xpath('//*[@id=\"ProdsPertenecia\"]'))\n",
    "\n",
    "    time.sleep(sleep)\n",
    "    url_products=browser.current_url\n",
    "\n",
    "\n",
    "    # map all products, store those id categories that amount is different to 0 and id products asociated.\n",
    "    # make queries with combinations of categories and products\n",
    "    # make urls with diferent combinations of quieries\n",
    "    # go to each of urls\n",
    "    # load page source\n",
    "    # catch table ( or tables) asociated with categories and products\n",
    "    # store tables\n",
    "\n",
    "    report = ''\n",
    "\n",
    "    list_of_prods =[] #[[cat,prod],[cat,prod]...]\n",
    "\n",
    "    # map all products and get products and subs diff to cero\n",
    "    for i in browser.find_elements_by_xpath('//div[@id=\"accordionCatgP\"]/h3'):\n",
    "\n",
    "        report += i.text + '\\n' \n",
    "        report += i.get_attribute('id') + '\\n'     \n",
    "\n",
    "        time.sleep(sleep)\n",
    "        h.click(i)\n",
    "        \n",
    "        # cat\n",
    "        cat_ = int(re.findall(r'\\d+',i.text)[0])\n",
    "        \n",
    "        # create cat key in dict, for estore diferents products by this categori: 'NC_': {'ART_E':TABLE,\n",
    "        #                                                                                 'ART_IMP':TABLE}\n",
    "        if cat_ > 0:\n",
    "            DBG[i.get_attribute('id')] = {}\n",
    "            \n",
    "        \n",
    "        for j in browser.find_elements_by_xpath('//div[@aria-labelledby=\"%s\"]/h3' % i.get_attribute('id')):\n",
    "\n",
    "            report += '\\t' + j.text + '\\n' \n",
    "            report += '\\t' + j.get_attribute('id') + '\\n'\n",
    "            \n",
    "            #prod\n",
    "            pro_ = int(re.findall(r'\\d+', j.text)[0])\n",
    "\n",
    "            if cat_ > 0 and pro_ > 0:  \n",
    "                \n",
    "                list_of_prods.append([i.get_attribute('id'),j.get_attribute('id')])\n",
    "\n",
    "        time.sleep(sleep) \n",
    "        # h.click(a)\n",
    "        h.click(i)\n",
    "    \n",
    "    # PAR: products with revisions\n",
    "    h.wait_until(lambda: browser.find_element_by_xpath('//*[@id=\"ProdsAval\"]'))\n",
    "    h.click(browser.find_element_by_xpath('//*[@id=\"ProdsAval\"]'))\n",
    "\n",
    "    # NC\n",
    "\n",
    "    _NC = browser.find_element_by_xpath('//*[@id=\"NC\"]')\n",
    "\n",
    "    h.click(_NC)\n",
    "\n",
    "    cat_ = int(re.findall(r'\\d+',_NC.text)[0])\n",
    "\n",
    "    LIB = browser.find_element_by_xpath('//*[@id=\"LIB\"]')\n",
    "\n",
    "    L = int(re.findall(r'\\d+', LIB.text)[0])\n",
    "\n",
    "    CAP_LIB = browser.find_element_by_xpath('//*[@id=\"CAP_LIB\"]')\n",
    "\n",
    "    CL = int(re.findall(r'\\d+', CAP_LIB.text)[0])\n",
    "\n",
    "    if (cat_ > 0 and L > 0) or (cat_ > 0 and CL > 0):\n",
    "\n",
    "        DBG[_NC.get_attribute('id')] = {}\n",
    "        \n",
    "    if (cat_ > 0 and L > 0):\n",
    "        \n",
    "        list_of_prods.append([_NC.get_attribute('id'),LIB.get_attribute('id')])\n",
    "    \n",
    "    if (cat_ > 0 and CL > 0):\n",
    "        \n",
    "        list_of_prods.append([_NC.get_attribute('id'),CAP_LIB.get_attribute('id')])\n",
    "                \n",
    "    # print(report)\n",
    "    # print('\\n')\n",
    "    # print('--------------------------------')\n",
    "    time.sleep(sleep*2)\n",
    "    \n",
    "    tables=[]\n",
    "    \n",
    "    for p in range(len(list_of_prods)):\n",
    "\n",
    "            # make query\n",
    "            if list_of_prods[p][0] == 'NC':\n",
    "                \n",
    "                query='categoria=%s&subcategoria=%s&aval=T' % (list_of_prods[p][0],list_of_prods[p][1])\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                query='categoriaP=%s&subcategoriaP=%s&aval=P' % (list_of_prods[p][0],list_of_prods[p][1])\n",
    "\n",
    "            # make url query\n",
    "            url_query = url_products.split('?')[0] + '?' + query + '&' + url_products.split('?')[1]\n",
    "\n",
    "            # retrieve id asociated tables\n",
    "            table_id = dict_tables[list_of_prods[p][0]][list_of_prods[p][1]]\n",
    "\n",
    "            # go to url product by group\n",
    "            time.sleep(sleep)\n",
    "        \n",
    "            browser.get(url_query)\n",
    "\n",
    "            # load page\n",
    "            time.sleep(sleep)\n",
    "            page_source = browser.page_source\n",
    "\n",
    "            # catch tables\n",
    "            if isinstance(table_id,str): # case one table\n",
    "\n",
    "                # catch title table\n",
    "                \n",
    "                #title_table = browser.find_element_by_xpath('//div/p[@class=\"titulo_tabla\"]').text \n",
    "                # cathc table\n",
    "                print(url_query)\n",
    "                time.sleep(sleep*2)\n",
    "                try:\n",
    "                    table = pd.read_html(page_source,attrs={'id':table_id}, header=2)[0][1:-1]\n",
    "                except ValueError:\n",
    "                    table=None\n",
    "\n",
    "                # store table\n",
    "                DBG[list_of_prods[p][0]][list_of_prods[p][1]] = {table_id:table}\n",
    "                # ---- in building ----\n",
    "\n",
    "            elif isinstance(table_id, list): # case multiple tables\n",
    "                \n",
    "                DBG[list_of_prods[p][0]][list_of_prods[p][1]] ={}\n",
    "\n",
    "                for i in range(len(table_id)):\n",
    "                    \n",
    "                    # fix bug\n",
    "                    if list_of_prods[p][1] == 'DC_P' and i == 3:\n",
    "                        # catch title specific table \n",
    "                        title_table = browser.find_elements_by_xpath('//div/p[@class=\"titulo_tabla\"]')[i].text\n",
    "                        \n",
    "                        # catch table software\n",
    "                        table = pd.read_html(page_source,attrs={'id':table_id[i]}, header=2)[1][1:-1]\n",
    "                        \n",
    "                        # store table\n",
    "                        DBG[list_of_prods[p][0]][list_of_prods[p][1]]['DC_DES_P_TABLE'] = table\n",
    "                        \n",
    "\n",
    "                    # catch title specific table \n",
    "                    title_table = browser.find_elements_by_xpath('//div/p[@class=\"titulo_tabla\"]')[i].text\n",
    "\n",
    "                    # catch table trasmedia\n",
    "                    table = pd.read_html(page_source,attrs={'id':table_id[i]}, header=2)[0][1:-1]\n",
    "\n",
    "                    # store table\n",
    "                    DBG[list_of_prods[p][0]][list_of_prods[p][1]][table_id[i]]=table\n",
    "                \n",
    "        \n",
    "                    # -----------\n",
    "    DB.append(DBG)\n",
    "    LP.append(list_of_prods)\n",
    "    LR.append(report)\n",
    "\n",
    "    with open('DB.pickle', 'wb') as f:\n",
    "    pickle.dump(DB, f)\n",
    "\n",
    "    with open('DB.pickle','rb') as f:\n",
    "    NEWDBG = pickle.load(f)    \n",
    "#Recover None tables here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NC_P', 'FRH_P']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "list(DB[i].keys())[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "k='NC_P'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ART_IMP_P', 'ART_ELE_P', 'CAP_LIB_P', 'PAT_P'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB[i][k].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-86b42e08c163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "DB[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Info_group', 'Members', 'NC_P', 'FRH_P'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB[i].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fix_df(DB):\n",
    "    for i in range(len(DB)):\n",
    "        for k in list(DB[i].keys())[2:]:\n",
    "            for kk in  DB[i][k].keys():\n",
    "                #print(i,k,kk)\n",
    "                if list(DB[i][k][kk].values())[0] is None:\n",
    "                    DB[i][k][kk]={'PAT_P_TABLE': pd.DataFrame()} \n",
    "    return DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB=dummy_fix_df(DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/xlsxwriter/worksheet.py:1686: UserWarning: Can't merge single cell\n",
      "  warn(\"Can't merge single cell\")\n",
      "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ONE GROUP IMPLEMENTATION\n",
    "for idxx in range(len(DB)):\n",
    "# DATA\n",
    "    DBG = DB[idxx]\n",
    "\n",
    "    ### excel name\n",
    "    name = 'Plantilla_Formato de verificación de información_GrupLAC_894-2021_%s'\n",
    "\n",
    "    cod_gr = dfg.loc[idxx,'COL Grupo']\n",
    "\n",
    "    # initialize object= output excel file\n",
    "    writer = pd.ExcelWriter(name % cod_gr +'.xlsx', engine='xlsxwriter')\n",
    "\n",
    "    workbook=writer.book\n",
    "\n",
    "    general=workbook.add_format({'text_wrap':True})\n",
    "\n",
    "    # PPT\n",
    "    format_ptt(workbook)\n",
    "\n",
    "    # INFO GROUP\n",
    "    df=get_info(DBG['Info_group'], cod_gr)\n",
    "    format_info(df, writer, '2.Datos de contacto')\n",
    "\n",
    "    # WORKSHEET 1\n",
    "    df = clean_df(DBG['Members']) \n",
    "    eh = DBEH['MEMBERS']\n",
    "    format_df(df, '3.Integrantes grupo', 1, writer, eh, veh=0) #### veh = 0\n",
    "\n",
    "    ### NC_P ### \n",
    "\n",
    "    #------- w4 -------\n",
    "    # 4.ART y N\n",
    "\n",
    "    var_w4 = 0\n",
    "\n",
    "    try:\n",
    "        df=clean_df(DBG['NC_P']['ART_IMP_P']['ART_P_TABLE'])\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc+1)\n",
    "\n",
    "        eh=DBEH['NC_P']['ART_IMP_P']['ART_P_TABLE']\n",
    "\n",
    "        format_df(df, '4.ART y N',  var_w4, writer,eh)\n",
    "\n",
    "        var_w4 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        df=clean_df(DBG['NC_P']['ART_ELE_P']['ART_E_P_TABLE'])\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "        eh=DBEH['NC_P']['ART_ELE_P']['ART_E_P_TABLE']\n",
    "\n",
    "        format_df(df, '4.ART y N', var_w4, writer,eh)\n",
    "\n",
    "        var_w4 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        df=clean_df(DBG['NC_P']['NOT_CIE_P']['NOT_CIE_P_TABLE'])\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "        eh=DBEH['NC_P']['NOT_CIE_P']['NOT_CIE_P_TABLE']\n",
    "\n",
    "        format_df(df, '4.ART y N', var_w4, writer,eh)\n",
    "\n",
    "        var_w4 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "    # -------------- w4 -------------------------\n",
    "\n",
    "    #------------ ---w5------------\n",
    "    # 5.LIB y LIB_FOR\n",
    "    var_w5 = 0\n",
    "\n",
    "    # libros por pertenencia\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['NC_P']['LIB_P']['LIB_P_TABLE']),'Título del artículo','Título del libro')\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "        eh=DBEH['NC_P']['LIB_P']['LIB_P_TABLE']\n",
    "\n",
    "        format_df(df, '5.LIB y LIB_FOR',  var_w5, writer,eh)\n",
    "\n",
    "        var_w5 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # libros avalados con revisión\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['NC']['LIB']['LIB_T_AVAL_TABLE']), 'Título del artículo' ,'Título del libro') \n",
    "\n",
    "        #df.to_excel(writer,sheet_name='FRH_P',startrow = var_rh)\n",
    "\n",
    "        eh=DBEH['NC']['LIB']['LIB_T_AVAL_TABLE']\n",
    "\n",
    "        format_df(df, '5.LIB y LIB_FOR', var_w5 , writer, eh)\n",
    "\n",
    "        var_w5  += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # libros formacion\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['ASC_P']['GEN_CONT_IMP_P']['GC_I_P_TABLE_5']),'Título del libro','Título del libro formación') # lib form\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "        eh=DBEH['ASC_P']['GEN_CONT_IMP_P']['GC_I_P_TABLE_5']\n",
    "\n",
    "        format_df(df, '5.LIB y LIB_FOR',  var_w5 , writer,eh)\n",
    "\n",
    "        var_w5 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass  \n",
    "    # --------------------w5--------------\n",
    "\n",
    "    #--------------------w6---------------\n",
    "    #6.CAP\n",
    "\n",
    "    # cap pertenencia\n",
    "\n",
    "    var_w6 = 0\n",
    "\n",
    "    try:\n",
    "        df=clean_df(DBG['NC_P']['CAP_LIB_P']['CAP_LIB_P_TABLE'])\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "        eh=DBEH['NC_P']['CAP_LIB_P']['CAP_LIB_P_TABLE']\n",
    "\n",
    "        format_df(df, '6.CAP',var_w6, writer,eh)\n",
    "\n",
    "        var_w6 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # caps avalados con revision\n",
    "    try:\n",
    "        df = clean_df(DBG['NC']['CAP_LIB']['CAP_LIB_T_AVAL_TABLE'])  ### ,veh = 2\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='FRH_P',startrow = var_rh)\n",
    "\n",
    "        eh = DBEH['NC']['CAP_LIB']['CAP_LIB_T_AVAL_TABLE']\n",
    "\n",
    "        format_df(df, '6.CAP', var_w6, writer, eh)\n",
    "\n",
    "        var_w6 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # traduccion filologica\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['NC_P']['TRA_FIL_P']['TRA_FIL_P_TABLE']),'Título del libro', 'Título traducción filologica')\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "        eh=DBEH['NC_P']['TRA_FIL_P']['TRA_FIL_P_TABLE']\n",
    "\n",
    "        format_df(df, '6.CAP', var_w6, writer,eh)\n",
    "\n",
    "        var_w6 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    #-------------------w6------------------\n",
    "\n",
    "    #------------w7-------------------------\n",
    "    #7.Patente_Variedades\n",
    "    var_w7 = 0\n",
    "\n",
    "    # patentes\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['NC_P']['PAT_P']['PAT_P_TABLE']),'Título del artículo','Título de la patente') ###### veh=1\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "        eh=DBEH['NC_P']['PAT_P']['PAT_P_TABLE']\n",
    "\n",
    "        format_df(df, '7.Patente_Variedades', var_w7, writer,eh, veh=1)\n",
    "\n",
    "        var_w7 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # variedad vegetal\n",
    "    try:\n",
    "        df=clean_df(DBG['NC_P']['VAR_VEG_P']['VV_P_TABLE'])\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "        eh=DBEH['NC_P']['VAR_VEG_P']['VV_P_TABLE']\n",
    "\n",
    "        format_df(df, '7.Patente_Variedades', var_w7, writer,eh)\n",
    "\n",
    "        var_w7 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # Variedad Animal\n",
    "    try:\n",
    "        df=clean_df(DBG['NC_P']['VAR_ANI_P']['VA_P_TABLE'])\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "        eh=DBEH['NC_P']['VAR_ANI_P']['VA_P_TABLE']\n",
    "\n",
    "        format_df(df, '7.Patente_Variedades', var_w7, writer,eh)\n",
    "\n",
    "        var_w7 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # razas pecuarias mejoradas\n",
    "    try:\n",
    "        df=clean_df(DBG['NC_P']['RAZ_PEC_P']['RAZ_PEC_P_TABLE'])\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "        eh=DBEH['NC_P']['RAZ_PEC_P']['RAZ_PEC_P_TABLE']\n",
    "\n",
    "        format_df(df, '7.Patente_Variedades', var_w7, writer,eh)\n",
    "\n",
    "        var_w7 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "    # ---------------w7---------------------\n",
    "\n",
    "    #---------------w8-------------------\n",
    "    var_w8 = 0\n",
    "\n",
    "    # productos investigacion creacion\n",
    "    try:\n",
    "        df=clean_df(DBG['NC_P']['PRD_INV_ART_P']['PAAD_P_TABLE']) ###### veh = 1\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "        eh=DBEH['NC_P']['PRD_INV_ART_P']['PAAD_P_TABLE']\n",
    "\n",
    "        format_df(df, '8.AAD', var_w8, writer,eh, veh=3)\n",
    "\n",
    "        var_w8 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    #-------------W8---------------------\n",
    "\n",
    "    #-------------W9----------------\n",
    "\n",
    "    # 9.Tecnológico\n",
    "    #### DTI_P\n",
    "\n",
    "    var_w9 = 0\n",
    "\n",
    "    # diseño industrial\n",
    "    try:\n",
    "\n",
    "        df=rename_col(clean_df(DBG['DTI_P']['DIS_IND_P']['DI_P_TABLE']),'Nombre del diseño','Nombre del diseño industrial')\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "        eh=DBEH['DTI_P']['DIS_IND_P']['DI_P_TABLE']\n",
    "\n",
    "        format_df(df, '9.Tecnológico', var_w9, writer, eh)\n",
    "\n",
    "        var_w9 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    #circuitos integrados\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['DTI_P']['CIR_INT_P']['ECI_P_TABLE']),'Nombre del diseño', 'Nombre del diseño circuito')\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "        eh=DBEH['DTI_P']['CIR_INT_P']['ECI_P_TABLE']\n",
    "\n",
    "        format_df(df, '9.Tecnológico', var_w9, writer,eh)\n",
    "\n",
    "        var_w9 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # colecciones\n",
    "    try:\n",
    "        df=clean_df(DBG['DTI_P']['COL_CIENT_P']['COL_CIENT_P_TABLE'])\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "        eh=DBEH['DTI_P']['COL_CIENT_P']['COL_CIENT_P_TABLE']\n",
    "\n",
    "        format_df(df, '9.Tecnológico', var_w9, writer,eh)\n",
    "\n",
    "        var_w9 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # software \n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['DTI_P']['SOFT_P']['SF_P_TABLE']),'Nombre del diseño', 'Nombre del diseño de software')\n",
    "\n",
    "        eh=DBEH['DTI_P']['SOFT_P']['SF_P_TABLE']\n",
    "\n",
    "        format_df(df, '9.Tecnológico', var_w9, writer,eh)\n",
    "\n",
    "        var_w9 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # secreto industrial\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['DTI_P']['SEC_IND_P']['SE_P_TABLE']),'Producto','Nombre secreto industrial')\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "        eh=DBEH['DTI_P']['SEC_IND_P']['SE_P_TABLE']\n",
    "\n",
    "        format_df(df, '9.Tecnológico', var_w9, writer,eh)\n",
    "\n",
    "        var_w9 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # prototipo insdustrial\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['DTI_P']['PRT_IND_P']['PI_P_TABLE']), 'Nombre del diseño', 'Nombre del prototipo')\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "        eh=DBEH['DTI_P']['PRT_IND_P']['PI_P_TABLE']\n",
    "\n",
    "        format_df(df, '9.Tecnológico',  var_w9, writer,eh)\n",
    "\n",
    "        var_w9 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # Registro distintivo\n",
    "    try:\n",
    "        df=clean_df(DBG['DTI_P']['SIG_DIS_P']['SD_P_TABLE'])\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "        eh=DBEH['DTI_P']['SIG_DIS_P']['SD_P_TABLE']\n",
    "\n",
    "        format_df(df, '9.Tecnológico', var_w9, writer,eh)\n",
    "\n",
    "        var_w9 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # registros de acuerdo licencias expl obras AAD\n",
    "    try:\n",
    "\n",
    "        df=clean_df(DBG['DTI_P']['REG_AAD_P']['AAAD_P_TABLE'])\n",
    "\n",
    "        eh=DBEH['DTI_P']['REG_AAD_P']['AAAD_P_TABLE']\n",
    "\n",
    "        format_df(df, '9.Tecnológico', var_w9, writer,eh)\n",
    "\n",
    "        var_w9 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # prod nutracetico\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['DTI_P']['NUTRA_P']['NUTRA_P_TABLE']),'Nombre del producto','Nombre del producto nutracetico')\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='DTI_P',startrow = var_nc)\n",
    "\n",
    "        eh=DBEH['DTI_P']['NUTRA_P']['NUTRA_P_TABLE']\n",
    "\n",
    "        format_df(df, '9.Tecnológico', var_w9, writer,eh)\n",
    "\n",
    "        var_w9 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # registro cienti\n",
    "    try:\n",
    "        df=clean_df(DBG['DTI_P']['REG_CIENT_P']['REG_CIENT_P_TABLE'])\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "        eh=DBEH['DTI_P']['REG_CIENT_P']['REG_CIENT_P_TABLE']\n",
    "\n",
    "        format_df(df, '9.Tecnológico',var_w9 , writer,eh)\n",
    "\n",
    "        var_w9 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # planta piloto\n",
    "\n",
    "    try:\n",
    "        df=clean_df(DBG['DTI_P']['PLT_PIL_P']['PP_P_TABLE'])\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "        eh=DBEH['DTI_P']['PLT_PIL_P']['PP_P_TABLE']\n",
    "\n",
    "        format_df(df, '9.Tecnológico', var_w9, writer,eh)\n",
    "\n",
    "        var_w9 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # protocolo vigilancia epidemologica\n",
    "\n",
    "    try:\n",
    "        df=clean_df(DBG['DTI_P']['PROT_VIG_EPID_P']['PROT_VIG_EPID_P_TABLE'])\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "        eh=DBEH['DTI_P']['PROT_VIG_EPID_P']['PROT_VIG_EPID_P_TABLE']\n",
    "\n",
    "        format_df(df, '9.Tecnológico',var_w9, writer,eh)\n",
    "\n",
    "        var_w9 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "    #---------------------w9----------------\n",
    "\n",
    "    #---------------------w10----------------\n",
    "    # 10.Empresarial\n",
    "    var_w10 = 0\n",
    "\n",
    "    # innovación gestion empresarial\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['DTI_P']['INN_GES_EMP_P']['IG_P_TABLE']),'Nombre de la innovación', 'Nombre de la innovación empresarial')\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "        eh=DBEH['DTI_P']['INN_GES_EMP_P']['IG_P_TABLE']\n",
    "\n",
    "        format_df(df, '10.Empresarial', var_w10, writer,eh)\n",
    "\n",
    "        var_w10 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "    # innovacion procesos y procedimiento\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['DTI_P']['INN_PROC_P']['IPP_P_TABLE']),'Nombre de la innovación','Nombre de la innovación procesos y procedimientos')\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "        eh=DBEH['DTI_P']['INN_PROC_P']['IPP_P_TABLE']\n",
    "\n",
    "        format_df(df, '10.Empresarial', var_w10, writer,eh)\n",
    "\n",
    "        var_w10 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # regulaciones normas reglamentos legislaciones\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['DTI_P']['REG_NORM_REGL_LEG_P']['RNR_P_TABLE']),'Tipo producto','Nombre regulación')\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "        eh=DBEH['DTI_P']['REG_NORM_REGL_LEG_P']['RNR_P_TABLE']\n",
    "\n",
    "        format_df(df, '10.Empresarial', var_w10, writer,eh)\n",
    "\n",
    "        var_w10 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # conceptos tecnicos\n",
    "    try:\n",
    "        df=clean_df(DBG['DTI_P']['CONP_TEC_P']['CONP_TEC_P_TABLE'])\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "        eh=DBEH['DTI_P']['CONP_TEC_P']['CONP_TEC_P_TABLE']\n",
    "\n",
    "        format_df(df, '10.Empresarial', var_w10, writer,eh)\n",
    "\n",
    "        var_w10 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # empresa base tecnologica\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['DTI_P']['EMP_BSE_TEC_P']['EBT_P_TABLE']),'Tipo','Tipo de empresa base tecnologica')\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "        eh=DBEH['DTI_P']['EMP_BSE_TEC_P']['EBT_P_TABLE']\n",
    "\n",
    "        format_df(df, '10.Empresarial', var_w10, writer,eh)\n",
    "\n",
    "        var_w10 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # empresa de base cultural\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['DTI_P']['EMP_CRE_CUL_P']['ICC_P_TABLE']),'Empresa', 'Tipo de empresa base cultural')\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "        eh=DBEH['DTI_P']['EMP_CRE_CUL_P']['ICC_P_TABLE']\n",
    "\n",
    "        format_df(df, '10.Empresarial', var_w10, writer,eh)\n",
    "\n",
    "        var_w10 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # -------------------------w10-------------\n",
    "    ######  ASC\n",
    "\n",
    "    # -------- w11\n",
    "    # 11.ASC y Divulgación\n",
    "    var_w11 = 0 \n",
    "\n",
    "    # productos de interes social\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['ASC_P']['PASC_P']['PASC_FOR_P_TABLE']),'Nombre','Nombre producto interes social')\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "        eh=DBEH['ASC_P']['PASC_P']['PASC_FOR_P_TABLE']\n",
    "\n",
    "        format_df(df, '11.ASC y Divulgación', var_w11, writer,eh)\n",
    "\n",
    "        var_w11 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # Proceso de apropiación social del conocimiento resultado del trabajo conjunto \n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['ASC_P']['PASC_P']['PASC_TRA_P_TABLE']), 'Nombre','Nombre del Proceso de apropiación social del conocimiento resultado del trabajo conjunto entre un Centro de Ciencia y un grupo de investigación')\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "        eh=DBEH['ASC_P']['PASC_P']['PASC_TRA_P_TABLE']\n",
    "\n",
    "        format_df(df, '11.ASC y Divulgación', var_w11, writer,eh)\n",
    "\n",
    "        var_w11 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "    # Nombre del Proceso de apropiación social del conocimiento para la generación de insumos de política pública y normatividad\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['ASC_P']['PASC_P']['PASC_GEN_P_TABLE']),'Nombre','Nombre del Proceso de apropiación social del conocimiento para la generación de insumos de política pública y normatividad')\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "        eh=DBEH['ASC_P']['PASC_P']['PASC_GEN_P_TABLE']\n",
    "\n",
    "        format_df(df, '11.ASC y Divulgación', var_w11, writer,eh)\n",
    "\n",
    "        var_w11 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    #Nombre del Proceso de apropiación social del conocimiento para el fortalecimiento de cadenas productivas\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['ASC_P']['PASC_P']['PASC_CAD_P_TABLE']),'Nombre', 'Nombre del Proceso de apropiación social del conocimiento para el fortalecimiento de cadenas productivas')\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "        eh=DBEH['ASC_P']['PASC_P']['PASC_CAD_P_TABLE']\n",
    "\n",
    "        format_df(df, '11.ASC y Divulgación', var_w11, writer,eh)\n",
    "\n",
    "        var_w11 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # Divulgacion\n",
    "    # Piezas digitales\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['ASC_P']['DC_P']['DC_CD_P_TABLE']),'Título del proyecto','Título del proyecto para la generación de piezas digitales')\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "        eh=DBEH['ASC_P']['DC_P']['DC_CD_P_TABLE']\n",
    "\n",
    "        format_df(df, '11.ASC y Divulgación', var_w11, writer,eh)\n",
    "\n",
    "        var_w11 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # textuales\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['ASC_P']['DC_P']['DC_CON_P_TABLE']),'Título del proyecto','Título del proyecto para la generación de piezas Textuales (incluyendo cartillas, periódicos, revistas, etc.), Producción de estrategias transmediáticas y Desarrollos web')\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "        eh=DBEH['ASC_P']['DC_P']['DC_CON_P_TABLE']\n",
    "\n",
    "        format_df(df, '11.ASC y Divulgación', var_w11, writer,eh)\n",
    "\n",
    "        var_w11 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # produccion estrategia trasmediatica\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['ASC_P']['DC_P']['DC_TRA_P_TABLE']), 'Título del proyecto','Título del proyecto estrategia trasmediatica')\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "        eh=DBEH['ASC_P']['DC_P']['DC_TRA_P_TABLE']\n",
    "\n",
    "        format_df(df, '11.ASC y Divulgación', var_w11, writer,eh)\n",
    "\n",
    "        var_w11 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # desarrollo web\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['ASC_P']['DC_P']['DC_DES_P_TABLE']),'Título del proyecto','Título del proyecto desarrollo web')\n",
    "\n",
    "        eh=DBEH['ASC_P']['DC_P']['DC_DES_P_TABLE']\n",
    "\n",
    "        format_df(df, '11.ASC y Divulgación', var_w11, writer,eh)\n",
    "\n",
    "        var_w11 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # --- --- --- -- w11 -- -- -- -- -- -- --\n",
    "\n",
    "    # ---------------w12--------------------\n",
    "\n",
    "    # FRH\n",
    "\n",
    "    var_w12 = 0\n",
    "\n",
    "    # tesis doctorado\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['FRH_P']['TES_DOC_P']['TD_P_TABLE']), 'Título','Título de la tesis de doctorado')  ### ,veh = 2\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='FRH_P',startrow = var_rh)\n",
    "\n",
    "        eh=DBEH['FRH_P']['TES_DOC_P']['TD_P_TABLE']\n",
    "\n",
    "        format_df(df, '12.Formación y programas', var_w12, writer, eh,veh=2)\n",
    "\n",
    "        var_w12 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # tesis maestria\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['FRH_P']['TES_MAST_P']['TM_P_TABLE']),'Título','Título del trabajo de grado de maestría') ### veh = 2\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='FRH_P',startrow = var_rh)\n",
    "\n",
    "        eh=DBEH['FRH_P']['TES_MAST_P']['TM_P_TABLE']\n",
    "\n",
    "        format_df(df, '12.Formación y programas',var_w12, writer,eh,veh=2)\n",
    "\n",
    "        var_w12 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "    # tesis pregrado\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['FRH_P']['TES_PREG_P']['TP_P_TABLE']),'Título','Título del trabajo de grado de pregrado') ### veh = 2\n",
    "\n",
    "        #df.to_excel(writer,sheet_name='FRH_P',startrow = var_rh)\n",
    "\n",
    "        eh=DBEH['FRH_P']['TES_PREG_P']['TP_P_TABLE']\n",
    "\n",
    "        format_df(df, '12.Formación y programas',var_w12, writer,eh,veh = 2)\n",
    "\n",
    "        var_w12 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # asesoria programa academico\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['FRH_P']['ASE_PRG_ACA_P']['APGA_P_TABLE']),'Tipo','Nombre programa academico creado') \n",
    "\n",
    "        eh=DBEH['FRH_P']['ASE_PRG_ACA_P']['APGA_P_TABLE']\n",
    "\n",
    "        format_df(df, '12.Formación y programas', var_w12, writer,eh)\n",
    "\n",
    "        var_w12 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # asesoria creacion de cursos\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['FRH_P']['ASE_CRE_CUR_P']['ACC_P_TABLE']),'Tipo','Nombre curso creado')\n",
    "\n",
    "        eh=DBEH['FRH_P']['ASE_CRE_CUR_P']['ACC_P_TABLE']\n",
    "\n",
    "        format_df(df, '12.Formación y programas', var_w12, writer,eh)\n",
    "\n",
    "        var_w12 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "\n",
    "    # programa ondas\n",
    "    try:\n",
    "        df=rename_col(clean_df(DBG['FRH_P']['ASE_PRG_ONDAS_P']['APO_P_TABLE']),'Integrante','Integrante programa ondas')\n",
    "\n",
    "        eh=DBEH['FRH_P']['ASE_PRG_ONDAS_P']['APO_P_TABLE']\n",
    "\n",
    "        format_df(df, '12.Formación y programas', var_w12, writer,eh)\n",
    "\n",
    "        var_w12 += df.shape[0] + 3\n",
    "\n",
    "    except KeyError as e:\n",
    "\n",
    "        pass\n",
    "    #----------------w12---------------------------\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.get(str(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm_range' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-7dfccab0afb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm_range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'm_range' is not defined"
     ]
    }
   ],
   "source": [
    "m_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#except:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NC_P', 'ART_IMP_P'],\n",
       " ['NC_P', 'ART_ELE_P'],\n",
       " ['NC_P', 'CAP_LIB_P'],\n",
       " ['NC_P', 'PAT_P'],\n",
       " ['FRH_P', 'TES_MAST_P'],\n",
       " ['FRH_P', 'TES_PREG_P']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_prods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAT_P_TABLE': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DBG['NC_P']['PAT_P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBG['NC_P']['PAT_P']=get_df('NC_P','PAT_P',id_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(DBG['NC_P']['PAT_P'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Título del artículo</th>\n",
       "      <th>Año de presentación</th>\n",
       "      <th>Mes de presentación</th>\n",
       "      <th>Volumen revista</th>\n",
       "      <th>Página inicial</th>\n",
       "      <th>Página final</th>\n",
       "      <th>ISSN</th>\n",
       "      <th>Categoría</th>\n",
       "      <th>Última actualización</th>\n",
       "      <th>Revisar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Paper strip-embedded graphene quantum dots: a screening device fully operated by a smartphone</td>\n",
       "      <td>2017</td>\n",
       "      <td>Enero</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>20452322</td>\n",
       "      <td>ART_A1</td>\n",
       "      <td>2021-04-27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Synthesis of graphene-coated CNT-supported metal nanoparticles as multifunctional hybrid material</td>\n",
       "      <td>2017</td>\n",
       "      <td>Enero</td>\n",
       "      <td>111</td>\n",
       "      <td>393</td>\n",
       "      <td>401</td>\n",
       "      <td>00086223</td>\n",
       "      <td>ART_A1</td>\n",
       "      <td>2021-04-27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Molecular Techniques for the Detection of Organisms in Aquatic Environments, with Emphasis on Harmful Algal Bloom Species</td>\n",
       "      <td>2017</td>\n",
       "      <td>Octubre</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>14248220</td>\n",
       "      <td>ART_A2</td>\n",
       "      <td>2021-04-27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Bioluminescent nanopaper for rapid screening of toxic substances</td>\n",
       "      <td>2018</td>\n",
       "      <td>Enero</td>\n",
       "      <td>11</td>\n",
       "      <td>114</td>\n",
       "      <td>125</td>\n",
       "      <td>19980124</td>\n",
       "      <td>ART_A1</td>\n",
       "      <td>2021-04-27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0  \\\n",
       "1          1   \n",
       "2          2   \n",
       "3          3   \n",
       "4          4   \n",
       "\n",
       "                                                                                                         Título del artículo  \\\n",
       "1                              Paper strip-embedded graphene quantum dots: a screening device fully operated by a smartphone   \n",
       "2                          Synthesis of graphene-coated CNT-supported metal nanoparticles as multifunctional hybrid material   \n",
       "3  Molecular Techniques for the Detection of Organisms in Aquatic Environments, with Emphasis on Harmful Algal Bloom Species   \n",
       "4                                                           Bioluminescent nanopaper for rapid screening of toxic substances   \n",
       "\n",
       "  Año de presentación Mes de presentación Volumen revista Página inicial  \\\n",
       "1                2017               Enero               7              1   \n",
       "2                2017               Enero             111            393   \n",
       "3                2017             Octubre              17              1   \n",
       "4                2018               Enero              11            114   \n",
       "\n",
       "  Página final      ISSN Categoría Última actualización Revisar  \n",
       "1            9  20452322    ART_A1           2021-04-27     NaN  \n",
       "2          401  00086223    ART_A1           2021-04-27     NaN  \n",
       "3           22  14248220    ART_A2           2021-04-27     NaN  \n",
       "4          125  19980124    ART_A1           2021-04-27     NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DBG['NC_P']['ART_IMP_P']['ART_P_TABLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([DBG]).to_pickle('kk.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_pickle('kk.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWDBG=df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-cf31a2973d13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNEWDBG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NC_P'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#['ART_IMP_P']['ART_P_TABLE']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to_dict'"
     ]
    }
   ],
   "source": [
    ".to_dict()#['ART_IMP_P']['ART_P_TABLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Título del artículo</th>\n",
       "      <th>Año de presentación</th>\n",
       "      <th>Mes de presentación</th>\n",
       "      <th>Volumen revista</th>\n",
       "      <th>Página inicial</th>\n",
       "      <th>Página final</th>\n",
       "      <th>ISSN</th>\n",
       "      <th>Categoría</th>\n",
       "      <th>Última actualización</th>\n",
       "      <th>Revisar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Paper strip-embedded graphene quantum dots: a screening device fully operated by a smartphone</td>\n",
       "      <td>2017</td>\n",
       "      <td>Enero</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>20452322</td>\n",
       "      <td>ART_A1</td>\n",
       "      <td>2021-04-27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Synthesis of graphene-coated CNT-supported metal nanoparticles as multifunctional hybrid material</td>\n",
       "      <td>2017</td>\n",
       "      <td>Enero</td>\n",
       "      <td>111</td>\n",
       "      <td>393</td>\n",
       "      <td>401</td>\n",
       "      <td>00086223</td>\n",
       "      <td>ART_A1</td>\n",
       "      <td>2021-04-27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Molecular Techniques for the Detection of Organisms in Aquatic Environments, with Emphasis on Harmful Algal Bloom Species</td>\n",
       "      <td>2017</td>\n",
       "      <td>Octubre</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>14248220</td>\n",
       "      <td>ART_A2</td>\n",
       "      <td>2021-04-27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Bioluminescent nanopaper for rapid screening of toxic substances</td>\n",
       "      <td>2018</td>\n",
       "      <td>Enero</td>\n",
       "      <td>11</td>\n",
       "      <td>114</td>\n",
       "      <td>125</td>\n",
       "      <td>19980124</td>\n",
       "      <td>ART_A1</td>\n",
       "      <td>2021-04-27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0  \\\n",
       "1          1   \n",
       "2          2   \n",
       "3          3   \n",
       "4          4   \n",
       "\n",
       "                                                                                                         Título del artículo  \\\n",
       "1                              Paper strip-embedded graphene quantum dots: a screening device fully operated by a smartphone   \n",
       "2                          Synthesis of graphene-coated CNT-supported metal nanoparticles as multifunctional hybrid material   \n",
       "3  Molecular Techniques for the Detection of Organisms in Aquatic Environments, with Emphasis on Harmful Algal Bloom Species   \n",
       "4                                                           Bioluminescent nanopaper for rapid screening of toxic substances   \n",
       "\n",
       "  Año de presentación Mes de presentación Volumen revista Página inicial  \\\n",
       "1                2017               Enero               7              1   \n",
       "2                2017               Enero             111            393   \n",
       "3                2017             Octubre              17              1   \n",
       "4                2018               Enero              11            114   \n",
       "\n",
       "  Página final      ISSN Categoría Última actualización Revisar  \n",
       "1            9  20452322    ART_A1           2021-04-27     NaN  \n",
       "2          401  00086223    ART_A1           2021-04-27     NaN  \n",
       "3           22  14248220    ART_A2           2021-04-27     NaN  \n",
       "4          125  19980124    ART_A1           2021-04-27     NaN  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEWDBG['NC_P']['ART_IMP_P']['ART_P_TABLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(DB)==len(LP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
