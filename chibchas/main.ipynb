{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incorrect-retailer",
   "metadata": {},
   "source": [
    "# Institulac "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "brilliant-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import helium as h\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "boring-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_rows\",100)\n",
    "#pd.set_option(\"display.max_columns\",100)\n",
    "pd.set_option(\"max_colwidth\",1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "everyday-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DICT CAT-PRODS-TAB\n",
    "with open('dict_tables.json') as file_json:\n",
    "    dict_tables=json.loads(file_json.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "provincial-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(df,cod_gr):\n",
    "    \n",
    "    info= {\n",
    "        'Nombre_Grupo' : df['Nombre Grupo'].dropna().iloc[0],\n",
    "\n",
    "        'Nombre_Lider' : df['Nombre Líder'].dropna().iloc[0],\n",
    "\n",
    "        'CCRG Grupo'  : cod_gr\n",
    "    }\n",
    "    \n",
    "    dfi = pd.DataFrame(info, index=[0])\n",
    "  \n",
    "    \n",
    "    return dfi\n",
    "\n",
    "# extra headers by products\n",
    "DBEH = {\n",
    "    \n",
    "    'INFO_GROUP': 'TABLE',\n",
    "    'MEMBERS':['Identificación', 'Nacionalidad', 'Tiene afiliación con UdeA', 'Si no tiene afiliación UdeA diligencie el nombre de la Institución','Nro. Horas de dedicación semanales que avala el Coordinador de grupo'], # 2\n",
    "       \n",
    "    'NC_P': {'ART_IMP_P': {'ART_P_TABLE':['URL','DOI','Si no tiene URL o DOI agregue una evidencia en el repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "             'ART_ELE_P': {'ART_E_P_TABLE':['URL','DOI','Si no tiene URL o DOI agregue una evidencia en el repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "             'LIB_P':     {'LIB_P_TABLE':['Proyecto de investigación del cual se derivó el libro (Código-Título)','Financiador(es) del proyecto del cual se derivó el libro', 'Financiador(es) de la publicación','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "             'CAP_LIB_P': {'CAP_LIB_P_TABLE':['Proyecto de investigación del cual se derivó el libro que contiene el capítulo (Código-Título)','Financiador del proyecto del cual se derivó el libro que contiene el capítulo','Financiador de la publicación','Autores','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "             'NOT_CIE_P': {'NOT_CIE_P_TABLE':['URL','DOI','Si no tiene URL o DOI genere una evidencia en el repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "             'PAT_P':     {'PAT_P_TABLE':['Autores', 'Examen de fondo favorable','Examen preliminar internacional favorable','Adjunta opiniones escritas de la bUsqueda internacional','Contrato de explotación','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']}, #  1 2 3 -1\n",
    "             'PRD_INV_ART_P': {'PAAD_P_TABLE':['Autores','Tiene certificado institucional de la obra','Tiene certificado de la entidad que convoca al evento en el que participa','Tiene certificado de la entidad que convoca al premio en el que obtiene','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']}, # 1 2 3 -1\n",
    "             'VAR_VEG_P':     {'VV_P_TABLE':['Autores','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "             'VAR_ANI_P':     {'VA_P_TABLE':['Autores','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "             'RAZ_PEC_P':     {'RAZ_PEC_P_TABLE':['Autores','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "             'TRA_FIL_P': {'TRA_FIL_P_TABLE':['Proyecto de investigación del cual se derivó el libro (Código-Título)','Financiador(es) del proyecto del cual se derivó el libro','Financiador(es) de la publicación','Autores','Citas recibidas (si tiene)','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']}\n",
    "            },\n",
    "     'DTI_P': {'DIS_IND_P': {'DI_P_TABLE':['Autores','Contrato (si aplica)','Nombre comercial (si aplica)','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'CIR_INT_P': {'ECI_P_TABLE':['Autores','Contrato (si aplica)','Nombre comercial (si aplica)','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'SOFT_P': {'SF_P_TABLE':['Autores','Contrato (si aplica)','Nombre comercial (si aplica)','TRL','Agregue la evidencia verificada al repositorio digital y copie el link del archivo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'NUTRA_P': {'NUTRA_P_TABLE':['Autores','Agregue la evidencia verificada al repositorio digital y copie el link del archivo en este campo','¿El producto cumple con los requisitos para ser avalado?']}, # add\n",
    "              'COL_CIENT_P': {'COL_CIENT_P_TABLE':['Autores','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo', '¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'REG_CIENT_P': {'REG_CIENT_P_TABLE':['Autores','Contrato licenciamiento (si aplica)','Agregue las evidencias verificadas al repositorio digital y copie el link del archivo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'PLT_PIL_P': {'PP_P_TABLE':['Autores','Agregue la evidencia verificada al repositorio digital y copie el link del archivo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'PRT_IND_P': {'PI_P_TABLE':['Autores','Nombre comercial (si aplica)','TRL','Agregue la evidencia verificada al repositorio digital y copie el link del archivo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'SEC_IND_P': {'SE_P_TABLE':['Autores','Agregue la evidencia verificada al repositorio digital y copie el link del archivo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'PROT_VIG_EPID_P': {'PROT_VIG_EPID_P_TABLE':['Autores','Agregue la evidencia verificada al repositorio digital y copie el link del archivo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'EMP_BSE_TEC_P': {'EBT_P_TABLE':['Autores','Agregue la evidencia verificada al repositorio digital y copie el link del archivo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'EMP_CRE_CUL_P': {'ICC_P_TABLE':['Autores','Agregue la evidencia verificada al repositorio digital y copie el link del archivo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'INN_GES_EMP_P': {'IG_P_TABLE':['Autores','Contrato (si aplica)','Nombre comercial (si aplica)','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'INN_PROC_P': {'IPP_P_TABLE':['Autores','Contrato (si aplica)','Nombre comercial (si aplica)','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'REG_NORM_REGL_LEG_P': {'RNR_P_TABLE':['Autores','Contrato (si aplica)','Convenio (si aplica)','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'CONP_TEC_P': {'CONP_TEC_P_TABLE':['Autores','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'REG_AAD_P': {'AAAD_P_TABLE':['Autores','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'SIG_DIS_P': {'SD_P_TABLE':['Autores','Contrato licenciamiento (si aplica)','Agregue las evidencias verificadas al repositorio digital y copie el link del archivo en este campo','¿El producto cumple con los requisitos para ser avalado?']}\n",
    "              },\n",
    "    'ASC_P': {'GEN_CONT_IMP_P': {'GC_I_P_TABLE_5':['Autores','Citas recibidas (si tiene)','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'PASC_P': {'PASC_FOR_P_TABLE':['Proyecto/Código','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?'],\n",
    "               'PASC_TRA_P_TABLE':['Proyecto/Código','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?'],\n",
    "               'PASC_GEN_P_TABLE':['Proyecto/Código','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?'],\n",
    "               'PASC_CAD_P_TABLE':['Proyecto/Código','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'DC_P': {'DC_CD_P_TABLE':['Proyecto/Código','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?'],\n",
    "               'DC_CON_P_TABLE':['Medio de verificación','Proyecto/Código','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?'],\n",
    "               'DC_TRA_P_TABLE':['Medio de verificación','Proyecto/Código','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?'],\n",
    "               'DC_DES_P_TABLE':['Medio de verificación','Proyecto/Código','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']}},\n",
    "    \n",
    "    'FRH_P': {'TES_DOC_P': {'TD_P_TABLE':['Número de cédula del graduado','¿La fecha fin coincide con la fecha de grado del estudiante?','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},  # 1 -1\n",
    "              'TES_MAST_P': {'TM_P_TABLE':['Número de cédula del graduado','¿La fecha fin coincide con la fecha de grado del estudiante?','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']}, # 1 -1\n",
    "              'TES_PREG_P': {'TP_P_TABLE':['Número de cédula del graduado','¿La fecha fin coincide con la fecha de grado del estudiante?','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']}, # 1 -1\n",
    "              'ASE_PRG_ACA_P': {'APGA_P_TABLE':['Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'ASE_CRE_CUR_P': {'ACC_P_TABLE':['Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']},\n",
    "              'ASE_PRG_ONDAS_P': {'APO_P_TABLE':['Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']}},\n",
    "    'NC' : {'LIB' : {'LIB_T_AVAL_TABLE': ['Proyecto de investigación del cual se derivó el libro (Código-Título)','Financiador(es) del proyecto del cual se derivó el libro', 'Financiador(es) de la publicación','Autores','Citas recibidas (si tiene)','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']}, \n",
    "            'CAP_LIB':{'CAP_LIB_T_AVAL_TABLE':['Proyecto de investigación del cual se derivó el libro que contiene el capítulo (Código-Título)','Financiador del proyecto del cual se derivó el libro que contiene el capítulo','Financiador de la publicación','Autores','Citas recibidas (si tiene)','Agregue las evidencias verificadas al repositorio digital y genere un hipervínculo en este campo','¿El producto cumple con los requisitos para ser avalado?']}}\n",
    "}\n",
    "\n",
    "d = {\n",
    "                '1': 'C',\n",
    "                '2': 'D',\n",
    "                '3': 'E',\n",
    "                '4': 'F',\n",
    "                '5': 'G',\n",
    "                '6': 'H',\n",
    "                '7': 'I',\n",
    "                '8': 'J',\n",
    "                '9': 'K',\n",
    "                '10': 'L',\n",
    "                '11': 'M',\n",
    "                '12': 'N',\n",
    "                '13': 'O',\n",
    "                '14': 'P',\n",
    "                '15': 'Q',\n",
    "                '16': 'R',\n",
    "                '17': 'S',\n",
    "                '18': 'T',\n",
    "                '19': 'U',\n",
    "                '20': 'V'\n",
    "}\n",
    "\n",
    "def clean_df(df):\n",
    "    'remove innecesari collums'\n",
    "    c=[x for x in df.columns if x.find('Unnamed:') == -1 and  x.find('Revisar') == -1 and x.find('Avalar integrante') == -1]\n",
    "    dfc=df[c]\n",
    "    return dfc\n",
    "\n",
    "def rename_col(df,colr,colf):\n",
    "    df.rename(columns = {colr: colf,}, inplace = True)\n",
    "    return df\n",
    "\n",
    "# WORKSHEET 4 - 12.\n",
    "def format_df(df, sheet_name, start_row, writer,eh, veh = None):\n",
    "    'format headers'\n",
    "    \n",
    "    df.to_excel(writer,sheet_name, startrow = start_row+1, startcol=2,index = False)\n",
    "\n",
    "    # Get the xlsxwriter workbook and worksheet objects.\n",
    "    worksheet = writer.sheets[sheet_name]\n",
    "    \n",
    "         \n",
    "    merge_format = workbook.add_format({\n",
    "    'bold': 1,\n",
    "    'border':1,\n",
    "    'text_wrap': True,    \n",
    "    'align': 'center',\n",
    "    'valign': 'vcenter',\n",
    "    'font_color': 'blue'})\n",
    "    \n",
    "    #form merge cells\n",
    "    #start,end = 1,df.shape[1]\n",
    "    if not df.empty:\n",
    "        start,end = 1,df.shape[1]\n",
    "    else:\n",
    "        start,end = 1,1\n",
    "\n",
    "    m_range = d.get(str(start)) + str(start_row + 1) + ':' + d.get(str(end)) + str(start_row +1)\n",
    "\n",
    "    worksheet.merge_range(m_range, 'Información suministrada por la Vicerrectoría de Investigación', merge_format)\n",
    "    \n",
    "    # for merge headers cells\n",
    "    _m_range = d.get(str(end+1)) + str(start_row +1) + ':' +  d.get(str(end+len(eh))) + str(start_row +1)\n",
    "    \n",
    "    worksheet.merge_range(_m_range, 'Validación del Centro, Instituto o Corporación', merge_format)\n",
    "        \n",
    "    worksheet.set_row_pixels(start_row+1, 120)\n",
    "    #worksheet.set_column('C:C',30,general)\n",
    "    \n",
    "    # SET COLUMS FORMAT BY SHEET\n",
    "    if sheet_name=='3.Integrantes grupo':\n",
    "        worksheet.set_column('A:A', 5)\n",
    "        worksheet.set_column('B:B', 2)\n",
    "        worksheet.set_column('D:K',15,general)\n",
    "    \n",
    "    if sheet_name=='4.ART y N':\n",
    "        worksheet.set_column('A:A', 5)\n",
    "        worksheet.set_column('B:B', 2)\n",
    "        worksheet.set_column('C:C',20,general)\n",
    "        worksheet.set_column('M:O',20, general)\n",
    "     \n",
    "\n",
    "    if sheet_name=='5.LIB y LIB_FOR':\n",
    "        worksheet.set_column('A:A', 5)\n",
    "        worksheet.set_column('B:B', 2)\n",
    "        worksheet.set_column('C:C',20,general)\n",
    "        worksheet.set_column('I:P',20,general)\n",
    "\n",
    "    if sheet_name=='6.CAP':\n",
    "        worksheet.set_column('A:A', 5)\n",
    "        worksheet.set_column('B:B', 2)\n",
    "        worksheet.set_column('C:C',20,general)\n",
    "        worksheet.set_column('D:H',10,general)\n",
    "        worksheet.set_column('I:K',18,general)\n",
    "        worksheet.set_column('J:P',20,general)\n",
    "\n",
    "    if sheet_name=='7.Patente_Variedades':\n",
    "        worksheet.set_column('A:A', 5)\n",
    "        worksheet.set_column('B:B', 2)\n",
    "        worksheet.set_column('C:C',20,general)\n",
    "        worksheet.set_column('D:I',10,general)\n",
    "        worksheet.set_column('J:K',20,general)\n",
    "        worksheet.set_column('L:S',20,general)\n",
    "\n",
    "    if sheet_name=='8.AAD':\n",
    "        worksheet.set_column('A:A', 5)\n",
    "        worksheet.set_column('B:B', 2)\n",
    "        worksheet.set_column('C:C',20,general)\n",
    "        worksheet.set_column('F:K',10,general)\n",
    "        worksheet.set_column('L:P',25,general)\n",
    "\n",
    "    if sheet_name=='9.Tecnológico':\n",
    "        worksheet.set_column('A:A', 5)\n",
    "        worksheet.set_column('B:B', 2)\n",
    "        worksheet.set_column('C:C',20,general)\n",
    "        worksheet.set_column('D:I',10,general)\n",
    "        worksheet.set_column('J:S',18,general)\n",
    "\n",
    "    if sheet_name=='10.Empresarial':\n",
    "        worksheet.set_column('A:A', 5)\n",
    "        worksheet.set_column('B:B', 2)\n",
    "        worksheet.set_column('C:C',20,general)\n",
    "        worksheet.set_column('D:H',10,general)\n",
    "        worksheet.set_column('I:N',20,general)\n",
    "\n",
    "    if sheet_name=='11.ASC y Divulgación':\n",
    "        worksheet.set_column('A:A', 5)\n",
    "        worksheet.set_column('B:B', 2)\n",
    "        worksheet.set_column('C:C',28,general)\n",
    "        worksheet.set_column('I:I',15,general)\n",
    "        worksheet.set_column('J:N',20,general)\n",
    "\n",
    "    if sheet_name=='12.Formación y programas':\n",
    "        worksheet.set_column('A:A', 5)\n",
    "        worksheet.set_column('B:B', 2)\n",
    "        worksheet.set_column('C:C',25,general)\n",
    "        worksheet.set_column('D:G',10,general)\n",
    "        worksheet.set_column('L:O',15,general)\n",
    "        worksheet.set_column('N:N',20,general)\n",
    "        \n",
    "    worksheet.write(start_row+1, 0, 'VoBo de VRI', merge_format)\n",
    "    # Add a header format.\n",
    "    \n",
    "    fmt_header = workbook.add_format({\n",
    "        'bold': True,\n",
    "        'align': 'center',    \n",
    "        'text_wrap': True,\n",
    "        'valign': 'vcenter',\n",
    "        'fg_color': '#33A584',\n",
    "        'font_color': '#FFFFFF',\n",
    "        'border': 1})\n",
    "    \n",
    "    # Write the column headers with the defined format.\n",
    "    for col_num, value in enumerate(df.columns.values):\n",
    "        worksheet.write(start_row+1, col_num + 2, value, fmt_header)\n",
    "        \n",
    "    # write extra headers\n",
    "    for col_num, value in enumerate(eh):\n",
    "        worksheet.write(start_row+1, col_num + df.shape[1] + 2, value, fmt_header)\n",
    "        \n",
    "    v_range = 'A' + str(start_row +3) + ':' + 'A' + str(df.shape[0] + start_row +2)\n",
    "    worksheet.data_validation(v_range,{'validate': 'list',\n",
    "                                  'source': ['Sí', 'No']})\n",
    "    \n",
    "    \n",
    "    if sheet_name !='3.Integrantes grupo':\n",
    "        \n",
    "        v_range = d.get(str(end+len(eh))) + str(start_row +3) + ':' + d.get(str(end+len(eh))) + str(df.shape[0] + start_row +2)\n",
    "        worksheet.data_validation(v_range,{'validate': 'list',\n",
    "                                  'source': ['Sí', 'No']})\n",
    "    \n",
    "    # Integrantes\n",
    "    if veh == 0:\n",
    "        v_range = d.get(str(end+len(eh)-2)) + str(start_row +3) + ':' + d.get(str(end+len(eh)-2)) + str(df.shape[0] + start_row +2)\n",
    "        worksheet.data_validation(v_range,{'validate': 'list',\n",
    "                                'source': ['Sí', 'No']})  \n",
    "    # patentes\n",
    "    if veh == 1 :\n",
    "        v_range = d.get(str(end+len(eh)-3)) + str(start_row +3) + ':' + d.get(str(end+len(eh)-3)) + str(df.shape[0] + start_row +2)\n",
    "        worksheet.data_validation(v_range,{'validate': 'list',\n",
    "                                  'source': ['Sí', 'No']})\n",
    "        v_range = d.get(str(end+len(eh)-4)) + str(start_row +3) + ':' + d.get(str(end+len(eh)-4)) + str(df.shape[0] + start_row +2)\n",
    "        worksheet.data_validation(v_range,{'validate': 'list',\n",
    "                                  'source': ['Sí', 'No']})\n",
    "        v_range = d.get(str(end+len(eh)-5)) + str(start_row +3) + ':' + d.get(str(end+len(eh)-5)) + str(df.shape[0] + start_row +2)\n",
    "        worksheet.data_validation(v_range,{'validate': 'list',\n",
    "                                  'source': ['Sí', 'No']})\n",
    "    if veh ==2:\n",
    "        v_range = d.get(str(end+len(eh)-2)) + str(start_row +3) + ':' + d.get(str(end+len(eh)-2)) + str(df.shape[0] + start_row +2)\n",
    "        worksheet.data_validation(v_range,{'validate': 'list',\n",
    "                                  'source': ['Sí', 'No']})\n",
    "        \n",
    "    if veh == 3:\n",
    "        v_range = d.get(str(end+len(eh)-2)) + str(start_row +3) + ':' + d.get(str(end+len(eh)-3)) + str(df.shape[0] + start_row +2)\n",
    "        worksheet.data_validation(v_range,{'validate': 'list',\n",
    "                                  'source': ['Sí', 'No']})\n",
    "        v_range = d.get(str(end+len(eh)-3)) + str(start_row +3) + ':' + d.get(str(end+len(eh)-4)) + str(df.shape[0] + start_row +2)\n",
    "        worksheet.data_validation(v_range,{'validate': 'list',\n",
    "                                  'source': ['Sí', 'No']})\n",
    "        v_range = d.get(str(end+len(eh)-4)) + str(start_row +3) + ':' + d.get(str(end+len(eh)-5)) + str(df.shape[0] + start_row +2)\n",
    "        worksheet.data_validation(v_range,{'validate': 'list',\n",
    "                                  'source': ['Sí', 'No']})\n",
    "        \n",
    "        \n",
    "##### WORKSHEET 2\n",
    "def format_info(df, writer, sheet_name):\n",
    "    \n",
    "    '''format worksheet'''\n",
    "    \n",
    "    workbook=writer.book\n",
    "    \n",
    "    normal=workbook.add_format({'font_size':12,'text_wrap':True})\n",
    "    \n",
    "    merge_format = workbook.add_format({\n",
    "    'bold': 1,\n",
    "    'border':1,\n",
    "    'text_wrap': True,    \n",
    "    'align': 'center',\n",
    "    'valign': 'vcenter',\n",
    "    'font_color': 'black'})\n",
    "    \n",
    "    fmt_header = workbook.add_format({\n",
    "        'align': 'center',    \n",
    "        'text_wrap': True,\n",
    "        'valign': 'top',\n",
    "        'fg_color': '#33A584',\n",
    "        'font_color': '#FFFFFF',\n",
    "        'border': 1})\n",
    "    \n",
    "    # write df\n",
    "    start_row = 6\n",
    "    start_col = 3\n",
    "    \n",
    "    df.to_excel(writer, sheet_name, startrow =start_row, startcol=start_col,index = False)\n",
    "\n",
    "    # get worksheet object\n",
    "    worksheet = writer.sheets[sheet_name]\n",
    "    \n",
    "    for col_num, value in enumerate(df.columns.values):\n",
    "        worksheet.write(start_row, col_num + 3, value, fmt_header)\n",
    "    \n",
    "    #Prepare image insertion: See → https://xlsxwriter.readthedocs.io/example_images.html\n",
    "    worksheet.set_column('A:A', 15)\n",
    "    worksheet.set_column('B:B', 15)\n",
    "    worksheet.insert_image('A1', 'img/udea.jpeg')\n",
    "    \n",
    "    # title 1 UNIVERSIDAD DE ANTIOQUIA\n",
    "    title = workbook.add_format({'font_size':16,'center_across':True})\n",
    "\n",
    "    # title 2 Vicerrectoria de Investigación\n",
    "    title2 = workbook.add_format({'font_size':16,'center_across':True})\n",
    "   \n",
    "    # sub title 2 datos identificacion contacto\n",
    "    title3 = workbook.add_format({'font_size':12,'center_across':True})\n",
    "    \n",
    "    # merge d1:f1\n",
    "    worksheet.merge_range('D1:F1', 'UNIVERSIDAD DE ANTIOQUIA', title)\n",
    "        \n",
    "    # merge d2:f2\n",
    "    worksheet.merge_range('D2:F2', ' Vicerrectoria de Investigación', title2)\n",
    "    \n",
    "    # merge d3:f3\n",
    "    worksheet.merge_range('D3:F3', ' Datos de identificación y contacto', title3)\n",
    "    \n",
    "    # D5: F5\n",
    "    worksheet.merge_range('D5:E5','Número inscripcion a la convocatoria:',merge_format)\n",
    "    worksheet.write('F5','#',merge_format)\n",
    "    \n",
    "    # d6:f6\n",
    "    worksheet.merge_range('D6:F6','Identificación del Grupo',merge_format)\n",
    "        \n",
    "    # d9:f9\n",
    "    worksheet.merge_range('D10:F10','Identificación del Centro de Investigación',merge_format)\n",
    "    # write \n",
    "    a='Nombre del Centro, Instituto o Corporación'\n",
    "    worksheet.write('D11',a, fmt_header)\n",
    "    worksheet.set_column('D11:D11',30, fmt_header)\n",
    "    \n",
    "    b='Nombre completo del Jefe de Centro, Instituto o Corporación'\n",
    "    worksheet.write('E11',b, fmt_header) \n",
    "    worksheet.set_column('E11:E11',30, fmt_header)\n",
    "    \n",
    "    c='Email'\n",
    "    worksheet.write('F11',c, fmt_header) \n",
    "    worksheet.set_column('F11:F11',30, fmt_header)\n",
    "    \n",
    "    # d13:f13\n",
    "    worksheet.merge_range('D13:F13','Identificación de quien diligencia el formato',merge_format)\n",
    "    a='Nombre completo del encargado de diligenciar el formato'\n",
    "    worksheet.write('D14',a, fmt_header)\n",
    "    worksheet.set_column('D14:D14',30, normal)\n",
    "    \n",
    "    b='Email'\n",
    "    worksheet.write('E14',b, fmt_header) \n",
    "    worksheet.set_column('E14:E14',30, normal)\n",
    "    \n",
    "    c='Teléfono de contacto'\n",
    "    worksheet.write('F14',c, fmt_header) \n",
    "    worksheet.set_column('F14:F14',30, normal)\n",
    "\n",
    "# WORKSHEET 1\n",
    "def format_ptt(workbook):\n",
    "    \n",
    "    #Global variables\n",
    "    abstract_text='VERIFICACIÓN DE INFORMACIÓN PARA OTORGAR AVAL A LOS GRUPOS DE INVESTIGACIÓN  E INVESTIGADORES PARA SU PARTICIPACIÓN EN LA CONVOCATORIA 894 DE 2021 DE MINCIENCIAS'\n",
    "    instructions='''Los grupos de investigación e investigadores de la Universidad de Antioquia que deseen participar en la Convocatoria Nacional para el reconocimiento y medición de grupos de investigación, desarrollo tecnológico o de innovación y para el reconocimiento de investigadores del Sistema Nacional de Ciencia, Tecnología e Innovación - SNCTI, 894 de 2021, deben presentar la información actualizada en las plataformas CvLAC y GrupLAC validada por el Centro de Investigación en el presente formato, y respaldada en el repositorio digital de evidencias dispuesto para este fin, para la obtención del aval institucional por parte de la Vicerrectoría de Investigación. \n",
    "\n",
    "    La información a validar corresponde a los años 2019-2020 y aquella que entra en la ventana de observación y debe ser modificada según el Modelo de medición de grupos. La validación comprende:\n",
    "\n",
    "    1. Verificación de la vinculación de los integrantes a la Universidad de Antioquia y al grupo de investigación.  Diligenciar los campos solicitados. \n",
    "\n",
    "    2. Verificación de la producción de GNC, DTeI, ASC y FRH, en los campos habilitados en cada hoja de este formato. Las evidencias requeridas para los productos deben ser anexadas al repositorio digital asignado al grupo y se deben enlazar a cada producto.  \n",
    "\n",
    "    Este documento debe ser diligenciado en línea.\n",
    "\n",
    "    De antemano, la Vicerrectoría de Investigación agradece su participación en este ejercicio, que resulta de vital importancia para llevar a buen término la Convocatoria de Reconocimiento y Medición de Grupos de Investigación\n",
    "    '''\n",
    "    #Final part of the first sheet\n",
    "    datos=clean_df(pd.read_excel('https://github.com/restrepo/InstituLAC/raw/main/data/template_data.xlsx'))\n",
    "\n",
    "    #Capture xlsxwriter object \n",
    "    # IMPORTANT → workbook is the same object used in the official document at https://xlsxwriter.readthedocs.io\n",
    "    #workbook=writer.book\n",
    "    #***************\n",
    "    #Styles as explained in https://xlsxwriter.readthedocs.io\n",
    "    title=workbook.add_format({'font_size':28,'center_across':True})\n",
    "    subtitle=workbook.add_format({'font_size':24,'center_across':True})\n",
    "    abstract=workbook.add_format({'font_size':20,'center_across':True,'text_wrap':True})\n",
    "    normal=workbook.add_format({'font_size':12,'text_wrap':True})\n",
    "\n",
    "    #***************\n",
    "    #Creates the first work-sheet\n",
    "    #IMPORTANT → worksheet is the same object  used in the official document at https://xlsxwriter.readthedocs.io\n",
    "    worksheet=workbook.add_worksheet(\"1.Presentación\")\n",
    "    #Prepare image insertion: See → https://xlsxwriter.readthedocs.io/example_images.html\n",
    "    worksheet.set_column('A:A', 15)\n",
    "    worksheet.set_column('B:B', 15)\n",
    "    worksheet.insert_image('A1', 'img/udea.jpeg')\n",
    "    #Prepare text insertion: See  → https://xlsxwriter.readthedocs.io/example_images.html\n",
    "    worksheet.set_column('C:C', 140,general)\n",
    "    worksheet.set_row_pixels(0, 60)\n",
    "    #Texts\n",
    "    worksheet.write('C1', 'UNIVERSIDAD DE ANTIOQUIA',title)\n",
    "    worksheet.set_row_pixels(2, 60)\n",
    "    worksheet.write('C3', 'VICERRECTORÍA DE INVESTIGACIÓN',subtitle)\n",
    "    worksheet.set_row_pixels(5, 100)\n",
    "    worksheet.write('C6', abstract_text,abstract)\n",
    "    worksheet.set_row_pixels(8, 40)\n",
    "    worksheet.write('C9','PRESENTACIÓN DEL EJERCICIO',\n",
    "                    workbook.add_format({'font_size':18,'center_across':True}) )\n",
    "    worksheet.set_row_pixels(10, 320)\n",
    "    worksheet.write('C11',instructions,normal)\n",
    "    #*** ADD PANDAS DATAFRAME IN SPECIFIC POSITION ****\n",
    "    #Add a data Frame in some specific position. See → https://stackoverflow.com/a/43510881/2268280\n",
    "    #                                       See also → https://xlsxwriter.readthedocs.io/working_with_pandas.html\n",
    "    writer.sheets[\"1.Presentación\"]=worksheet\n",
    "    datos.to_excel(writer,sheet_name=\"1.Presentación\",startrow=12,startcol=2,index=False)\n",
    "    #**************************************************\n",
    "    #Fix columns heights for long text\n",
    "    worksheet.set_row_pixels(17, 40)\n",
    "    worksheet.set_row_pixels(18, 40)\n",
    "    worksheet.set_row_pixels(19, 40)\n",
    "    worksheet.set_row_pixels(20, 40)\n",
    "    worksheet.set_row_pixels(22, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "secondary-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "# login\n",
    "def login(user,password):\n",
    "    \n",
    "    browser = h.start_firefox('https://scienti.minciencias.gov.co/institulac2-war/')\n",
    "    sleep=0.8\n",
    "\n",
    "    #browser = h.start_firefox('https://scienti.minciencias.gov.co/institulac2-war/')\n",
    "    time.sleep(sleep)\n",
    "    h.click('Consulte Aquí')\n",
    "\n",
    "    time.sleep(sleep)\n",
    "    h.write('UNIVERSIDAD DE ANTIOQUIA',into='Digite el nombre de la Institución') # name ins\n",
    "\n",
    "    time.sleep(sleep)\n",
    "    h.click('Buscar')\n",
    "\n",
    "    time.sleep(sleep)\n",
    "    h.click(browser.find_element_by_id('list_instituciones'))\n",
    "\n",
    "    time.sleep(sleep)\n",
    "\n",
    "    time.sleep(sleep)\n",
    "    h.select('seleccione una','UNIVERSIDAD DE ANTIOQUIA') # name_ins\n",
    "\n",
    "    time.sleep(sleep)\n",
    "    h.write(user,into='Usuario')                  # user\n",
    "\n",
    "    time.sleep(sleep)\n",
    "    h.write(password, into='Contraseña')                # passw\n",
    "\n",
    "    time.sleep(sleep)\n",
    "    h.click(h.Button('Ingresar'))\n",
    "\n",
    "    # cookie injection\n",
    "    time.sleep(sleep)\n",
    "    # implementation cookie injection\n",
    "\n",
    "    # get current cookie and store\n",
    "    new_cookie=browser.get_cookies()[0]\n",
    "\n",
    "    # create new_cookie with time_expire\n",
    "    time_expire = (datetime(2022,1,1) - datetime(1970,1,1)).total_seconds()\n",
    "    new_cookie['expiry'] = int(time_expire)\n",
    "\n",
    "    # delete cookie sites\n",
    "    browser.delete_all_cookies()\n",
    "\n",
    "    # add new cookie\n",
    "    browser.add_cookie(new_cookie)\n",
    "    \n",
    "    return browser\n",
    "\n",
    "    # -- end login --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "touched-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dfg(browser):\n",
    "    sleep=0.8\n",
    "    # navigation 1\n",
    "    time.sleep(sleep)\n",
    "    h.click('Aval')\n",
    "\n",
    "    time.sleep(sleep)\n",
    "    h.click('Avalar grupos')\n",
    "\n",
    "    time.sleep(sleep)\n",
    "    h.click('Grupos Avalados')\n",
    "\n",
    "    # list of total groups\n",
    "    #select max results per page\n",
    "    h.wait_until(h.Text('Ver Reporte').exists)\n",
    "    h.click(browser.find_element_by_xpath('//table[@id=\"grupos_avalados\"]//select[@name=\"maxRows\"]'))\n",
    "\n",
    "    time.sleep(sleep)\n",
    "    h.select(browser.find_element_by_xpath('//table[@id=\"grupos_avalados\"]//select[@name=\"maxRows\"]'),'100')\n",
    "\n",
    "    # catch 1: groups info [name, lider, cod,  link to producs]  \n",
    "    # schema\n",
    "    # empty df\n",
    "    # select max items per page\n",
    "    # while until end\n",
    "    # try:\n",
    "        # catch table\n",
    "        # preproces table\n",
    "        # catch urls\n",
    "        # add url colums\n",
    "        # add df\n",
    "        # click next page -> raise error\n",
    "    # except Nosuchelement:\n",
    "        # break\n",
    "\n",
    "    # catch 1: list of groups\n",
    "    dfg=pd.DataFrame()\n",
    "    cont=True\n",
    "\n",
    "    while cont:\n",
    "\n",
    "        try:\n",
    "            # catch source\n",
    "            time.sleep(sleep)\n",
    "            source_g=browser.page_source\n",
    "\n",
    "            # catch table\n",
    "            time.sleep(sleep)\n",
    "            df=pd.read_html(source_g, attrs={\"id\":\"grupos_avalados\"}, header=2)[0]\n",
    "\n",
    "            # and preprocces it\n",
    "            c=[x for x in df.columns if x.find('Unnamed:') == -1]\n",
    "            dfgp=df[c][1:-1]\n",
    "            print(dfgp.columns,dfgp.shape)\n",
    "\n",
    "            # catch urls\n",
    "            url=[a.get_attribute('href') for a in browser.find_elements_by_xpath('//table[@id=\"grupos_avalados\"]//td[5]/a')]\n",
    "            dfgp['Revisar'] = url\n",
    "            dfg=dfg.append(dfgp)\n",
    "\n",
    "            # click next page. this instruction rise error of the end. \n",
    "            h.click(browser.find_element_by_xpath('//table[@id=\"grupos_avalados\"]//tr/td[3]/a'))\n",
    "\n",
    "        except NoSuchElementException as e:\n",
    "\n",
    "            print(e)\n",
    "            print('out of cicle')\n",
    "            break\n",
    "\n",
    "        time.sleep(sleep)\n",
    "        time.sleep(sleep)\n",
    "\n",
    "    dfg = dfg.reset_index(drop=True)\n",
    "    assert dfg.shape[0] == 324\n",
    "    \n",
    "    return dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "advised-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_db(dfg,browser):\n",
    "    \n",
    "    sleep=0.8\n",
    "    DB = [] # \n",
    "    LP = []\n",
    "    ##LR = [] \n",
    "    for idx in dfg.index[0:1]:       # TEST\n",
    "\n",
    "        # create db for store things related to group\n",
    "        DBG = {}\n",
    "\n",
    "        # part info group\n",
    "        print(dfg.loc[idx,'Nombre del grupo'])\n",
    "\n",
    "        # specific group url\n",
    "        time.sleep(sleep)\n",
    "        url_group = dfg.loc[idx,'Revisar']\n",
    "\n",
    "        # go to url group\n",
    "        time.sleep(sleep)\n",
    "        browser.get(url_group)\n",
    "\n",
    "        # catch two tables: info grupo and  members\n",
    "        source=browser.page_source\n",
    "\n",
    "        #info\n",
    "        l_info=pd.read_html(source, match='Nombre Grupo')\n",
    "        info_g=l_info[3].pivot(columns=0,values=1)\n",
    "\n",
    "        # STORE INFO_GROUP\n",
    "        DBG['Info_group'] = info_g\n",
    "\n",
    "        # members\n",
    "        l_int = pd.read_html(source,attrs={'id':'tblIntegrantes'},header=2)\n",
    "        mem_g=l_int[0]\n",
    "\n",
    "        # STORE_MEMBERS\n",
    "        DBG['Members'] =  mem_g\n",
    "\n",
    "        # Products\n",
    "\n",
    "        #time.sleep(sleep*5) # time time time !!!\n",
    "        h.wait_until(lambda: browser.find_element_by_xpath('//td[@id=\"bodyPrincipal\"]//a[text()=\"Ver productos\"]') is not None)\n",
    "        h.click(browser.find_element_by_xpath('//td[@id=\"bodyPrincipal\"]//a[text()=\"Ver productos\"]'))\n",
    "\n",
    "        # products by belongs to  # time time time\n",
    "        #time.sleep(sleep*7)       # time time time\n",
    "        h.wait_until(lambda: browser.find_element_by_xpath('//*[@id=\"ProdsPertenecia\"]') is not None)\n",
    "        h.click(browser.find_element_by_xpath('//*[@id=\"ProdsPertenecia\"]'))\n",
    "\n",
    "        time.sleep(sleep)\n",
    "        url_products=browser.current_url\n",
    "\n",
    "\n",
    "        # map all products, store those id categories that amount is different to 0 and id products asociated.\n",
    "        # make queries with combinations of categories and products\n",
    "        # make urls with diferent combinations of quieries\n",
    "        # go to each of urls\n",
    "        # load page source\n",
    "        # catch table ( or tables) asociated with categories and products\n",
    "        # store tables\n",
    "\n",
    "        report = ''\n",
    "\n",
    "        list_of_prods =[] #[[cat,prod],[cat,prod]...]\n",
    "\n",
    "        # map all products and get products and subs diff to cero\n",
    "        for i in browser.find_elements_by_xpath('//div[@id=\"accordionCatgP\"]/h3'):\n",
    "\n",
    "            report += i.text + '\\n' \n",
    "            report += i.get_attribute('id') + '\\n'     \n",
    "\n",
    "            time.sleep(sleep)\n",
    "            h.click(i)\n",
    "\n",
    "            # cat\n",
    "            cat_ = int(re.findall(r'\\d+',i.text)[0])\n",
    "\n",
    "            # create cat key in dict, for estore diferents products by this categori: 'NC_': {'ART_E':TABLE,\n",
    "            #                                                                                 'ART_IMP':TABLE}\n",
    "            if cat_ > 0:\n",
    "                DBG[i.get_attribute('id')] = {}\n",
    "\n",
    "\n",
    "            for j in browser.find_elements_by_xpath('//div[@aria-labelledby=\"%s\"]/h3' % i.get_attribute('id')):\n",
    "\n",
    "                report += '\\t' + j.text + '\\n' \n",
    "                report += '\\t' + j.get_attribute('id') + '\\n'\n",
    "\n",
    "                #prod\n",
    "                pro_ = int(re.findall(r'\\d+', j.text)[0])\n",
    "\n",
    "                if cat_ > 0 and pro_ > 0:  \n",
    "\n",
    "                    list_of_prods.append([i.get_attribute('id'),j.get_attribute('id')])\n",
    "\n",
    "            time.sleep(sleep) \n",
    "            # h.click(a)\n",
    "            h.click(i)\n",
    "\n",
    "        # PAR: products with revisions\n",
    "        h.wait_until(lambda: browser.find_element_by_xpath('//*[@id=\"ProdsAval\"]'))\n",
    "        h.click(browser.find_element_by_xpath('//*[@id=\"ProdsAval\"]'))\n",
    "\n",
    "        # NC\n",
    "\n",
    "        _NC = browser.find_element_by_xpath('//*[@id=\"NC\"]')\n",
    "\n",
    "        h.click(_NC)\n",
    "\n",
    "        cat_ = int(re.findall(r'\\d+',_NC.text)[0])\n",
    "\n",
    "        LIB = browser.find_element_by_xpath('//*[@id=\"LIB\"]')\n",
    "\n",
    "        L = int(re.findall(r'\\d+', LIB.text)[0])\n",
    "\n",
    "        CAP_LIB = browser.find_element_by_xpath('//*[@id=\"CAP_LIB\"]')\n",
    "\n",
    "        CL = int(re.findall(r'\\d+', CAP_LIB.text)[0])\n",
    "\n",
    "        if (cat_ > 0 and L > 0) or (cat_ > 0 and CL > 0):\n",
    "\n",
    "            DBG[_NC.get_attribute('id')] = {}\n",
    "\n",
    "        if (cat_ > 0 and L > 0):\n",
    "\n",
    "            list_of_prods.append([_NC.get_attribute('id'),LIB.get_attribute('id')])\n",
    "\n",
    "        if (cat_ > 0 and CL > 0):\n",
    "\n",
    "            list_of_prods.append([_NC.get_attribute('id'),CAP_LIB.get_attribute('id')])\n",
    "\n",
    "        # print(report)\n",
    "        # print('\\n')\n",
    "        # print('--------------------------------')\n",
    "        time.sleep(sleep*2)\n",
    "\n",
    "        tables=[]\n",
    "\n",
    "        for p in range(len(list_of_prods)):\n",
    "\n",
    "                # make query\n",
    "                if list_of_prods[p][0] == 'NC':\n",
    "\n",
    "                    query='categoria=%s&subcategoria=%s&aval=T' % (list_of_prods[p][0],list_of_prods[p][1])\n",
    "\n",
    "                else:\n",
    "\n",
    "                    query='categoriaP=%s&subcategoriaP=%s&aval=P' % (list_of_prods[p][0],list_of_prods[p][1])\n",
    "\n",
    "                # make url query\n",
    "                url_query = url_products.split('?')[0] + '?' + query + '&' + url_products.split('?')[1]\n",
    "\n",
    "                # retrieve id asociated tables\n",
    "                table_id = dict_tables[list_of_prods[p][0]][list_of_prods[p][1]]\n",
    "\n",
    "                # go to url product by group\n",
    "                time.sleep(sleep)\n",
    "\n",
    "                browser.get(url_query)\n",
    "\n",
    "                # load page\n",
    "                time.sleep(sleep)\n",
    "                page_source = browser.page_source\n",
    "\n",
    "                # catch tables\n",
    "                if isinstance(table_id,str): # case one table\n",
    "\n",
    "                    # catch title table\n",
    "\n",
    "                    #title_table = browser.find_element_by_xpath('//div/p[@class=\"titulo_tabla\"]').text \n",
    "                    # cathc table\n",
    "                    print(url_query)\n",
    "                    time.sleep(sleep*2)\n",
    "\n",
    "                    # checkpoint\n",
    "                    try:\n",
    "\n",
    "                        table = pd.read_html(page_source,attrs={'id':table_id}, header=2)[0][1:-1]\n",
    "\n",
    "                    except ValueError:\n",
    "\n",
    "                        table = None\n",
    "\n",
    "\n",
    "                    # store table\n",
    "                    DBG[list_of_prods[p][0]][list_of_prods[p][1]] = {table_id:table}\n",
    "                    # ---- in building ----\n",
    "\n",
    "                elif isinstance(table_id, list): # case multiple tables\n",
    "\n",
    "                    DBG[list_of_prods[p][0]][list_of_prods[p][1]] ={}\n",
    "\n",
    "                    for i in range(len(table_id)):\n",
    "\n",
    "                        # fix bug\n",
    "                        if list_of_prods[p][1] == 'DC_P' and i == 3:\n",
    "                            # catch title specific table \n",
    "                            title_table = browser.find_elements_by_xpath('//div/p[@class=\"titulo_tabla\"]')[i].text\n",
    "\n",
    "                            # catch table software\n",
    "\n",
    "                            # checkpoint\n",
    "                            try:\n",
    "\n",
    "                                table = pd.read_html(page_source,attrs={'id':table_id[i]}, header=2)[1][1:-1]\n",
    "\n",
    "                            except ValueError:\n",
    "\n",
    "                                table = None\n",
    "\n",
    "                            # store table\n",
    "                            DBG[list_of_prods[p][0]][list_of_prods[p][1]]['DC_DES_P_TABLE'] = table\n",
    "\n",
    "\n",
    "                        # catch title specific table \n",
    "                        title_table = browser.find_elements_by_xpath('//div/p[@class=\"titulo_tabla\"]')[i].text\n",
    "\n",
    "                        # catch table trasmedia\n",
    "                        # checkpoint\n",
    "                        try:\n",
    "\n",
    "                            table = pd.read_html(page_source,attrs={'id':table_id[i]}, header=2)[0][1:-1]\n",
    "\n",
    "                        except ValueError:\n",
    "\n",
    "                            table = None\n",
    "\n",
    "                        # store table\n",
    "                        DBG[list_of_prods[p][0]][list_of_prods[p][1]][table_id[i]]=table\n",
    "\n",
    "\n",
    "                        # -----------\n",
    "        DB.append(DBG)\n",
    "        LP.append(list_of_prods)\n",
    "    \n",
    "    return DB\n",
    "        #LR.append(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "informational-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'InstituLAC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "biblical-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pickle(DB,dfg, DIR='InstituLAC'):\n",
    "    \n",
    "    with open(f'{DIR}/DB.pickle','wb') as f:\n",
    "        pickle.dump(DB, f)\n",
    "    \n",
    "    with open(f'{DIR}/dfg.pickle','wb') as g:\n",
    "        pickle.dump(DB, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "split-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_none(DB):\n",
    "    \n",
    "    list_none = []\n",
    "    for i in range(len(DB)):\n",
    "        for k in list(DB[i].keys())[2:]:\n",
    "            for kk in  DB[i][k].keys():\n",
    "                print(i,k,kk)\n",
    "                if DB[i][k][kk] is None:\n",
    "                    list_none.append([i,k,kk])\n",
    "    return list_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "intensive-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_df(idx, list_cat_prod, dfg, dict_tables, user, pasword, sleep=0.8):\n",
    "\n",
    "        # log\n",
    "        browser = login(user,password) \n",
    "        \n",
    "        # url group\n",
    "        time.sleep(sleep)\n",
    "        url_group = dfg.loc[idx,'Revisar']\n",
    "\n",
    "        # go to url group\n",
    "        time.sleep(sleep)\n",
    "        browser.get(url_group)\n",
    "\n",
    "        # Products\n",
    "        #time.sleep(sleep*5) # time time time !!!\n",
    "        h.wait_until(lambda: browser.find_element_by_xpath('//td[@id=\"bodyPrincipal\"]//a[text()=\"Ver productos\"]') is not None)\n",
    "        h.click(browser.find_element_by_xpath('//td[@id=\"bodyPrincipal\"]//a[text()=\"Ver productos\"]'))\n",
    "\n",
    "        # products by belongs to  \n",
    "        # time.sleep(sleep*7)       \n",
    "        h.wait_until(lambda: browser.find_element_by_xpath('//*[@id=\"ProdsPertenecia\"]') is not None)\n",
    "        h.click(browser.find_element_by_xpath('//*[@id=\"ProdsPertenecia\"]'))\n",
    "\n",
    "        time.sleep(sleep)\n",
    "        url_products=browser.current_url\n",
    "        \n",
    "        # make query\n",
    "        if list_cat_prod[0] == 'NC':\n",
    "\n",
    "            query='categoria=%s&subcategoria=%s&aval=T' % (list_cat_prod[0],list_cat_prod[1])\n",
    "\n",
    "        else:\n",
    "\n",
    "            query='categoriaP=%s&subcategoriaP=%s&aval=P' % (list_cat_prod[0],list_cat_prod[1])\n",
    "            \n",
    "        # make url query\n",
    "        url_query = url_products.split('?')[0] + '?' + query + '&' + url_products.split('?')[1]\n",
    "\n",
    "        # retrieve id asociated tables\n",
    "        table_id = dict_tables[list_cat_prod[0]][list_cat_prod[1]]\n",
    "\n",
    "        # go to url product \n",
    "        time.sleep(sleep)\n",
    "        browser.get(url_query)\n",
    "        \n",
    "        # load page\n",
    "        time.sleep(sleep)\n",
    "        page_source = browser.page_source\n",
    "        \n",
    "        if isinstance(table_id,str): # case one table\n",
    "\n",
    "\n",
    "            #print(url_query)\n",
    "            time.sleep(sleep*2)\n",
    "\n",
    "                    # checkpoint\n",
    "            try:\n",
    "\n",
    "                table = pd.read_html(page_source,attrs={'id':table_id}, header=2)[0][1:-1]\n",
    "\n",
    "            except ValueError:\n",
    "\n",
    "                table = None\n",
    "                \n",
    "            return table\n",
    "\n",
    "\n",
    "        elif isinstance(table_id, list): # case multiple tables\n",
    "            \n",
    "            list_of_tables =[]\n",
    "\n",
    "\n",
    "            for i in range(len(table_id)):\n",
    "\n",
    "            \n",
    "                if list_of_prods[p][1] == 'DC_P' and i == 3:\n",
    "                    \n",
    "            \n",
    "                    try:\n",
    "\n",
    "                        table = pd.read_html(page_source,attrs={'id':table_id[i]}, header=2)[1][1:-1]\n",
    "\n",
    "                    except ValueError:\n",
    "\n",
    "                        table = None\n",
    "\n",
    "                        # store table\n",
    "                    list_of_tables.append(table)\n",
    "            \n",
    "                try:\n",
    "\n",
    "                    table = pd.read_html(page_source,attrs={'id':table_id[i]}, header=2)[0][1:-1]\n",
    "\n",
    "                except ValueError:\n",
    "\n",
    "                    table = None\n",
    "                    \n",
    "                list_of_tables.append(table)\n",
    "                \n",
    "            return list_of_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "junior-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fix_df(DB):\n",
    "    for i in range(len(DB)):\n",
    "        for k in list(DB[i].keys())[2:]:\n",
    "            for kk in  DB[i][k].keys():\n",
    "                print(i,k,kk)\n",
    "                if DB[i][k][kk] is None:\n",
    "                    DB[i][k][kk]={kk: pd.DataFrame()} \n",
    "    return DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "departmental-fitness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 NC_P ART_IMP_P\n",
      "0 NC_P ART_ELE_P\n",
      "0 NC_P CAP_LIB_P\n",
      "0 NC_P PAT_P\n",
      "0 FRH_P TES_MAST_P\n",
      "0 FRH_P TES_PREG_P\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Info_group': 0  NaN  (Fecha de última actualización del aval  05 de junio de 2019)  \\\n",
       "  0  NaN  (Fecha de última actualización del aval  05 de junio de 2019)   \n",
       "  1  NaN                                                            NaN   \n",
       "  2  NaN                                                            NaN   \n",
       "  3  NaN                                                            NaN   \n",
       "  4  NaN                                                            NaN   \n",
       "  5  NaN                                                            NaN   \n",
       "  6  NaN                                                            NaN   \n",
       "  \n",
       "  0                     Nombre Grupo   Nombre Líder             Nombre área  \\\n",
       "  0                              NaN            NaN                     NaN   \n",
       "  1                              NaN            NaN                     NaN   \n",
       "  2  Grupo Tandem en Nano-bio-física            NaN                     NaN   \n",
       "  3                              NaN            NaN                     NaN   \n",
       "  4                              NaN            NaN  Biotecnología en Salud   \n",
       "  5                              NaN  JAHIR  OROZCO                     NaN   \n",
       "  6                              NaN            NaN                     NaN   \n",
       "  \n",
       "  0 Revisión de productos (Nuevo) Verificador de información  \n",
       "  0                           NaN                        NaN  \n",
       "  1                           NaN                        NaN  \n",
       "  2                           NaN                        NaN  \n",
       "  3                           NaN                        Ver  \n",
       "  4                           NaN                        NaN  \n",
       "  5                           NaN                        NaN  \n",
       "  6                 Ver productos                        NaN  ,\n",
       "  'Members':     Nro.                         Integrante Fecha Inicio/Vinculación  \\\n",
       "  0      1           Camargo Camargo Carolina            julio de 2017   \n",
       "  1      2          Toro Londoño Miguel Angel        noviembre de 2018   \n",
       "  2      3        Soto Enriquez Ingrid Dayana            enero de 2017   \n",
       "  3      4      suarez Barrera Miguel Orlando           agosto de 2018   \n",
       "  4      5           Pérez Cardona David José          febrero de 2020   \n",
       "  5      6     Palacio Rodríguez Karen Lisbet           agosto de 2017   \n",
       "  6      7                 LANS VARGAS Isaias            enero de 2018   \n",
       "  7      8     Osorio Atehortúa Jessica Paola            enero de 2020   \n",
       "  8      9            Fernandez Culma Maritza            enero de 2019   \n",
       "  9     10       Escobar Chaves Elkin Leandro            marzo de 2019   \n",
       "  10    11                Sánchez Toro Arturo             mayo de 2018   \n",
       "  11    12        Ochoa Deossa Rodrigo Alonso            enero de 2017   \n",
       "  12    13     ALZATE GUTIERREZ DANIEL STIVEN            enero de 2017   \n",
       "  13    14            Jiménez Valencia Camila            enero de 2021   \n",
       "  14    15         Cruz Pacheco Andrés Felipe          febrero de 2019   \n",
       "  15    16  Cajigas Bastidas Nestor Sebastian            enero de 2017   \n",
       "  16    17    mejia de los rios Susana Pamela             mayo de 2017   \n",
       "  17    18                Cossio Tejada Pilar            julio de 2016   \n",
       "  18    19               Orozco Holguín Jahir            julio de 2016   \n",
       "  19    20             Alfonso Parra Catalina            marzo de 2017   \n",
       "  20    21           Lopez Acevedo Olga Lucia            abril de 2021   \n",
       "  21    22                Avila Frank William            julio de 2016   \n",
       "  22    23          Echeverri Hincapié Danilo          febrero de 2019   \n",
       "  23    24    Quinchia Cardona Jennifer Paola          febrero de 2019   \n",
       "  24    25              Díaz Zuleta Sebastián          febrero de 2017   \n",
       "  25    26        Ortiz Girón Jhoan Sebastian            enero de 2018   \n",
       "  26    27       Mena Giraldo Pedro Alejandro            julio de 2016   \n",
       "  \n",
       "      Nro. Horas Dedicación  Avalar integrante  Unnamed: 5  \n",
       "  0                      32                NaN         NaN  \n",
       "  1                      40                NaN         NaN  \n",
       "  2                      40                NaN         NaN  \n",
       "  3                       5                NaN         NaN  \n",
       "  4                      20                NaN         NaN  \n",
       "  5                      40                NaN         NaN  \n",
       "  6                      40                NaN         NaN  \n",
       "  7                       0                NaN         NaN  \n",
       "  8                      40                NaN         NaN  \n",
       "  9                      20                NaN         NaN  \n",
       "  10                     40                NaN         NaN  \n",
       "  11                     40                NaN         NaN  \n",
       "  12                     40                NaN         NaN  \n",
       "  13                     15                NaN         NaN  \n",
       "  14                     40                NaN         NaN  \n",
       "  15                     40                NaN         NaN  \n",
       "  16                     40                NaN         NaN  \n",
       "  17                     40                NaN         NaN  \n",
       "  18                     40                NaN         NaN  \n",
       "  19                     40                NaN         NaN  \n",
       "  20                     20                NaN         NaN  \n",
       "  21                     40                NaN         NaN  \n",
       "  22                     40                NaN         NaN  \n",
       "  23                     40                NaN         NaN  \n",
       "  24                     40                NaN         NaN  \n",
       "  25                     40                NaN         NaN  \n",
       "  26                     40                NaN         NaN  ,\n",
       "  'NC_P': {'ART_IMP_P': {'ART_P_TABLE':   Unnamed: 0  \\\n",
       "    1          1   \n",
       "    2          2   \n",
       "    3          3   \n",
       "    4          4   \n",
       "    \n",
       "                                                                                                             Título del artículo  \\\n",
       "    1                              Paper strip-embedded graphene quantum dots: a screening device fully operated by a smartphone   \n",
       "    2                          Synthesis of graphene-coated CNT-supported metal nanoparticles as multifunctional hybrid material   \n",
       "    3  Molecular Techniques for the Detection of Organisms in Aquatic Environments, with Emphasis on Harmful Algal Bloom Species   \n",
       "    4                                                           Bioluminescent nanopaper for rapid screening of toxic substances   \n",
       "    \n",
       "      Año de presentación Mes de presentación Volumen revista Página inicial  \\\n",
       "    1                2017               Enero               7              1   \n",
       "    2                2017               Enero             111            393   \n",
       "    3                2017             Octubre              17              1   \n",
       "    4                2018               Enero              11            114   \n",
       "    \n",
       "      Página final      ISSN Categoría Última actualización Revisar  \n",
       "    1            9  20452322    ART_A1           2021-04-27     NaN  \n",
       "    2          401  00086223    ART_A1           2021-04-27     NaN  \n",
       "    3           22  14248220    ART_A2           2021-04-27     NaN  \n",
       "    4          125  19980124    ART_A1           2021-04-27     NaN  },\n",
       "   'ART_ELE_P': {'ART_E_P_TABLE':   Unnamed: 0  \\\n",
       "    1          1   \n",
       "    \n",
       "                                                                                    Título del artículo  \\\n",
       "    1  Architecting Graphene Oxide Rolled-Up Micromotors: A Simple Paper-Based Manufacturing Technology   \n",
       "    \n",
       "      Año de presentación Mes de presentación Volumen  \\\n",
       "    1                2018               Enero      14   \n",
       "    \n",
       "                                                                         URL del artículo  \\\n",
       "    1  https://onlinelibrary.wiley.com/doi/full/10.1002/smll.201702746#accessDenialLayout   \n",
       "    \n",
       "             DOI del artículo      ISSN Categoría Última actualización Revisar  \n",
       "    1  10.1002/smll.201702746  16136810    ART_A1           2021-04-27     NaN  },\n",
       "   'CAP_LIB_P': {'CAP_LIB_P_TABLE':   Unnamed: 0  \\\n",
       "    1          1   \n",
       "    2          2   \n",
       "    \n",
       "                                                                                                                                                                        Título del capítulo del libro  \\\n",
       "    1                                                                                                                        Development and anatomy of reproductive organs: Insect accessory glands.   \n",
       "    2  Aplicación de nuevas tecnologías para entender la evolución de resistencia a diferentes estrategias de manejo de plagas: Aplicación de ARNi para entender resistencia en cultivos transgénicos   \n",
       "    \n",
       "                                                                                                   Título del libro  \\\n",
       "    1                                                                              The Encyclopedia of Reproduction   \n",
       "    2  Entomología de Impacto Solución a problemas integrando disciplinas Memorias & Resúmenes. 44 Congreso SOCOLEN   \n",
       "    \n",
       "          ISBN del libro Año de presentación Mes de presentación Categoría  \\\n",
       "    1  978-0-12-815145-7                2018              Agosto       NaN   \n",
       "    2          2389-7694                2017               Julio       NaN   \n",
       "    \n",
       "      Última actualización Revisar  \n",
       "    1           2019-03-12     NaN  \n",
       "    2           2017-07-11     NaN  },\n",
       "   'PAT_P': {'PAT_P_TABLE': Empty DataFrame\n",
       "    Columns: []\n",
       "    Index: []}},\n",
       "  'FRH_P': {'TES_MAST_P': {'TM_P_TABLE':   Unnamed: 0  \\\n",
       "    1          1   \n",
       "    \n",
       "                                                                                                Título  \\\n",
       "    1  Peptide-functionalized photosensitive nanocarriers for specific drug delivery in cardiomyocytes   \n",
       "    \n",
       "                              Autor                         Institución  \\\n",
       "    1  Pedro Alejandro Mena Giraldo  UNIVERSIDAD PONTIFICIA BOLIVARIANA   \n",
       "    \n",
       "                   Director Fecha inicio Fecha fin Reconocimiento Categoría  \\\n",
       "    1  Jahir Orozco Holguín       2016-7    2018-3       Aprobada       NaN   \n",
       "    \n",
       "      Última actualización Revisar  \n",
       "    1           2019-06-08     NaN  },\n",
       "   'TES_PREG_P': {'TP_P_TABLE':   Unnamed: 0  \\\n",
       "    1          1   \n",
       "    \n",
       "                                                                                                                         Título  \\\n",
       "    1  Estudio de las bacterias cultivables asociadas a Aedes aegypti y su impacto en parámetros de tabla de vida de la especie   \n",
       "    \n",
       "                              Autor               Institución  \\\n",
       "    1  Luisa María Barrientos Usuga  UNIVERSIDAD DE ANTIOQUIA   \n",
       "    \n",
       "                  Director Fecha inicio Fecha fin Reconocimiento Categoría  \\\n",
       "    1  Frank William Avila       2017-9    2020-5       Aprobada       NaN   \n",
       "    \n",
       "      Última actualización Revisar  \n",
       "    1           2021-04-18     NaN  }}}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_fix_df(DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "narrow-stress",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(DIR):\n",
    "    \n",
    "    with open(f'{DIR}/DB.pickle', 'rb') as f:\n",
    "        DB = pickle.load(f)\n",
    "        \n",
    "    with open(f'{DIR}/dfg.pickle', 'rb') as f:\n",
    "        dfg = pickle.load(f)\n",
    "        \n",
    "    return DB, dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "comfortable-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_excel(DB,dfg,DIR='InstituLAC'):\n",
    "    \n",
    "    global writer\n",
    "    global workbook\n",
    "    global general\n",
    "    \n",
    "    # make excel\n",
    "    # ONE GROUP IMPLEMENTATION\n",
    "    for idxx in range(len(DB)):\n",
    "    # DATA\n",
    "        DBG = DB[idxx]\n",
    "\n",
    "        ### excel name\n",
    "        name = 'Plantilla_Formato de verificación de información_GrupLAC_894-2021_%s'\n",
    "\n",
    "        cod_gr = dfg.loc[idxx,'COL Grupo']\n",
    "\n",
    "        # initialize object= output excel file\n",
    "        writer = pd.ExcelWriter(DIR + '/' + name % cod_gr +'.xlsx', engine='xlsxwriter')\n",
    "\n",
    "        workbook = writer.book\n",
    "\n",
    "        general=workbook.add_format({'text_wrap':True})\n",
    "\n",
    "        # PPT\n",
    "        format_ptt(workbook)\n",
    "\n",
    "        # INFO GROUP\n",
    "        df=get_info(DBG['Info_group'], cod_gr)\n",
    "        format_info(df, writer, '2.Datos de contacto')\n",
    "\n",
    "        # WORKSHEET 1\n",
    "        df = clean_df(DBG['Members']) \n",
    "        eh = DBEH['MEMBERS']\n",
    "        format_df(df, '3.Integrantes grupo', 1, writer, eh, veh=0) #### veh = 0\n",
    "\n",
    "        ### NC_P ### \n",
    "\n",
    "        #------- w4 -------\n",
    "        # 4.ART y N\n",
    "\n",
    "        var_w4 = 0\n",
    "\n",
    "        try:\n",
    "            df=clean_df(DBG['NC_P']['ART_IMP_P']['ART_P_TABLE'])\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc+1)\n",
    "\n",
    "            eh=DBEH['NC_P']['ART_IMP_P']['ART_P_TABLE']\n",
    "\n",
    "            format_df(df, '4.ART y N',  var_w4, writer,eh)\n",
    "\n",
    "            var_w4 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            df=clean_df(DBG['NC_P']['ART_ELE_P']['ART_E_P_TABLE'])\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "            eh=DBEH['NC_P']['ART_ELE_P']['ART_E_P_TABLE']\n",
    "\n",
    "            format_df(df, '4.ART y N', var_w4, writer,eh)\n",
    "\n",
    "            var_w4 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            df=clean_df(DBG['NC_P']['NOT_CIE_P']['NOT_CIE_P_TABLE'])\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "            eh=DBEH['NC_P']['NOT_CIE_P']['NOT_CIE_P_TABLE']\n",
    "\n",
    "            format_df(df, '4.ART y N', var_w4, writer,eh)\n",
    "\n",
    "            var_w4 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "        # -------------- w4 -------------------------\n",
    "\n",
    "        #------------ ---w5------------\n",
    "        # 5.LIB y LIB_FOR\n",
    "        var_w5 = 0\n",
    "\n",
    "        # libros por pertenencia\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['NC_P']['LIB_P']['LIB_P_TABLE']),'Título del artículo','Título del libro')\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "            eh=DBEH['NC_P']['LIB_P']['LIB_P_TABLE']\n",
    "\n",
    "            format_df(df, '5.LIB y LIB_FOR',  var_w5, writer,eh)\n",
    "\n",
    "            var_w5 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # libros avalados con revisión\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['NC']['LIB']['LIB_T_AVAL_TABLE']), 'Título del artículo' ,'Título del libro') \n",
    "\n",
    "            #df.to_excel(writer,sheet_name='FRH_P',startrow = var_rh)\n",
    "\n",
    "            eh=DBEH['NC']['LIB']['LIB_T_AVAL_TABLE']\n",
    "\n",
    "            format_df(df, '5.LIB y LIB_FOR', var_w5 , writer, eh)\n",
    "\n",
    "            var_w5  += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # libros formacion\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['ASC_P']['GEN_CONT_IMP_P']['GC_I_P_TABLE_5']),'Título del libro','Título del libro formación') # lib form\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "            eh=DBEH['ASC_P']['GEN_CONT_IMP_P']['GC_I_P_TABLE_5']\n",
    "\n",
    "            format_df(df, '5.LIB y LIB_FOR',  var_w5 , writer,eh)\n",
    "\n",
    "            var_w5 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass  \n",
    "        # --------------------w5--------------\n",
    "\n",
    "        #--------------------w6---------------\n",
    "        #6.CAP\n",
    "\n",
    "        # cap pertenencia\n",
    "\n",
    "        var_w6 = 0\n",
    "\n",
    "        try:\n",
    "            df=clean_df(DBG['NC_P']['CAP_LIB_P']['CAP_LIB_P_TABLE'])\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "            eh=DBEH['NC_P']['CAP_LIB_P']['CAP_LIB_P_TABLE']\n",
    "\n",
    "            format_df(df, '6.CAP',var_w6, writer,eh)\n",
    "\n",
    "            var_w6 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # caps avalados con revision\n",
    "        try:\n",
    "            df = clean_df(DBG['NC']['CAP_LIB']['CAP_LIB_T_AVAL_TABLE'])  ### ,veh = 2\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='FRH_P',startrow = var_rh)\n",
    "\n",
    "            eh = DBEH['NC']['CAP_LIB']['CAP_LIB_T_AVAL_TABLE']\n",
    "\n",
    "            format_df(df, '6.CAP', var_w6, writer, eh)\n",
    "\n",
    "            var_w6 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # traduccion filologica\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['NC_P']['TRA_FIL_P']['TRA_FIL_P_TABLE']),'Título del libro', 'Título traducción filologica')\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "            eh=DBEH['NC_P']['TRA_FIL_P']['TRA_FIL_P_TABLE']\n",
    "\n",
    "            format_df(df, '6.CAP', var_w6, writer,eh)\n",
    "\n",
    "            var_w6 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        #-------------------w6------------------\n",
    "\n",
    "        #------------w7-------------------------\n",
    "        #7.Patente_Variedades\n",
    "        var_w7 = 0\n",
    "\n",
    "        # patentes\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['NC_P']['PAT_P']['PAT_P_TABLE']),'Título del artículo','Título de la patente') ###### veh=1\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "            eh=DBEH['NC_P']['PAT_P']['PAT_P_TABLE']\n",
    "\n",
    "            format_df(df, '7.Patente_Variedades', var_w7, writer,eh, veh=1)\n",
    "\n",
    "            var_w7 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # variedad vegetal\n",
    "        try:\n",
    "            df=clean_df(DBG['NC_P']['VAR_VEG_P']['VV_P_TABLE'])\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "            eh=DBEH['NC_P']['VAR_VEG_P']['VV_P_TABLE']\n",
    "\n",
    "            format_df(df, '7.Patente_Variedades', var_w7, writer,eh)\n",
    "\n",
    "            var_w7 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # Variedad Animal\n",
    "        try:\n",
    "            df=clean_df(DBG['NC_P']['VAR_ANI_P']['VA_P_TABLE'])\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "            eh=DBEH['NC_P']['VAR_ANI_P']['VA_P_TABLE']\n",
    "\n",
    "            format_df(df, '7.Patente_Variedades', var_w7, writer,eh)\n",
    "\n",
    "            var_w7 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # razas pecuarias mejoradas\n",
    "        try:\n",
    "            df=clean_df(DBG['NC_P']['RAZ_PEC_P']['RAZ_PEC_P_TABLE'])\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "            eh=DBEH['NC_P']['RAZ_PEC_P']['RAZ_PEC_P_TABLE']\n",
    "\n",
    "            format_df(df, '7.Patente_Variedades', var_w7, writer,eh)\n",
    "\n",
    "            var_w7 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "        # ---------------w7---------------------\n",
    "\n",
    "        #---------------w8-------------------\n",
    "        var_w8 = 0\n",
    "\n",
    "        # productos investigacion creacion\n",
    "        try:\n",
    "            df=clean_df(DBG['NC_P']['PRD_INV_ART_P']['PAAD_P_TABLE']) ###### veh = 1\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "            eh=DBEH['NC_P']['PRD_INV_ART_P']['PAAD_P_TABLE']\n",
    "\n",
    "            format_df(df, '8.AAD', var_w8, writer,eh, veh=3)\n",
    "\n",
    "            var_w8 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        #-------------W8---------------------\n",
    "\n",
    "        #-------------W9----------------\n",
    "\n",
    "        # 9.Tecnológico\n",
    "        #### DTI_P\n",
    "\n",
    "        var_w9 = 0\n",
    "\n",
    "        # diseño industrial\n",
    "        try:\n",
    "\n",
    "            df=rename_col(clean_df(DBG['DTI_P']['DIS_IND_P']['DI_P_TABLE']),'Nombre del diseño','Nombre del diseño industrial')\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "            eh=DBEH['DTI_P']['DIS_IND_P']['DI_P_TABLE']\n",
    "\n",
    "            format_df(df, '9.Tecnológico', var_w9, writer, eh)\n",
    "\n",
    "            var_w9 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        #circuitos integrados\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['DTI_P']['CIR_INT_P']['ECI_P_TABLE']),'Nombre del diseño', 'Nombre del diseño circuito')\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "            eh=DBEH['DTI_P']['CIR_INT_P']['ECI_P_TABLE']\n",
    "\n",
    "            format_df(df, '9.Tecnológico', var_w9, writer,eh)\n",
    "\n",
    "            var_w9 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # colecciones\n",
    "        try:\n",
    "            df=clean_df(DBG['DTI_P']['COL_CIENT_P']['COL_CIENT_P_TABLE'])\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "            eh=DBEH['DTI_P']['COL_CIENT_P']['COL_CIENT_P_TABLE']\n",
    "\n",
    "            format_df(df, '9.Tecnológico', var_w9, writer,eh)\n",
    "\n",
    "            var_w9 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # software \n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['DTI_P']['SOFT_P']['SF_P_TABLE']),'Nombre del diseño', 'Nombre del diseño de software')\n",
    "\n",
    "            eh=DBEH['DTI_P']['SOFT_P']['SF_P_TABLE']\n",
    "\n",
    "            format_df(df, '9.Tecnológico', var_w9, writer,eh)\n",
    "\n",
    "            var_w9 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # secreto industrial\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['DTI_P']['SEC_IND_P']['SE_P_TABLE']),'Producto','Nombre secreto industrial')\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "            eh=DBEH['DTI_P']['SEC_IND_P']['SE_P_TABLE']\n",
    "\n",
    "            format_df(df, '9.Tecnológico', var_w9, writer,eh)\n",
    "\n",
    "            var_w9 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # prototipo insdustrial\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['DTI_P']['PRT_IND_P']['PI_P_TABLE']), 'Nombre del diseño', 'Nombre del prototipo')\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "            eh=DBEH['DTI_P']['PRT_IND_P']['PI_P_TABLE']\n",
    "\n",
    "            format_df(df, '9.Tecnológico',  var_w9, writer,eh)\n",
    "\n",
    "            var_w9 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # Registro distintivo\n",
    "        try:\n",
    "            df=clean_df(DBG['DTI_P']['SIG_DIS_P']['SD_P_TABLE'])\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "            eh=DBEH['DTI_P']['SIG_DIS_P']['SD_P_TABLE']\n",
    "\n",
    "            format_df(df, '9.Tecnológico', var_w9, writer,eh)\n",
    "\n",
    "            var_w9 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # registros de acuerdo licencias expl obras AAD\n",
    "        try:\n",
    "\n",
    "            df=clean_df(DBG['DTI_P']['REG_AAD_P']['AAAD_P_TABLE'])\n",
    "\n",
    "            eh=DBEH['DTI_P']['REG_AAD_P']['AAAD_P_TABLE']\n",
    "\n",
    "            format_df(df, '9.Tecnológico', var_w9, writer,eh)\n",
    "\n",
    "            var_w9 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # prod nutracetico\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['DTI_P']['NUTRA_P']['NUTRA_P_TABLE']),'Nombre del producto','Nombre del producto nutracetico')\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='DTI_P',startrow = var_nc)\n",
    "\n",
    "            eh=DBEH['DTI_P']['NUTRA_P']['NUTRA_P_TABLE']\n",
    "\n",
    "            format_df(df, '9.Tecnológico', var_w9, writer,eh)\n",
    "\n",
    "            var_w9 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # registro cienti\n",
    "        try:\n",
    "            df=clean_df(DBG['DTI_P']['REG_CIENT_P']['REG_CIENT_P_TABLE'])\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "            eh=DBEH['DTI_P']['REG_CIENT_P']['REG_CIENT_P_TABLE']\n",
    "\n",
    "            format_df(df, '9.Tecnológico',var_w9 , writer,eh)\n",
    "\n",
    "            var_w9 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # planta piloto\n",
    "\n",
    "        try:\n",
    "            df=clean_df(DBG['DTI_P']['PLT_PIL_P']['PP_P_TABLE'])\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "            eh=DBEH['DTI_P']['PLT_PIL_P']['PP_P_TABLE']\n",
    "\n",
    "            format_df(df, '9.Tecnológico', var_w9, writer,eh)\n",
    "\n",
    "            var_w9 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # protocolo vigilancia epidemologica\n",
    "\n",
    "        try:\n",
    "            df=clean_df(DBG['DTI_P']['PROT_VIG_EPID_P']['PROT_VIG_EPID_P_TABLE'])\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "            eh=DBEH['DTI_P']['PROT_VIG_EPID_P']['PROT_VIG_EPID_P_TABLE']\n",
    "\n",
    "            format_df(df, '9.Tecnológico',var_w9, writer,eh)\n",
    "\n",
    "            var_w9 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "        #---------------------w9----------------\n",
    "\n",
    "        #---------------------w10----------------\n",
    "        # 10.Empresarial\n",
    "        var_w10 = 0\n",
    "\n",
    "        # innovación gestion empresarial\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['DTI_P']['INN_GES_EMP_P']['IG_P_TABLE']),'Nombre de la innovación', 'Nombre de la innovación empresarial')\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "            eh=DBEH['DTI_P']['INN_GES_EMP_P']['IG_P_TABLE']\n",
    "\n",
    "            format_df(df, '10.Empresarial', var_w10, writer,eh)\n",
    "\n",
    "            var_w10 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "\n",
    "        # innovacion procesos y procedimiento\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['DTI_P']['INN_PROC_P']['IPP_P_TABLE']),'Nombre de la innovación','Nombre de la innovación procesos y procedimientos')\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "            eh=DBEH['DTI_P']['INN_PROC_P']['IPP_P_TABLE']\n",
    "\n",
    "            format_df(df, '10.Empresarial', var_w10, writer,eh)\n",
    "\n",
    "            var_w10 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # regulaciones normas reglamentos legislaciones\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['DTI_P']['REG_NORM_REGL_LEG_P']['RNR_P_TABLE']),'Tipo producto','Nombre regulación')\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "            eh=DBEH['DTI_P']['REG_NORM_REGL_LEG_P']['RNR_P_TABLE']\n",
    "\n",
    "            format_df(df, '10.Empresarial', var_w10, writer,eh)\n",
    "\n",
    "            var_w10 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # conceptos tecnicos\n",
    "        try:\n",
    "            df=clean_df(DBG['DTI_P']['CONP_TEC_P']['CONP_TEC_P_TABLE'])\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "            eh=DBEH['DTI_P']['CONP_TEC_P']['CONP_TEC_P_TABLE']\n",
    "\n",
    "            format_df(df, '10.Empresarial', var_w10, writer,eh)\n",
    "\n",
    "            var_w10 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # empresa base tecnologica\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['DTI_P']['EMP_BSE_TEC_P']['EBT_P_TABLE']),'Tipo','Tipo de empresa base tecnologica')\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "            eh=DBEH['DTI_P']['EMP_BSE_TEC_P']['EBT_P_TABLE']\n",
    "\n",
    "            format_df(df, '10.Empresarial', var_w10, writer,eh)\n",
    "\n",
    "            var_w10 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # empresa de base cultural\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['DTI_P']['EMP_CRE_CUL_P']['ICC_P_TABLE']),'Empresa', 'Tipo de empresa base cultural')\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "            eh=DBEH['DTI_P']['EMP_CRE_CUL_P']['ICC_P_TABLE']\n",
    "\n",
    "            format_df(df, '10.Empresarial', var_w10, writer,eh)\n",
    "\n",
    "            var_w10 += df.shape[0] + 3\n",
    "\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # -------------------------w10-------------\n",
    "        ######  ASC\n",
    "\n",
    "        # -------- w11\n",
    "        # 11.ASC y Divulgación\n",
    "        var_w11 = 0 \n",
    "\n",
    "        # productos de interes social\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['ASC_P']['PASC_P']['PASC_FOR_P_TABLE']),'Nombre','Nombre producto interes social')\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "            eh=DBEH['ASC_P']['PASC_P']['PASC_FOR_P_TABLE']\n",
    "\n",
    "            format_df(df, '11.ASC y Divulgación', var_w11, writer,eh)\n",
    "\n",
    "            var_w11 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # Proceso de apropiación social del conocimiento resultado del trabajo conjunto \n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['ASC_P']['PASC_P']['PASC_TRA_P_TABLE']), 'Nombre','Nombre del Proceso de apropiación social del conocimiento resultado del trabajo conjunto entre un Centro de Ciencia y un grupo de investigación')\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "            eh=DBEH['ASC_P']['PASC_P']['PASC_TRA_P_TABLE']\n",
    "\n",
    "            format_df(df, '11.ASC y Divulgación', var_w11, writer,eh)\n",
    "\n",
    "            var_w11 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "        # Nombre del Proceso de apropiación social del conocimiento para la generación de insumos de política pública y normatividad\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['ASC_P']['PASC_P']['PASC_GEN_P_TABLE']),'Nombre','Nombre del Proceso de apropiación social del conocimiento para la generación de insumos de política pública y normatividad')\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "            eh=DBEH['ASC_P']['PASC_P']['PASC_GEN_P_TABLE']\n",
    "\n",
    "            format_df(df, '11.ASC y Divulgación', var_w11, writer,eh)\n",
    "\n",
    "            var_w11 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        #Nombre del Proceso de apropiación social del conocimiento para el fortalecimiento de cadenas productivas\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['ASC_P']['PASC_P']['PASC_CAD_P_TABLE']),'Nombre', 'Nombre del Proceso de apropiación social del conocimiento para el fortalecimiento de cadenas productivas')\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "            eh=DBEH['ASC_P']['PASC_P']['PASC_CAD_P_TABLE']\n",
    "\n",
    "            format_df(df, '11.ASC y Divulgación', var_w11, writer,eh)\n",
    "\n",
    "            var_w11 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # Divulgacion\n",
    "        # Piezas digitales\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['ASC_P']['DC_P']['DC_CD_P_TABLE']),'Título del proyecto','Título del proyecto para la generación de piezas digitales')\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "            eh=DBEH['ASC_P']['DC_P']['DC_CD_P_TABLE']\n",
    "\n",
    "            format_df(df, '11.ASC y Divulgación', var_w11, writer,eh)\n",
    "\n",
    "            var_w11 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # textuales\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['ASC_P']['DC_P']['DC_CON_P_TABLE']),'Título del proyecto','Título del proyecto para la generación de piezas Textuales (incluyendo cartillas, periódicos, revistas, etc.), Producción de estrategias transmediáticas y Desarrollos web')\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "            eh=DBEH['ASC_P']['DC_P']['DC_CON_P_TABLE']\n",
    "\n",
    "            format_df(df, '11.ASC y Divulgación', var_w11, writer,eh)\n",
    "\n",
    "            var_w11 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # produccion estrategia trasmediatica\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['ASC_P']['DC_P']['DC_TRA_P_TABLE']), 'Título del proyecto','Título del proyecto estrategia trasmediatica')\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "            eh=DBEH['ASC_P']['DC_P']['DC_TRA_P_TABLE']\n",
    "\n",
    "            format_df(df, '11.ASC y Divulgación', var_w11, writer,eh)\n",
    "\n",
    "            var_w11 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # desarrollo web\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['ASC_P']['DC_P']['DC_DES_P_TABLE']),'Título del proyecto','Título del proyecto desarrollo web')\n",
    "\n",
    "            eh=DBEH['ASC_P']['DC_P']['DC_DES_P_TABLE']\n",
    "\n",
    "            format_df(df, '11.ASC y Divulgación', var_w11, writer,eh)\n",
    "\n",
    "            var_w11 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # --- --- --- -- w11 -- -- -- -- -- -- --\n",
    "\n",
    "        # ---------------w12--------------------\n",
    "\n",
    "        # FRH\n",
    "\n",
    "        var_w12 = 0\n",
    "\n",
    "        # tesis doctorado\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['FRH_P']['TES_DOC_P']['TD_P_TABLE']), 'Título','Título de la tesis de doctorado')  ### ,veh = 2\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='FRH_P',startrow = var_rh)\n",
    "\n",
    "            eh=DBEH['FRH_P']['TES_DOC_P']['TD_P_TABLE']\n",
    "\n",
    "            format_df(df, '12.Formación y programas', var_w12, writer, eh,veh=2)\n",
    "\n",
    "            var_w12 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # tesis maestria\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['FRH_P']['TES_MAST_P']['TM_P_TABLE']),'Título','Título del trabajo de grado de maestría') ### veh = 2\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='FRH_P',startrow = var_rh)\n",
    "\n",
    "            eh=DBEH['FRH_P']['TES_MAST_P']['TM_P_TABLE']\n",
    "\n",
    "            format_df(df, '12.Formación y programas',var_w12, writer,eh,veh=2)\n",
    "\n",
    "            var_w12 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "        # tesis pregrado\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['FRH_P']['TES_PREG_P']['TP_P_TABLE']),'Título','Título del trabajo de grado de pregrado') ### veh = 2\n",
    "\n",
    "            #df.to_excel(writer,sheet_name='FRH_P',startrow = var_rh)\n",
    "\n",
    "            eh=DBEH['FRH_P']['TES_PREG_P']['TP_P_TABLE']\n",
    "\n",
    "            format_df(df, '12.Formación y programas',var_w12, writer,eh,veh = 2)\n",
    "\n",
    "            var_w12 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # asesoria programa academico\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['FRH_P']['ASE_PRG_ACA_P']['APGA_P_TABLE']),'Tipo','Nombre programa academico creado') \n",
    "\n",
    "            eh=DBEH['FRH_P']['ASE_PRG_ACA_P']['APGA_P_TABLE']\n",
    "\n",
    "            format_df(df, '12.Formación y programas', var_w12, writer,eh)\n",
    "\n",
    "            var_w12 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # asesoria creacion de cursos\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['FRH_P']['ASE_CRE_CUR_P']['ACC_P_TABLE']),'Tipo','Nombre curso creado')\n",
    "\n",
    "            eh=DBEH['FRH_P']['ASE_CRE_CUR_P']['ACC_P_TABLE']\n",
    "\n",
    "            format_df(df, '12.Formación y programas', var_w12, writer,eh)\n",
    "\n",
    "            var_w12 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "\n",
    "        # programa ondas\n",
    "        try:\n",
    "            df=rename_col(clean_df(DBG['FRH_P']['ASE_PRG_ONDAS_P']['APO_P_TABLE']),'Integrante','Integrante programa ondas')\n",
    "\n",
    "            eh=DBEH['FRH_P']['ASE_PRG_ONDAS_P']['APO_P_TABLE']\n",
    "\n",
    "            format_df(df, '12.Formación y programas', var_w12, writer,eh)\n",
    "\n",
    "            var_w12 += df.shape[0] + 3\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            pass\n",
    "        #----------------w12---------------------------\n",
    "        writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "passive-reader",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Nombre del grupo', 'Nombre del líder', 'COL Grupo', 'Revisar'], dtype='object') (100, 4)\n",
      "Index(['Nombre del grupo', 'Nombre del líder', 'COL Grupo', 'Revisar'], dtype='object') (100, 4)\n",
      "Index(['Nombre del grupo', 'Nombre del líder', 'COL Grupo', 'Revisar'], dtype='object') (100, 4)\n",
      "Index(['Nombre del grupo', 'Nombre del líder', 'COL Grupo', 'Revisar'], dtype='object') (24, 4)\n",
      "Message: Unable to locate element: //table[@id=\"grupos_avalados\"]//tr/td[3]/a\n",
      "\n",
      "out of cicle\n",
      "Grupo Tandem en Nano-bio-física\n",
      "https://scienti.minciencias.gov.co/institulac2-war/verificador/allproductosGr.do?categoriaP=NC_P&subcategoriaP=ART_IMP_P&aval=P&idGrupo=00000000020456&avalGr=T&codigoGrupo=&grCAv=2&nmeGrupo=Grupo%20Tandem%20en%20Nano-bio-f%EDsica&codIdGrupo=COL0202529\n",
      "https://scienti.minciencias.gov.co/institulac2-war/verificador/allproductosGr.do?categoriaP=NC_P&subcategoriaP=ART_ELE_P&aval=P&idGrupo=00000000020456&avalGr=T&codigoGrupo=&grCAv=2&nmeGrupo=Grupo%20Tandem%20en%20Nano-bio-f%EDsica&codIdGrupo=COL0202529\n",
      "https://scienti.minciencias.gov.co/institulac2-war/verificador/allproductosGr.do?categoriaP=NC_P&subcategoriaP=CAP_LIB_P&aval=P&idGrupo=00000000020456&avalGr=T&codigoGrupo=&grCAv=2&nmeGrupo=Grupo%20Tandem%20en%20Nano-bio-f%EDsica&codIdGrupo=COL0202529\n",
      "https://scienti.minciencias.gov.co/institulac2-war/verificador/allproductosGr.do?categoriaP=NC_P&subcategoriaP=PAT_P&aval=P&idGrupo=00000000020456&avalGr=T&codigoGrupo=&grCAv=2&nmeGrupo=Grupo%20Tandem%20en%20Nano-bio-f%EDsica&codIdGrupo=COL0202529\n",
      "https://scienti.minciencias.gov.co/institulac2-war/verificador/allproductosGr.do?categoriaP=FRH_P&subcategoriaP=TES_MAST_P&aval=P&idGrupo=00000000020456&avalGr=T&codigoGrupo=&grCAv=2&nmeGrupo=Grupo%20Tandem%20en%20Nano-bio-f%EDsica&codIdGrupo=COL0202529\n",
      "https://scienti.minciencias.gov.co/institulac2-war/verificador/allproductosGr.do?categoriaP=FRH_P&subcategoriaP=TES_PREG_P&aval=P&idGrupo=00000000020456&avalGr=T&codigoGrupo=&grCAv=2&nmeGrupo=Grupo%20Tandem%20en%20Nano-bio-f%EDsica&codIdGrupo=COL0202529\n",
      "0 NC_P ART_IMP_P\n",
      "0 NC_P ART_ELE_P\n",
      "0 NC_P CAP_LIB_P\n",
      "0 NC_P PAT_P\n",
      "0 FRH_P TES_MAST_P\n",
      "0 FRH_P TES_PREG_P\n",
      "0 NC_P ART_IMP_P\n",
      "0 NC_P ART_ELE_P\n",
      "0 NC_P CAP_LIB_P\n",
      "0 NC_P PAT_P\n",
      "0 FRH_P TES_MAST_P\n",
      "0 FRH_P TES_PREG_P\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-3babe65f604b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'InstituLAC Fail'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDIR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'InstituLAC'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-67d7b4a90788>\u001b[0m in \u001b[0;36mto_excel\u001b[0;34m(DB, dfg, DIR)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;31m# patentes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrename_col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDBG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NC_P'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PAT_P'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PAT_P_TABLE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Título del artículo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Título de la patente'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m###### veh=1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;31m#df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-593b88c32cfe>\u001b[0m in \u001b[0;36mclean_df\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;34m'remove innecesari collums'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unnamed:'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m  \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Revisar'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Avalar integrante'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mdfc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdfc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "DIR='InstituLAC'\n",
    "IGNORENONE=True #(False) #For developer mode\n",
    "CHECKPOINT=False #(False)\n",
    "\n",
    "os.makedirs(DIR, exist_ok=True)\n",
    "\n",
    "browser = login(user,password)\n",
    "time.sleep(2)\n",
    "\n",
    "if not CHECKPOINT:\n",
    "    dfg = get_dfg(browser)\n",
    "    time.sleep(2)\n",
    "    DB = gen_db(dfg,browser)\n",
    "    \n",
    "else:\n",
    "    DB, dfg = read_pickle(DIR)\n",
    "    \n",
    "nones = check_none(DB)\n",
    "\n",
    "if nones:\n",
    "    for cat_prod in nones:\n",
    "        time.sleep(2)\n",
    "        DB[cat_prod[0]][cat_prod[1]][cat_prod[2]] = get_missing_df(cat_prod[0], [cat_prod[1],cat_prod[2]], dfg, dict_tables, user, password)\n",
    "    \n",
    "to_pickle(DB,dfg)\n",
    "\n",
    "nones = check_none(DB)  \n",
    "    \n",
    "if nones:\n",
    "    if IGNORENONE:\n",
    "        DB=dummy_fix_df(DB)\n",
    "    else:\n",
    "        sys.exit('InstituLAC Fail')\n",
    "\n",
    "to_excel(DB,dfg,DIR='InstituLAC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "active-discretion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 NC_P ART_IMP_P\n",
      "0 NC_P ART_ELE_P\n",
      "0 NC_P CAP_LIB_P\n",
      "0 NC_P PAT_P\n",
      "0 FRH_P TES_MAST_P\n",
      "0 FRH_P TES_PREG_P\n"
     ]
    }
   ],
   "source": [
    "DB=dummy_fix_df(DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "coupled-directive",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-70afee2c298a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDIR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'InstituLAC'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-67d7b4a90788>\u001b[0m in \u001b[0;36mto_excel\u001b[0;34m(DB, dfg, DIR)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;31m# patentes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrename_col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDBG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NC_P'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PAT_P'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PAT_P_TABLE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Título del artículo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Título de la patente'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m###### veh=1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;31m#df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-593b88c32cfe>\u001b[0m in \u001b[0;36mclean_df\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;34m'remove innecesari collums'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unnamed:'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m  \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Revisar'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Avalar integrante'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0mdfc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdfc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "to_excel(DB,dfg,DIR='InstituLAC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "electric-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(user,password,DIR='InstituLAC',IGNORENONE=False,CHECKPOINT=False):\n",
    "    '''\n",
    "    IGNORENONE=True: is for developer mode;\n",
    "    '''\n",
    "    browser = login(user,password)\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "    if not CHECKPOINT:\n",
    "        dfg = get_dfg(browser)\n",
    "        time.sleep(2)\n",
    "        DB = gen_db(dfg,browser)\n",
    "\n",
    "    else:\n",
    "        DB, dfg = read_pickle(DIR)\n",
    "\n",
    "    nones = check_none(DB)\n",
    "\n",
    "    if nones:\n",
    "        for cat_prod in nones:\n",
    "            DB[cat_prod[0]][cat_prod[1]][cat_prod[2]] = get_missing_df(cat_prod[0], [cat_prod[1],cat_prod[2]], dfg, dict_tables, user, pasword)\n",
    "\n",
    "    to_pickle(DB,dfg)\n",
    "\n",
    "    nones = check_none(DB)  \n",
    "\n",
    "    if nones:\n",
    "        if IGNORENONE:\n",
    "            DB=dummy_fix_df(DB)\n",
    "        else:\n",
    "            sys.exit('InstituLAC Fail')\n",
    "\n",
    "    to_excel(DB,dfg,DIR='InstituLAC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-colors",
   "metadata": {},
   "source": [
    "# ------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
