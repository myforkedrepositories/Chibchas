{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incorrect-retailer",
   "metadata": {},
   "source": [
    "# Institulac "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "brilliant-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"max_rows\",100)\n",
    "#pd.set_option(\"display.max_columns\",100)\n",
    "pd.set_option(\"max_colwidth\",1000)\n",
    "\n",
    "import helium as h\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "everyday-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DICT CAT-PRODS-TAB\n",
    "with open('dict_tables.json') as file_json:\n",
    "    dict_tables=json.loads(file_json.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "polyphonic-queens",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Nombre del grupo', 'Nombre del líder', 'COL Grupo', 'Revisar'], dtype='object') (100, 4)\n",
      "Index(['Nombre del grupo', 'Nombre del líder', 'COL Grupo', 'Revisar'], dtype='object') (100, 4)\n",
      "Index(['Nombre del grupo', 'Nombre del líder', 'COL Grupo', 'Revisar'], dtype='object') (100, 4)\n",
      "Index(['Nombre del grupo', 'Nombre del líder', 'COL Grupo', 'Revisar'], dtype='object') (24, 4)\n",
      "Message: Unable to locate element: //table[@id=\"grupos_avalados\"]//tr/td[3]/a\n",
      "\n",
      "out of cicle\n"
     ]
    }
   ],
   "source": [
    "# login =\n",
    "# name_ins =\n",
    "# usser =\n",
    "# passw=\n",
    "\n",
    "# login\n",
    "browser = h.start_firefox('https://scienti.minciencias.gov.co/institulac2-war/')\n",
    "sleep=0.8\n",
    "\n",
    "#browser = h.start_firefox('https://scienti.minciencias.gov.co/institulac2-war/')\n",
    "time.sleep(sleep)\n",
    "h.click('Consulte Aquí')\n",
    "\n",
    "time.sleep(sleep)\n",
    "h.write('UNIVERSIDAD DE ANTIOQUIA',into='Digite el nombre de la Institución') # name ins\n",
    "\n",
    "time.sleep(sleep)\n",
    "h.click('Buscar')\n",
    "\n",
    "time.sleep(sleep)\n",
    "h.click(browser.find_element_by_id('list_instituciones'))\n",
    "\n",
    "time.sleep(sleep)\n",
    "\n",
    "time.sleep(sleep)\n",
    "h.select('seleccione una','UNIVERSIDAD DE ANTIOQUIA') # name_ins\n",
    "\n",
    "time.sleep(sleep)\n",
    "h.write('annyarango',into='Usuario')                  # user\n",
    "\n",
    "time.sleep(sleep)\n",
    "h.write('1@Silver', into='Contraseña')                # passw\n",
    "\n",
    "time.sleep(sleep)\n",
    "h.click(h.Button('Ingresar'))\n",
    "\n",
    "# cookie injection\n",
    "time.sleep(sleep)\n",
    "# implementation cookie injection\n",
    "\n",
    "# get current cookie and store\n",
    "new_cookie=browser.get_cookies()[0]\n",
    "    \n",
    "# create new_cookie with time_expire\n",
    "time_expire = (datetime(2022,1,1) - datetime(1970,1,1)).total_seconds()\n",
    "new_cookie['expiry'] = int(time_expire)\n",
    "    \n",
    "# delete cookie sites\n",
    "browser.delete_all_cookies()\n",
    "    \n",
    "# add new cookie\n",
    "browser.add_cookie(new_cookie)\n",
    "\n",
    "# navigation 1\n",
    "time.sleep(sleep)\n",
    "h.click('Aval')\n",
    "\n",
    "time.sleep(sleep)\n",
    "h.click('Avalar grupos')\n",
    "\n",
    "time.sleep(sleep)\n",
    "h.click('Grupos Avalados')\n",
    "\n",
    "# -- end login --\n",
    "\n",
    "# list of total groups\n",
    "#select max results per page\n",
    "h.wait_until(h.Text('Ver Reporte').exists)\n",
    "h.click(browser.find_element_by_xpath('//table[@id=\"grupos_avalados\"]//select[@name=\"maxRows\"]'))\n",
    "\n",
    "time.sleep(sleep)\n",
    "h.select(browser.find_element_by_xpath('//table[@id=\"grupos_avalados\"]//select[@name=\"maxRows\"]'),'100')\n",
    "\n",
    "# catch 1: groups info [name, lider, cod,  link to producs]  \n",
    "# schema\n",
    "# empty df\n",
    "# select max items per page\n",
    "# while until end\n",
    "# try:\n",
    "    # catch table\n",
    "    # preproces table\n",
    "    # catch urls\n",
    "    # add url colums\n",
    "    # add df\n",
    "    # click next page -> raise error\n",
    "# except Nosuchelement:\n",
    "    # break\n",
    "    \n",
    "# catch 1: list of groups\n",
    "dfg=pd.DataFrame()\n",
    "cont=True\n",
    "\n",
    "while cont:\n",
    "    \n",
    "    try:\n",
    "        # catch source\n",
    "        time.sleep(sleep)\n",
    "        source_g=browser.page_source\n",
    "        \n",
    "        # catch table\n",
    "        time.sleep(sleep)\n",
    "        df=pd.read_html(source_g, attrs={\"id\":\"grupos_avalados\"}, header=2)[0]\n",
    "        \n",
    "        # and preprocces it\n",
    "        c=[x for x in df.columns if x.find('Unnamed:') == -1]\n",
    "        dfgp=df[c][1:-1]\n",
    "        print(dfgp.columns,dfgp.shape)\n",
    "        \n",
    "        # catch urls\n",
    "        url=[a.get_attribute('href') for a in browser.find_elements_by_xpath('//table[@id=\"grupos_avalados\"]//td[5]/a')]\n",
    "        dfgp['Revisar'] = url\n",
    "        dfg=dfg.append(dfgp)\n",
    "        \n",
    "        # click next page. this instruction rise error of the end. \n",
    "        h.click(browser.find_element_by_xpath('//table[@id=\"grupos_avalados\"]//tr/td[3]/a'))\n",
    "        \n",
    "    except NoSuchElementException as e:\n",
    "        \n",
    "        print(e)\n",
    "        print('out of cicle')\n",
    "        break\n",
    "        \n",
    "    time.sleep(sleep)\n",
    "    time.sleep(sleep)\n",
    "    \n",
    "dfg = dfg.reset_index(drop=True)\n",
    "assert dfg.shape[0] == 324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "circular-conservation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grupo Tandem en Nano-bio-física \n",
      " https://scienti.minciencias.gov.co/institulac2-war/ReGrupoInstitucion/query.do?avalGr=T&codigoGrupo=&idGrupo=00000000020456\n",
      "Promoción de la Salud \n",
      " https://scienti.minciencias.gov.co/institulac2-war/ReGrupoInstitucion/query.do?avalGr=T&codigoGrupo=&idGrupo=00000000001767\n"
     ]
    }
   ],
   "source": [
    "print(dfg['Nombre del grupo'][0:1].values[0],'\\n',dfg['Revisar'][:1].values[0])\n",
    "\n",
    "print(dfg['Nombre del grupo'][-1:].values[0],'\\n',dfg['Revisar'][-1:].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wooden-buddy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grupo Académico de Epidemiología Clínica\n"
     ]
    }
   ],
   "source": [
    "time.sleep(sleep*2)\n",
    "\n",
    "LD = [] # \n",
    "LP = []\n",
    "LR = [] \n",
    "for idx in dfg.index[34:35]:\n",
    "    \n",
    "    # create db\n",
    "    DBG = {}\n",
    "    # part info group\n",
    "    print(dfg.loc[idx,'Nombre del grupo'])\n",
    "\n",
    "    # specific group url\n",
    "    time.sleep(sleep)\n",
    "    url_group = dfg.loc[idx,'Revisar']\n",
    "\n",
    "    # go to url group\n",
    "    time.sleep(sleep)\n",
    "    browser.get(url_group)\n",
    "\n",
    "    # catch two tables: info grupo and  members\n",
    "    source=browser.page_source\n",
    "\n",
    "    #info\n",
    "    l_info=pd.read_html(source, match='Nombre Grupo')\n",
    "    info_g=l_info[3].pivot(columns=0,values=1)\n",
    "    \n",
    "    # STORE INFO_GROUP\n",
    "    DBG['Info_group'] = info_g\n",
    "\n",
    "    # members\n",
    "    l_int = pd.read_html(source,attrs={'id':'tblIntegrantes'},header=2)\n",
    "    mem_g=l_int[0]\n",
    "    \n",
    "    # STORE_MEMBERS\n",
    "    DBG['Members'] =  mem_g\n",
    "\n",
    "    # Products\n",
    "\n",
    "    #time.sleep(sleep*5) # time time time !!!\n",
    "    h.wait_until(lambda: browser.find_element_by_xpath('//td[@id=\"bodyPrincipal\"]//a[text()=\"Ver productos\"]') is not None)\n",
    "    h.click(browser.find_element_by_xpath('//td[@id=\"bodyPrincipal\"]//a[text()=\"Ver productos\"]'))\n",
    "\n",
    "    # products by belongs to  # time time time\n",
    "    #time.sleep(sleep*7)       # time time time\n",
    "    h.wait_until(lambda: browser.find_element_by_xpath('//*[@id=\"ProdsPertenecia\"]') is not None)\n",
    "    h.click(browser.find_element_by_xpath('//*[@id=\"ProdsPertenecia\"]'))\n",
    "\n",
    "    time.sleep(sleep)\n",
    "    url_products=browser.current_url\n",
    "\n",
    "\n",
    "    # map all products, store those id categories that amount is different to 0 and id products asociated.\n",
    "    # make queries with combinations of categories and products\n",
    "    # make urls with diferent combinations of quieries\n",
    "    # go to each of urls\n",
    "    # load page source\n",
    "    # catch table ( or tables) asociated with categories and products\n",
    "    # store tables\n",
    "\n",
    "    report = ''\n",
    "\n",
    "    list_of_prods =[] #[[cat,prod],[cat,prod]...]\n",
    "\n",
    "    # map all products and get products and subs diff to cero\n",
    "    for i in browser.find_elements_by_xpath('//div[@id=\"accordionCatgP\"]/h3'):\n",
    "\n",
    "        report += i.text + '\\n' \n",
    "        report += i.get_attribute('id') + '\\n'     \n",
    "\n",
    "        time.sleep(sleep)\n",
    "        h.click(i)\n",
    "        \n",
    "        # cat\n",
    "        cat_ = int(re.findall(r'\\d+',i.text)[0])\n",
    "        \n",
    "        # create cat key in dict, for estore diferents products by this categori: 'NC_': {'ART_E':TABLE,\n",
    "        #                                                                                 'ART_IMP':TABLE}\n",
    "        if cat_ > 0:\n",
    "            DBG[i.get_attribute('id')] = {}\n",
    "            \n",
    "        \n",
    "        for j in browser.find_elements_by_xpath('//div[@aria-labelledby=\"%s\"]/h3' % i.get_attribute('id')):\n",
    "\n",
    "            report += '\\t' + j.text + '\\n' \n",
    "            report += '\\t' + j.get_attribute('id') + '\\n'\n",
    "            \n",
    "            #prod\n",
    "            pro_ = int(re.findall(r'\\d+', j.text)[0])\n",
    "\n",
    "            if cat_ > 0 and pro_ > 0:  \n",
    "                \n",
    "                list_of_prods.append([i.get_attribute('id'),j.get_attribute('id')])\n",
    "\n",
    "        time.sleep(sleep) \n",
    "        # h.click(a)\n",
    "        h.click(i)\n",
    "\n",
    "    # print(report)\n",
    "    # print('\\n')\n",
    "    # print('--------------------------------')\n",
    "    time.sleep(sleep*2)\n",
    "    \n",
    "    tables=[]\n",
    "    \n",
    "    for p in range(len(list_of_prods)):\n",
    "\n",
    "            # make query \n",
    "            query='categoriaP=%s&subcategoriaP=%s&aval=P' % (list_of_prods[p][0],list_of_prods[p][1])\n",
    "\n",
    "            # make url query\n",
    "            url_query = url_products.split('?')[0] + '?' + query + '&' + url_products.split('?')[1]\n",
    "\n",
    "            # retrieve id asociated tables\n",
    "            table_id = dict_tables[list_of_prods[p][0]][list_of_prods[p][1]]\n",
    "\n",
    "            # go to url product by group\n",
    "            time.sleep(sleep)\n",
    "        \n",
    "            browser.get(url_query)\n",
    "\n",
    "            # load page\n",
    "            time.sleep(sleep)\n",
    "            page_source = browser.page_source\n",
    "\n",
    "            # catch tables\n",
    "            if isinstance(table_id,str): # case one table\n",
    "\n",
    "                # catch title table\n",
    "                \n",
    "                title_table = browser.find_element_by_xpath('//div/p[@class=\"titulo_tabla\"]').text \n",
    "                # cathc table\n",
    "                #print(table_id)\n",
    "                time.sleep(sleep*2)\n",
    "                table = pd.read_html(page_source,attrs={'id':table_id}, header=2)[0][1:-1]\n",
    "\n",
    "                # store table\n",
    "                DBG[list_of_prods[p][0]][list_of_prods[p][1]] = {table_id:table}\n",
    "                # ---- in building ----\n",
    "\n",
    "            elif isinstance(table_id, list): # case multiple tables\n",
    "\n",
    "                for i in range(len(table_id)):\n",
    "\n",
    "                    # catch title specific table \n",
    "                    title_table = browser.find_elements_by_xpath('//div/p[@class=\"titulo_tabla\"]')[i].text\n",
    "\n",
    "                    # catch table\n",
    "                    table = pd.read_html(page_source,attrs={'id':table_id[i]}, header=2)[0][1:-1]\n",
    "\n",
    "                    # store table\n",
    "                    DBG[list_of_prods[p][0]][list_of_prods[p][1]] = {table_id[i]:table}\n",
    "                    # -----------\n",
    "    LD.append(DBG)\n",
    "    LP.append(list_of_prods)\n",
    "    LR.append(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "productive-blend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevo Conocimiento (53)\n",
      "NC_P\n",
      "\tArtículos publicados en revistas especializadas - Impresos (18)\n",
      "\tART_IMP_P\n",
      "\tArtículos publicados en revistas especializadas - Electrónicos (30)\n",
      "\tART_ELE_P\n",
      "\tLibros resultado de investigación (1)\n",
      "\tLIB_P\n",
      "\tCapítulos en Libro resultado de investigación (4)\n",
      "\tCAP_LIB_P\n",
      "\tNotas científicas (0)\n",
      "\tNOT_CIE_P\n",
      "\tPatentes (0)\n",
      "\tPAT_P\n",
      "\tObras o productos de investigación creación en Artes, Arquitectura y Diseño (0)\n",
      "\tPRD_INV_ART_P\n",
      "\tVariedad Vegetal (0)\n",
      "\tVAR_VEG_P\n",
      "\tNueva Raza Animal (0)\n",
      "\tVAR_ANI_P\n",
      "\tPoblaciones mejoradas de razas pecuarias (0)\n",
      "\tRAZ_PEC_P\n",
      "\tTraducciones Filológicas y Edición de Fuentes (0)\n",
      "\tTRA_FIL_P\n",
      "Desarrollo Tecnológico e Innovación (0)\n",
      "DTI_P\n",
      "\tDiseño industrial (0)\n",
      "\tDIS_IND_P\n",
      "\tCircuitos integrados (0)\n",
      "\tCIR_INT_P\n",
      "\tSoftware (0)\n",
      "\tSOFT_P\n",
      "\tProductos nutraceúticos (0)\n",
      "\tNUTRA_P\n",
      "\tColecciones científicas (0)\n",
      "\tCOL_CIENT_P\n",
      "\tNuevos registros científicos (0)\n",
      "\tREG_CIENT_P\n",
      "\tPlanta Piloto (0)\n",
      "\tPLT_PIL_P\n",
      "\tPrototipo Industrial (0)\n",
      "\tPRT_IND_P\n",
      "\tSecreto Industrial (0)\n",
      "\tSEC_IND_P\n",
      "\tProtocolos de vigilancia epidemiológica (0)\n",
      "\tPROT_VIG_EPID_P\n",
      "\tEmpresas de base tecnológica (0)\n",
      "\tEMP_BSE_TEC_P\n",
      "\tEmpresas creativas y culturales (0)\n",
      "\tEMP_CRE_CUL_P\n",
      "\tInnovaciones generadas en la gestión empresarial (0)\n",
      "\tINN_GES_EMP_P\n",
      "\tInnovaciones en procesos o procedimientos (0)\n",
      "\tINN_PROC_P\n",
      "\tRegulaciones, normas, reglamentos o legislaciones (0)\n",
      "\tREG_NORM_REGL_LEG_P\n",
      "\tConceptos técnicos (0)\n",
      "\tCONP_TEC_P\n",
      "\tRegistros de Acuerdos de licencia para la explotación de obras de AAD (0)\n",
      "\tREG_AAD_P\n",
      "\tSignos distintivos (0)\n",
      "\tSIG_DIS_P\n",
      "Apropiación social del conocimiento y Divulgación Pública de la Ciencia (24)\n",
      "ASC_P\n",
      "\tGeneración de contenido impreso (2)\n",
      "\tGEN_CONT_IMP_P\n",
      "\tEventos Científicos (9)\n",
      "\tEV_CIENT_P\n",
      "\tRed de conocimiento especializado (1)\n",
      "\tRED_P\n",
      "\tTalleres de creación (0)\n",
      "\tTALL_CRE_P\n",
      "\tEventos Artísticos (0)\n",
      "\tEVE_ART_P\n",
      "\tDocumento de trabajo (12)\n",
      "\tDOC_TRAB_P\n",
      "\tNuevas secuencias genéticas (0)\n",
      "\tSEC_GENE_P\n",
      "\tBoletín divulgativo de resultado de investigación (0)\n",
      "\tBOL_RES_INV_P\n",
      "\tConsultoría cientìfico tecnológica (0)\n",
      "\tCON_INF_TEC_P\n",
      "\tEdición (0)\n",
      "\tEDIC_P\n",
      "\tInformes (0)\n",
      "\tINF_TEC_P\n",
      "\tProcesos de apropiación social del conocimiento (PASC) (0)\n",
      "\tPASC_P\n",
      "\tProductos de divulgación pública de la ciencia (0)\n",
      "\tDC_P\n",
      "Formación del Recurso Humano (53)\n",
      "FRH_P\n",
      "\tDirecciones de tesis de doctorado (2)\n",
      "\tTES_DOC_P\n",
      "\tDirecciones de trabajo de grado de maestría (33)\n",
      "\tTES_MAST_P\n",
      "\tDirecciones de trabajos de pregrado (0)\n",
      "\tTES_PREG_P\n",
      "\tProyecto de investigación y desarrollo (7)\n",
      "\tPROY_INV_DES_P\n",
      "\tProyecto de investigación + creación (0)\n",
      "\tPROY_INV_CRE_P\n",
      "\tProyecto de investigación, desarrollo e innovación (ID+I) (0)\n",
      "\tPROY_INV_DES_INN_P\n",
      "\tProyecto de extensión y de responsabilidad social en CTeI (0)\n",
      "\tPROY_INV_RESP_SOC_P\n",
      "\tApoyo a la creación de programas académicos (0)\n",
      "\tASE_PRG_ACA_P\n",
      "\tApoyo a la creación de cursos (11)\n",
      "\tASE_CRE_CUR_P\n",
      "\tAcompañamiento y asesoría de línea temática del Programa Ondas (0)\n",
      "\tASE_PRG_ONDAS_P\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(LR[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "satisfactory-kingston",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['NC_P', 'ART_IMP_P'],\n",
       " ['NC_P', 'ART_ELE_P'],\n",
       " ['NC_P', 'LIB_P'],\n",
       " ['NC_P', 'CAP_LIB_P'],\n",
       " ['ASC_P', 'GEN_CONT_IMP_P'],\n",
       " ['ASC_P', 'EV_CIENT_P'],\n",
       " ['ASC_P', 'RED_P'],\n",
       " ['ASC_P', 'DOC_TRAB_P'],\n",
       " ['FRH_P', 'TES_DOC_P'],\n",
       " ['FRH_P', 'TES_MAST_P'],\n",
       " ['FRH_P', 'PROY_INV_DES_P'],\n",
       " ['FRH_P', 'ASE_CRE_CUR_P']]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LP[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "collective-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(LD)==len(LP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "provincial-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(df):\n",
    "    \n",
    "    info= {\n",
    "        'Nombre_Grupo' : df['Nombre Grupo'].dropna().iloc[0],\n",
    "\n",
    "        'Nombre_Lider' : df['Nombre Líder'].dropna().iloc[0],\n",
    "\n",
    "        'Nombre_Area'  : df['Nombre área'].dropna().iloc[0]\n",
    "    }\n",
    "    \n",
    "    dfi = pd.DataFrame(info, index=[0])\n",
    "  \n",
    "    \n",
    "    return dfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "korean-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "                '1': 'C',\n",
    "                '2': 'D',\n",
    "                '3': 'E',\n",
    "                '4': 'F',\n",
    "                '5': 'G',\n",
    "                '6': 'H',\n",
    "                '7': 'I',\n",
    "                '8': 'J',\n",
    "                '9': 'K',\n",
    "                '10': 'L',\n",
    "                '11': 'M',\n",
    "                '12': 'N',\n",
    "                '13': 'O',\n",
    "                '14': 'P',\n",
    "                '15': 'Q'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "illegal-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    'remove innecesari collums'\n",
    "    c=[x for x in df.columns if x.find('Unnamed:') == -1 and  x.find('Revisar') == -1]\n",
    "    dfc=df[c]\n",
    "    return dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "detected-pricing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df(df, sheet_name, start_row, writer):\n",
    "    'format headers'\n",
    "    \n",
    "    df.to_excel(writer,sheet_name, startrow = start_row+1, startcol=2,index = False)\n",
    "\n",
    "    # Get the xlsxwriter workbook and worksheet objects.\n",
    "    worksheet = writer.sheets[sheet_name]\n",
    "\n",
    "    start,end = 1,df.shape[1]\n",
    "\n",
    "    m_range = d.get(str(start)) + str(start_row + 1) + ':' + d.get(str(end)) + str(start_row +1)\n",
    "\n",
    "    worksheet.merge_range(m_range, 'Información suministrada por la Vicerrectoría de Investigación', merge_format)\n",
    "    \n",
    "    worksheet.set_row_pixels(start_row+1, 120)\n",
    "    #worksheet.set_column('C:C',30,general)\n",
    "    \n",
    "    if sheet_name == '3.Integrantes grupo':\n",
    "        worksheet.set_column('C:C', 10,general)\n",
    "        worksheet.set_column('D:D', 30,general)\n",
    "        worksheet.set_column('E:G', 15,general)\n",
    "    elif sheet_name== 'NC_':\n",
    "        worksheet.set_column('C:C', 30,general)\n",
    "        worksheet.set_column('D:K', 20,general)\n",
    "    elif sheet_name == 'FRH_P':\n",
    "        worksheet.set_column('C:C', 30,general)\n",
    "        worksheet.set_column('D:K', 20,general)\n",
    "    else:\n",
    "        worksheet.set_column('C:C', 30,general)\n",
    "        worksheet.set_column('D:K', 20,general)\n",
    "        \n",
    "        \n",
    "    worksheet.write(start_row+1, 0, 'VoBo de VRI', merge_format)\n",
    "    # Add a header format.\n",
    "    \n",
    "    fmt_header = workbook.add_format({\n",
    "        'bold': True,\n",
    "        'align': 'center',    \n",
    "        'text_wrap': True,\n",
    "        'valign': 'top',\n",
    "        'fg_color': '#33A584',\n",
    "        'font_color': '#FFFFFF',\n",
    "        'border': 1})\n",
    "    \n",
    "    \n",
    "    # Write the column headers with the defined format.\n",
    "    for col_num, value in enumerate(df.columns.values):\n",
    "        worksheet.write(start_row+1, col_num + 2, value, fmt_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "interim-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_info(df, writer, sheet_name):\n",
    "    \n",
    "    '''format worksheet'''\n",
    "    \n",
    "    workbook=writer.book\n",
    "    \n",
    "    normal=workbook.add_format({'font_size':12,'text_wrap':True})\n",
    "    \n",
    "    merge_format = workbook.add_format({\n",
    "    'bold': 1,\n",
    "    'border':1,\n",
    "    'text_wrap': True,    \n",
    "    'align': 'center',\n",
    "    'valign': 'vcenter',\n",
    "    'font_color': 'black'})\n",
    "    \n",
    "    fmt_header = workbook.add_format({\n",
    "        'align': 'center',    \n",
    "        'text_wrap': True,\n",
    "        'valign': 'top',\n",
    "        'fg_color': '#33A584',\n",
    "        'font_color': '#FFFFFF',\n",
    "        'border': 1})\n",
    "    \n",
    "    # write df\n",
    "    start_row = 5\n",
    "    start_col = 3\n",
    "    \n",
    "    df.to_excel(writer, sheet_name, startrow =start_row, startcol=start_col,index = False)\n",
    "\n",
    "    # get worksheet object\n",
    "    worksheet = writer.sheets[sheet_name]\n",
    "    \n",
    "    for col_num, value in enumerate(df.columns.values):\n",
    "        worksheet.write(start_row, col_num + 3, value, fmt_header)\n",
    "    \n",
    "    #Prepare image insertion: See → https://xlsxwriter.readthedocs.io/example_images.html\n",
    "    worksheet.set_column('A:A', 15)\n",
    "    worksheet.set_column('B:B', 15)\n",
    "    worksheet.insert_image('A1', 'img/udea.jpeg')\n",
    "    \n",
    "    # title 1 UNIVERSIDAD DE ANTIOQUIA\n",
    "    title = workbook.add_format({'font_size':16,'center_across':True})\n",
    "\n",
    "    # title 2 Vicerrectoria de Investigación\n",
    "    title2 = workbook.add_format({'font_size':16,'center_across':True})\n",
    "   \n",
    "    # sub title 2 datos identificacion contacto\n",
    "    title3 = workbook.add_format({'font_size':12,'center_across':True})\n",
    "    \n",
    "    # merge d1:f1\n",
    "    worksheet.merge_range('D1:F1', 'UNIVERSIDAD DE ANTIOQUIA', title)\n",
    "        \n",
    "    # merge d2:f2\n",
    "    worksheet.merge_range('D2:F2', ' Vicerrectoria de Investigación', title2)\n",
    "    \n",
    "    # merge d3:f3\n",
    "    worksheet.merge_range('D3:F3', ' Datos de identificación y contacto', title3)\n",
    "    \n",
    "    # d5:f5\n",
    "    worksheet.merge_range('D5:F5','Identificación del Grupo',merge_format)\n",
    "        \n",
    "    # d9:f9\n",
    "    worksheet.merge_range('D9:F9','Identificación del Centro de Investigación',merge_format)\n",
    "    # write \n",
    "    a='Nombre del Centro, Instituto o Corporación'\n",
    "    worksheet.write('D10',a, fmt_header)\n",
    "    worksheet.set_column('D10:D10',30, fmt_header)\n",
    "    \n",
    "    b='Nombre completo del Jefe de Centro, Instituto o Corporación'\n",
    "    worksheet.write('E10',b, fmt_header) \n",
    "    worksheet.set_column('E10:E10',30, fmt_header)\n",
    "    \n",
    "    c='Email'\n",
    "    worksheet.write('F10',c, fmt_header) \n",
    "    worksheet.set_column('F10:F10',30, fmt_header)\n",
    "    \n",
    "    # d13:f13\n",
    "    worksheet.merge_range('D13:F13','Identificación de quien diligencia el formato',merge_format)\n",
    "    a='Nombre completo del encargado de diligenciar el formato'\n",
    "    worksheet.write('D14',a, fmt_header)\n",
    "    worksheet.set_column('D14:D14',30, normal)\n",
    "    \n",
    "    b='Email'\n",
    "    worksheet.write('E14',b, fmt_header) \n",
    "    worksheet.set_column('E14:E14',30, normal)\n",
    "    \n",
    "    c='Teléfono de contacto'\n",
    "    worksheet.write('F14',c, fmt_header) \n",
    "    worksheet.set_column('F14:F14',30, normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "disturbed-filename",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def format_df(df, sheet_name, start_row, writer):\\n    'format headers'\\n    \\n    # Get the xlsxwriter workbook and worksheet objects.\\n    worksheet = writer.sheets[sheet_name]\\n\\n    # Add a header format.\\n    \\n    fmt_header = workbook.add_format({\\n        'bold': True,\\n        'align': 'center',    \\n        'text_wrap': True,\\n        'valign': 'top',\\n        'fg_color': '#33A584',\\n        'font_color': '#FFFFFF',\\n        'border': 1})\\n    \\n    \\n    # Write the column headers with the defined format.\\n    for col_num, value in enumerate(df.columns.values):\\n        worksheet.write(start_row, col_num + 1, value, fmt_header) # !!!\""
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def format_df(df, sheet_name, start_row, writer):\n",
    "    'format headers'\n",
    "    \n",
    "    # Get the xlsxwriter workbook and worksheet objects.\n",
    "    worksheet = writer.sheets[sheet_name]\n",
    "\n",
    "    # Add a header format.\n",
    "    \n",
    "    fmt_header = workbook.add_format({\n",
    "        'bold': True,\n",
    "        'align': 'center',    \n",
    "        'text_wrap': True,\n",
    "        'valign': 'top',\n",
    "        'fg_color': '#33A584',\n",
    "        'font_color': '#FFFFFF',\n",
    "        'border': 1})\n",
    "    \n",
    "    \n",
    "    # Write the column headers with the defined format.\n",
    "    for col_num, value in enumerate(df.columns.values):\n",
    "        worksheet.write(start_row, col_num + 1, value, fmt_header) # !!!'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "dated-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/42370977/how-to-save-a-new-sheet-in-an-existing-excel-file-using-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "bridal-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store info group in dict\n",
    "    # each key store list of tables related with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "sudden-discrimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE GROUP IMPLEMENTATION\n",
    "idxx = 34\n",
    "\n",
    "# DATA\n",
    "DBG = LD[0]\n",
    "\n",
    "### excel name\n",
    "name = 'AA_Plantilla_Formato de verificación de información_GrupLAC_894-2021_%s'\n",
    "\n",
    "# initialize object= output excel file\n",
    "writer = pd.ExcelWriter(name % dfg.loc[idxx,'COL Grupo']+'.xlsx', engine='xlsxwriter')\n",
    "\n",
    "#Global variables\n",
    "abstract_text='VERIFICACIÓN DE INFORMACIÓN PARA OTORGAR AVAL A LOS GRUPOS DE INVESTIGACIÓN  E INVESTIGADORES PARA SU PARTICIPACIÓN EN LA CONVOCATORIA 894 DE 2021 DE MINCIENCIAS'\n",
    "instructions='''Los grupos de investigación e investigadores de la Universidad de Antioquia que deseen participar en la Convocatoria Nacional para el reconocimiento y medición de grupos de investigación, desarrollo tecnológico o de innovación y para el reconocimiento de investigadores del Sistema Nacional de Ciencia, Tecnología e Innovación - SNCTI, 894 de 2021, deben presentar la información actualizada en las plataformas CvLAC y GrupLAC validada por el Centro de Investigación en el presente formato, y respaldada en el repositorio digital de evidencias dispuesto para este fin, para la obtención del aval institucional por parte de la Vicerrectoría de Investigación. \n",
    "\n",
    "La información a validar corresponde a los años 2019-2020 y aquella que entra en la ventana de observación y debe ser modificada según el Modelo de medición de grupos. La validación comprende:\n",
    "\n",
    "1. Verificación de la vinculación de los integrantes a la Universidad de Antioquia y al grupo de investigación.  Diligenciar los campos solicitados. \n",
    "\n",
    "2. Verificación de la producción de GNC, DTeI, ASC y FRH, en los campos habilitados en cada hoja de este formato. Las evidencias requeridas para los productos deben ser anexadas al repositorio digital asignado al grupo y se deben enlazar a cada producto.  \n",
    "\n",
    "Este documento debe ser diligenciado en línea.\n",
    "\n",
    "De antemano, la Vicerrectoría de Investigación agradece su participación en este ejercicio, que resulta de vital importancia para llevar a buen término la Convocatoria de Reconocimiento y Medición de Grupos de Investigación\n",
    "'''\n",
    "#Final part of the first sheet\n",
    "datos=pd.read_excel('https://github.com/restrepo/InstituLAC/raw/main/data/template_data.xlsx')\n",
    "\n",
    "#Capture xlsxwriter object \n",
    "# IMPORTANT → workbook is the same object used in the official document at https://xlsxwriter.readthedocs.io\n",
    "workbook=writer.book\n",
    "#***************\n",
    "#Styles as explained in https://xlsxwriter.readthedocs.io\n",
    "general=workbook.add_format({'text_wrap':True})\n",
    "title=workbook.add_format({'font_size':28,'center_across':True})\n",
    "subtitle=workbook.add_format({'font_size':24,'center_across':True})\n",
    "abstract=workbook.add_format({'font_size':20,'center_across':True,'text_wrap':True})\n",
    "normal=workbook.add_format({'font_size':12,'text_wrap':True})\n",
    "\n",
    "merge_format = workbook.add_format({\n",
    "    'bold': 1,\n",
    "    'border':1,\n",
    "    'text_wrap': True,    \n",
    "    'align': 'center',\n",
    "    'valign': 'vcenter',\n",
    "    'font_color': 'blue'})\n",
    "\n",
    "fmt_header = workbook.add_format({\n",
    "    'bold': True,\n",
    "    'align': 'center',    \n",
    "    'text_wrap': True,\n",
    "    'valign': 'top',\n",
    "    'fg_color': '#33A584',\n",
    "    'font_color': '#FFFFFF',\n",
    "    'border': 1})\n",
    "#***************\n",
    "#Creates the first work-sheet\n",
    "#IMPORTANT → worksheet is the same object  used in the official document at https://xlsxwriter.readthedocs.io\n",
    "worksheet=workbook.add_worksheet(\"1.Presentación\")\n",
    "#Prepare image insertion: See → https://xlsxwriter.readthedocs.io/example_images.html\n",
    "worksheet.set_column('A:A', 15)\n",
    "worksheet.set_column('B:B', 15)\n",
    "worksheet.insert_image('A1', 'img/udea.jpeg')\n",
    "#Prepare text insertion: See  → https://xlsxwriter.readthedocs.io/example_images.html\n",
    "worksheet.set_column('C:C', 140,general)\n",
    "worksheet.set_row_pixels(0, 60)\n",
    "#Texts\n",
    "worksheet.write('C1', 'UNIVERSIDAD DE ANTIOQUIA',title)\n",
    "worksheet.set_row_pixels(2, 60)\n",
    "worksheet.write('C3', 'VICERRECTORÍA DE INVESTIGACIÓN',subtitle)\n",
    "worksheet.set_row_pixels(5, 100)\n",
    "worksheet.write('C6', abstract_text,abstract)\n",
    "worksheet.set_row_pixels(8, 40)\n",
    "worksheet.write('C9','PRESENTACIÓN DEL EJERCICIO',\n",
    "                workbook.add_format({'font_size':18,'center_across':True}) )\n",
    "worksheet.set_row_pixels(10, 320)\n",
    "worksheet.write('C11',instructions,normal )\n",
    "#*** ADD PANDAS DATAFRAME IN SPECIFIC POSITION ****\n",
    "#Add a data Frame in some specific position. See → https://stackoverflow.com/a/43510881/2268280\n",
    "#                                       See also → https://xlsxwriter.readthedocs.io/working_with_pandas.html\n",
    "writer.sheets[\"1.Presentación\"]=worksheet\n",
    "datos.to_excel(writer,sheet_name=\"1.Presentación\",startrow=12,startcol=2,index=False)\n",
    "#**************************************************\n",
    "#Fix columns heights for long text\n",
    "worksheet.set_row_pixels(17, 40)\n",
    "worksheet.set_row_pixels(18, 40)\n",
    "worksheet.set_row_pixels(19, 40)\n",
    "worksheet.set_row_pixels(20, 40)\n",
    "worksheet.set_row_pixels(22, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "educated-korean",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # Add extra headers\\nworksheet2.write(1, 0, 'VoBo de VRI', merge_format)\\n#New columns\\nextra_url='Si no tiene URL o DOI agregue una evidencia en el repositorio digital y genere un hipervínculo en este campo'\\ncols=['DOI',extra_url,'¿El producto cumple con los requisitos para ser avalado?']\\nfor col , value in enumerate(cols):\\n    worksheet2.write(1, col+2+table.columns.size, value, fmt_header)\\nworksheet2.set_column('L:L',20)\\nworksheet2.set_column('M:M',20)\\n#Creates a set of cells with a drop-down menu Sí/No. See → https://xlsxwriter.readthedocs.io/working_with_data_validation.html\\nworksheet2.data_validation('M3:M{}'.format(table.shape[0]+2), {'validate': 'list',\\n                                  'source': ['Sí', 'No']})\\n\\nworkbook.close()\\n\""
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' # Add extra headers\n",
    "worksheet2.write(1, 0, 'VoBo de VRI', merge_format)\n",
    "#New columns\n",
    "extra_url='Si no tiene URL o DOI agregue una evidencia en el repositorio digital y genere un hipervínculo en este campo'\n",
    "cols=['DOI',extra_url,'¿El producto cumple con los requisitos para ser avalado?']\n",
    "for col , value in enumerate(cols):\n",
    "    worksheet2.write(1, col+2+table.columns.size, value, fmt_header)\n",
    "worksheet2.set_column('L:L',20)\n",
    "worksheet2.set_column('M:M',20)\n",
    "#Creates a set of cells with a drop-down menu Sí/No. See → https://xlsxwriter.readthedocs.io/working_with_data_validation.html\n",
    "worksheet2.data_validation('M3:M{}'.format(table.shape[0]+2), {'validate': 'list',\n",
    "                                  'source': ['Sí', 'No']})\n",
    "\n",
    "workbook.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-milan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "planned-formation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFO GROUP\n",
    "df=get_info(DBG['Info_group'])\n",
    "format_info(df, writer, '2.Datos de contacto')\n",
    "\n",
    "# WORKSHEET 1\n",
    "df = clean_df(DBG['Members'])\n",
    "format_df(df, '3.Integrantes grupo', 1, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "inappropriate-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NC_P ### WORKSHEET 2\n",
    "     \n",
    "var_nc = 0\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['NC_P']['ART_IMP_P']['ART_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc+1)\n",
    "\n",
    "    format_df(df, 'NC_P', var_nc, writer=writer)\n",
    "        \n",
    "    var_nc += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['NC_P']['ART_ELE_P']['ART_E_P_TABLE'])\n",
    "    \n",
    "    #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "    format_df(df, 'NC_P', var_nc, writer=writer)\n",
    "        \n",
    "    var_nc += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['NC_P']['LIB_P']['LIB_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "    format_df(df, 'NC_P', var_nc, writer=writer)\n",
    "        \n",
    "    var_nc += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['NC_P']['CAP_LIB_P']['CAP_LIB_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "    format_df(df, 'NC_P', var_nc, writer=writer)\n",
    "        \n",
    "    var_nc += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['NC_P']['NOT_CIE_P']['NOT_CIE_P_TABLE'])\n",
    "    \n",
    "    #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "    format_df(df, 'NC_P', var_nc, writer=writer)\n",
    "        \n",
    "    var_nc += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['NC_P']['PAT_P']['PAT_P_TABLE'])\n",
    "    \n",
    "    #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "    format_df(df, 'NC_P', var_nc, writer=writer)\n",
    "        \n",
    "    var_nc += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['NC_P']['PRD_INV_ART_P']['PAAD_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "    format_df(df, 'NC_P', var_nc, writer=writer)\n",
    "        \n",
    "    var_nc += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['NC_P']['VAR_VEG_P']['VV_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "    format_df(df, 'NC_P', var_nc, writer=writer)\n",
    "        \n",
    "    var_nc += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['NC_P']['VAR_ANI_P']['VA_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "    format_df(df, 'NC_P', var_nc, writer=writer)\n",
    "        \n",
    "    var_nc += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['NC_P']['RAZ_PEC_P']['RAZ_PEC_P_TABLE'])\n",
    "    \n",
    "    #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "    format_df(df, 'NC_P', var_nc, writer=writer)\n",
    "        \n",
    "    var_nc += df.shape[0] + 3\n",
    "\n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['NC_P']['TRA_FIL_P']['TRA_FIL_P_TABLE'])\n",
    "    \n",
    "    #df.to_excel(writer,sheet_name='NC_P',startrow = var_nc)\n",
    "\n",
    "    format_df(df, 'NC_P', var_nc, writer=writer)\n",
    "        \n",
    "    var_nc += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "#### DTI_P\n",
    "\n",
    "var_dt = 0\n",
    "try:\n",
    "\n",
    "    df=clean_df(DBG['DTI_P']['DIS_IND_P']['DI_P_TABLE'])\n",
    "    \n",
    "    #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "    format_df(df, 'DTI_P', var_dt, writer=writer)\n",
    "        \n",
    "    var_dt += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['DTI_P']['CIR_INT_P']['ECI_P_TABLE'])\n",
    "    \n",
    "    #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "    format_df(df, 'DTI_P', var_dt, writer=writer)\n",
    "        \n",
    "    var_dt += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['DTI_P']['SOFT_P']['SF_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "    format_df(df, 'DTI_P', var_dt, writer=writer)\n",
    "        \n",
    "    var_dt += df.shape[0] + 3\n",
    "    \n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['DTI_P']['NUTRA_P']['NUTRA_P_TABLE'])\n",
    "    \n",
    "    #df.to_excel(writer,sheet_name='DTI_P',startrow = var_nc)\n",
    "\n",
    "    format_df(df, 'DTI_P', var_dt, writer=writer)\n",
    "        \n",
    "    var_dt += df.shape[0] + 3\n",
    "    \n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['DTI_P']['COL_CIENT_P']['COL_CIENT_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "    format_df(df, 'DTI_P', var_dt, writer=writer)\n",
    "        \n",
    "    var_dt += df.shape[0] + 3\n",
    "    \n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['DTI_P']['REG_CIENT_P']['REG_CIENT_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "    format_df(df, 'DTI_P', var_dt, writer=writer)\n",
    "        \n",
    "    var_dt += df.shape[0] + 3\n",
    "    \n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['DTI_P']['PLT_PIL_P']['PP_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "    format_df(df, 'DTI_P', var_dt, writer=writer)\n",
    "        \n",
    "    var_dt += df.shape[0] + 3\n",
    "    \n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['DTI_P']['PRT_IND_P']['PI_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "    format_df(df, 'DTI_P', var_dt, writer=writer)\n",
    "        \n",
    "    var_dt += df.shape[0] + 3\n",
    "    \n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['DTI_P']['SEC_IND_P']['SE_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "    format_df(df, 'DTI_P', var_dt, writer=writer)\n",
    "        \n",
    "    var_dt += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['DTI_P']['PROT_VIG_EPID_P']['PROT_VIG_EPID_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "    format_df(df, 'DTI_P', var_dt, writer=writer)\n",
    "        \n",
    "    var_dt += df.shape[0] + 3\n",
    "    \n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['DTI_P']['EMP_BSE_TEC_P']['EBT_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "    format_df(df, 'DTI_P', var_dt, writer=writer)\n",
    "        \n",
    "    var_dt += df.shape[0] + 3\n",
    "    \n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['DTI_P']['EMP_CRE_CUL_P']['ICC_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "    format_df(df, 'DTI_P', var_dt, writer=writer)\n",
    "        \n",
    "    var_dt += df.shape[0] + 3\n",
    "    \n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['DTI_P']['INN_GES_EMP_P']['IG_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "    format_df(df, 'DTI_P', var_dt, writer=writer)\n",
    "        \n",
    "    var_dt += df.shape[0] + 3\n",
    "    \n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['DTI_P']['INN_PROC_P']['IPP_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "    format_df(df, 'DTI_P', var_dt, writer=writer)\n",
    "        \n",
    "    var_dt += df.shape[0] + 3\n",
    "    \n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['DTI_P']['REG_NORM_REGL_LEG_P']['RNR_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "    format_df(df, 'DTI_P', var_dt, writer=writer)\n",
    "        \n",
    "    var_dt += df.shape[0] + 3\n",
    "    \n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['DTI_P']['CONP_TEC_P']['CONP_TEC_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "    format_df(df, 'DTI_P', var_dt, writer=writer)\n",
    "        \n",
    "    var_dt += df.shape[0] + 3\n",
    "    \n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['DTI_P']['REG_AAD_P']['AAAD_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "    format_df(df, 'DTI_P', var_dt, writer=writer)\n",
    "        \n",
    "    var_dt += df.shape[0] + 3\n",
    "    \n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['DTI_P']['SIG_DIS_P']['SD_P_TABLE'])\n",
    "    \n",
    "    #df.to_excel(writer,sheet_name='DTI_P',startrow = var_dt)\n",
    "\n",
    "    format_df(df, 'DTI_P', var_dt, writer=writer)\n",
    "        \n",
    "    var_dt += df.shape[0] + 3\n",
    "    \n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "######  ASC\n",
    "\n",
    "var_as = 0\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['ASC_P']['GEN_CONT_IMP_P']['GC_I_P_TABLE_5'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "    format_df(df, 'ASC_P', var_as, writer=writer)\n",
    "        \n",
    "    var_as += df.shape[0] + 3\n",
    "    \n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass                  \n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['ASC_P']['PASC_P']['PASC_FOR_P_TABLE'])\n",
    "    \n",
    "    #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "    format_df(df, 'ASC_P', var_as, writer=writer)\n",
    "        \n",
    "    var_as += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['ASC_P']['PASC_P']['PASC_TRA_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "    format_df(df, 'ASC_P', var_as, writer=writer)\n",
    "        \n",
    "    var_as += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['ASC_P']['PASC_P']['PASC_GEN_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "    format_df(df, 'ASC_P', var_as, writer=writer)\n",
    "        \n",
    "    var_as += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['ASC_P']['PASC_P']['PASC_CAD_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "    format_df(df, 'ASC_P', var_as, writer=writer)\n",
    "        \n",
    "    var_as += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['ASC_P']['DC_P']['DC_CD_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "    format_df(df, 'ASC_P', var_as, writer=writer)\n",
    "        \n",
    "    var_as += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['ASC_P']['DC_P']['DC_CON_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "    format_df(df, 'ASC_P', var_as, writer=writer)\n",
    "        \n",
    "    var_as += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['ASC_P']['DC_P']['DC_TRA_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "    format_df(df, 'ASC_P', var_as, writer=writer)\n",
    "        \n",
    "    var_as += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['ASC_P']['DC_P']['DC_TRA_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='ASC_P',startrow = var_as)\n",
    "\n",
    "    format_df(df, 'ASC_P', var_as, writer=writer)\n",
    "        \n",
    "    var_as += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "# FRH\n",
    "\n",
    "var_rh = 0\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['FRH_P']['TES_DOC_P']['TD_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='FRH_P',startrow = var_rh)\n",
    "\n",
    "    format_df(df, 'FRH_P', var_rh, writer=writer)\n",
    "        \n",
    "    var_rh += df.shape[0] + 3\n",
    "\n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['FRH_P']['TES_MAST_P']['TM_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='FRH_P',startrow = var_rh)\n",
    "\n",
    "    format_df(df, 'FRH_P', var_rh, writer=writer)\n",
    "        \n",
    "    var_rh += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['FRH_P']['TES_PREG_P']['TP_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='FRH_P',startrow = var_rh)\n",
    "\n",
    "    format_df(df, 'FRH_P', var_rh, writer=writer)\n",
    "        \n",
    "    var_rh += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['FRH_P']['ASE_PRG_ACA_P']['APGA_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='FRH_P',startrow = var_rh)\n",
    "\n",
    "    format_df(df, 'FRH_P', var_rh, writer=writer)\n",
    "        \n",
    "    var_rh += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['FRH_P']['ASE_CRE_CUR_P']['ACC_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='FRH_P',startrow = var_rh)\n",
    "\n",
    "    format_df(df, 'FRH_P', var_rh, writer=writer)\n",
    "        \n",
    "    var_rh += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df=clean_df(DBG['FRH_P']['ASE_PRG_ONDAS_P']['APO_P_TABLE'])\n",
    "\n",
    "    #df.to_excel(writer,sheet_name='FRH_P',startrow = var_rh)\n",
    "\n",
    "    format_df(df, 'FRH_P', var_rh, writer=writer)\n",
    "        \n",
    "    var_rh += df.shape[0] + 3\n",
    "    \n",
    "except KeyError as e:\n",
    "    \n",
    "    pass\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "embedded-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "schema for store tables\n",
    "\n",
    "DBG = {\n",
    "    \n",
    "    'INFO_GROUP': 'TABLE',\n",
    "    'MEMBERS':'TABLE',\n",
    "       \n",
    "    'NC_P': {'ART_IMP_P': {'ART_P_TABLE':'TABLE'},\n",
    "             'ART_ELE_P': {'ART_E_P_TABLE':'TABLE'},\n",
    "             'LIB_P':     {'LIB_P_TABLE':'TABLE'},\n",
    "             'CAP_LIB_P': {'CAP_LIB_P_TABLE':'TABLE'},\n",
    "             'NOT_CIE_P': {'NOT_CIE_P_TABLE':'TABLE'},\n",
    "             'PAT_P':     {'PAT_P_TABLE':'TABLE'},\n",
    "             'PRD_INV_ART_P': {'PAAD_P_TABLE':'TABLE'},\n",
    "             'VAR_VEG_P':     {'VV_P_TABLE':'TABLE'},\n",
    "             'VAR_ANI_P':     {'VA_P_TABLE':'TABLE'},\n",
    "             'RAZ_PEC_P':     {'RAZ_PEC_P_TABLE':'TABLE'},\n",
    "             'TRA_FIL_P': {'TRA_FIL_P_TABLE':'TABLE'}\n",
    "            },\n",
    "     'DTI_P': {'DIS_IND_P': {'DI_P_TABLE':'TABLE'},\n",
    "              'CIR_INT_P': {'ECI_P_TABLE':'TABLE'},\n",
    "              'SOFT_P': {'SF_P_TABLE':'TABLE'},\n",
    "              'NUTRA_P': {'NUTRA_P_TABLE':'TABLE'},\n",
    "              'COL_CIENT_P': {'COL_CIENT_P_TABLE':'TABLE'},\n",
    "              'REG_CIENT_P': {'REG_CIENT_P_TABLE':'TABLE'},\n",
    "              'PLT_PIL_P': {'PP_P_TABLE':'TABLE'},\n",
    "              'PRT_IND_P': {'PI_P_TABLE':'TABLE'},\n",
    "              'SEC_IND_P': {'SE_P_TABLE':'TABLE'},\n",
    "              'PROT_VIG_EPID_P': {'PROT_VIG_EPID_P_TABLE':'TABLE'},\n",
    "              'EMP_BSE_TEC_P': {'EBT_P_TABLE':'TABLE'},\n",
    "              'EMP_CRE_CUL_P': {'ICC_P_TABLE':'TABLE'},\n",
    "              'INN_GES_EMP_P': {'IG_P_TABLE':'TABLE'},\n",
    "              'INN_PROC_P': {'IPP_P_TABLE':'TABLE'},\n",
    "              'REG_NORM_REGL_LEG_P': {'RNR_P_TABLE':'TABLE'},\n",
    "              'CONP_TEC_P': {'CONP_TEC_P_TABLE':'TABLE'},\n",
    "              'REG_AAD_P': {'AAAD_P_TABLE':'TABLE'},\n",
    "              'SIG_DIS_P': {'SD_P_TABLE':'TABLE'}\n",
    "              },\n",
    "    'ASC_P': {'GEN_CONT_IMP_P': {'GC_I_P_TABLE_5':'TABLE'},\n",
    "              'PASC_P': {'PASC_FOR_P_TABLE':'TABLE',\n",
    "               'PASC_TRA_P_TABLE':'TABLE',\n",
    "               'PASC_GEN_P_TABLE':'TABLE',\n",
    "               'PASC_CAD_P_TABLE':'TABLE'},\n",
    "              'DC_P': {'DC_CD_P_TABLE':'TABLE',\n",
    "               'DC_CON_P_TABLE':'TABLE',\n",
    "               'DC_TRA_P_TABLE':'TABLE',\n",
    "               'DC_TRA_P_TABLE':'TABLE'}},\n",
    "    'FRH_P': {'TES_DOC_P': {'TD_P_TABLE':'TABLE'},\n",
    "              'TES_MAST_P': {'TM_P_TABLE':'TABLE'},\n",
    "              'TES_PREG_P': {'TP_P_TABLE':'TABLE'},\n",
    "              'PROY_INV_DES_P': {'PID_P_TABLE':'TABLE'},\n",
    "              'PROY_INV_CRE_P': {'INV_CRE_P_TABLE':'TABLE'},\n",
    "              'PROY_INV_DES_INN_P': {'PF_P_TABLE':'TABLE'},\n",
    "              'PROY_INV_RESP_SOC_P': {'PE_P_TABLE':'TABLE'},\n",
    "              'ASE_PRG_ACA_P': {'APGA_P_TABLE':'TABLE'},\n",
    "              'ASE_CRE_CUR_P': {'ACC_P_TABLE':'TABLE'},\n",
    "              'ASE_PRG_ONDAS_P': {'APO_P_TABLE':'TABLE'}}\n",
    "}'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-colors",
   "metadata": {},
   "source": [
    "# ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing multiple dataframes to worksheets using pandas an\n",
    "# XlsxWriter\n",
    "# create dataFrames\n",
    "\n",
    "# create a pandas excel writer using Xlsxwriter as the engine\n",
    "writer = pd.ExcelWriter('name_excel.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# save in ram\n",
    "df1.to_excel(writer, sheet_name='NAME')\n",
    "df2.to_excel(writer, sheet_name='NAME')\n",
    "df3.to_excel(writer,sheet_name='NAME')\n",
    "\n",
    "# save in disk\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "elementary-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_excel('plantillas.xlsx')\n",
    "\n",
    "df1 = clean_df(datos['NC_P']['ART_IMP_P']['ART_P_TABLE'])\n",
    "\n",
    "df2 = clean_df(datos['NC_P']['ART_ELE_P']['ART_E_P_TABLE'])\n",
    "\n",
    "df3 = clean_df(datos['NC_P']['LIB_P']['LIB_P_TABLE'])\n",
    "\n",
    "df4 = clean_df(datos['NC_P']['CAP_LIB_P']['CAP_LIB_P_TABLE'])\n",
    "\n",
    "writer = pd.ExcelWriter('teest.xlsx', engine = 'xlsxwriter')\n",
    "\n",
    "workbook = writer.book\n",
    "\n",
    "# PPT\n",
    "df0 = pd.read_excel('plantillas.xlsx')\n",
    "df0.to_excel(writer,sheet_name = 'Pt', startrow=0,startcol=0)\n",
    "\n",
    "# INFO\n",
    "#df00.to_excel(writer,sheet_name = 'Info', startrow=0,startcol=0)\n",
    "#format_df(df00,'Info', 0, writer=writer)\n",
    "\n",
    "# MEMBERS\n",
    "df000.to_excel(writer,sheet_name='Members', startrow=0,startcol=0)\n",
    "format_df(df000, 'Members', 0, writer=writer)\n",
    "\n",
    "# SHETT NC\n",
    "rows = 0\n",
    "\n",
    "df1.to_excel(writer,sheet_name='NC', startrow=rows,startcol=0)\n",
    "\n",
    "format_df(df1, 'NC', rows, writer=writer)\n",
    "\n",
    "rows += df1.shape[0] + 2\n",
    "\n",
    "df2.to_excel(writer,sheet_name='NC', startrow=rows,startcol=0)\n",
    "\n",
    "format_df(df2, 'NC', rows, writer=writer)\n",
    "\n",
    "rows += df2.shape[0] + 2\n",
    "\n",
    "df3.to_excel(writer,sheet_name='NC', startrow=rows,startcol=0)\n",
    "\n",
    "format_df(df3, 'NC', rows, writer=writer)\n",
    "\n",
    "rows += df3.shape[0] + 2\n",
    "\n",
    "df4.to_excel(writer,sheet_name='NC', startrow=rows,startcol=0)\n",
    "\n",
    "format_df(df3, 'NC', rows, writer=writer)\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'AA_Plantilla_Formato de verificación de información_GrupLAC_894-2021_%s'\n",
    "\n",
    "# Positioning dataframes in a worksheet using pandas Xlswriter\n",
    "import pandas as pd\n",
    "\n",
    "# create dataframes\n",
    "# ... df1, df2, df3, df4\n",
    "\n",
    "# create a pandas excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('pandas_pos.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# position a pandas Excel writer using XlsWriter as the engine\n",
    "# var_pos = 0; must be update with val of rows for last df \n",
    "df1.to_excel(writer,sheet_name='sheet_name')\n",
    "var_pos+=df1.shape[0]\n",
    "df2.to_excel(writer,sheet_name='sheet_name_1'start_row=var_pos)\n",
    "var_pos+=df2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "structured-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas dataframe from the data.\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('thingsprogramatically.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Convert the dataframe to an XlsxWriter Excel object.\n",
    "df.to_excel(writer, sheet_name='PT')\n",
    "\n",
    "# Get the xlsxwriter objects from the dataframe writer object.\n",
    "workbook  = writer.book\n",
    "worksheet = writer.sheets['PT']\n",
    "\n",
    "#insert image\n",
    "worksheet.insert_image(0, 0, './indice.png',{'x_scale': 0.6, 'y_scale': 0.6})\n",
    "\n",
    "# cell format C1\n",
    "cell_format = workbook.add_format({'font_name': 'Tahoma',\n",
    "                                   'font_size':28,\n",
    "                                   'align':'center',\n",
    "                                   'text_wrap': True\n",
    "                                  })\n",
    "# cell weigh C1\n",
    "worksheet.set_column('C1:C1',100)\n",
    "\n",
    "worksheet.write('C1','UNIVERSIDAD DE ANTIOQUIA',cell_format)\n",
    "\n",
    "worksheet.write('C3','VICERRECTORÍA DE INVESTIGACIÓN',cell_format)\n",
    "\n",
    "worksheet.write('C6','VERIFICACIÓN DE INFORMACIÓN PARA OTORGAR AVAL A LOS GRUPOS DE INVESTIGACIÓN  E INVESTIGADORES PARA SU PARTICIPACIÓN EN LA CONVOCATORIA 894 DE 2021 DE MINCIENCIAS',cell_format)\n",
    "\n",
    "worksheet.write('C9','PRESENTACIÓN DEL EJERCICIO',cell_format)\n",
    "\n",
    "cell_format_c11 = workbook.add_format({'font_name': 'Tahoma',\n",
    "                                   'font_size':11,\n",
    "                                   'align':'center',\n",
    "                                   'text_wrap': True\n",
    "                                  })\n",
    "\n",
    "worksheet.write('C11','''Los grupos de investigación e investigadores de la Universidad de Antioquia que deseen participar en la Convocatoria Nacional para el reconocimiento y medición de grupos de investigación, desarrollo tecnológico o de innovación y para el reconocimiento de investigadores del Sistema Nacional de Ciencia, Tecnología e Innovación - SNCTI, 894 de 2021, deben presentar la información actualizada en las plataformas CvLAC y GrupLAC validada por el Centro de Investigación en el presente formato, y respaldada en el repositorio digital de evidencias dispuesto para este fin, para la obtención del aval institucional por parte de la Vicerrectoría de Investigación. \n",
    "\n",
    "La información a validar corresponde a los años 2019-2020 y aquella que entra en la ventana de observación y debe ser modificada según el Modelo de medición de grupos. La validación comprende:\n",
    "\n",
    "1. Verificación de la vinculación de los integrantes a la Universidad de Antioquia y al grupo de investigación.  Diligenciar los campos solicitados. \n",
    "\n",
    "2. Verificación de la producción de GNC, DTeI, ASC y FRH, en los campos habilitados en cada hoja de este formato. Las evidencias requeridas para los productos deben ser anexadas al repositorio digital asignado al grupo y se deben enlazar a cada producto.  \n",
    "\n",
    "Este documento debe ser diligenciado en línea.\n",
    "\n",
    "De antemano, la Vicerrectoría de Investigación agradece su participación en este ejercicio, que resulta de vital importancia para llevar a buen término la Convocatoria de Reconocimiento y Medición de Grupos de Investigación\n",
    "''',cell_format_c11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "desirable-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "communist-grade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ONE GROUP IMPLEMENTATION\n",
    "idxx = 13\n",
    "\n",
    "# DATA\n",
    "DBG = LD[0]\n",
    "\n",
    "### excel name\n",
    "name = 'AA_Plantilla_Formato de verificación de información_GrupLAC_894-2021_%s'\n",
    "\n",
    "#get objects\n",
    "writer = pd.ExcelWriter(name % dfg.loc[idxx,'COL Grupo']+'.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Create a Pandas dataframe from the PT sheet.\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Convert the dataframe to an XlsxWriter Excel object.\n",
    "df.to_excel(writer, sheet_name='PT')\n",
    "\n",
    "# Get the xlsxwriter objects from the dataframe writer object.\n",
    "workbook  = writer.book\n",
    "worksheet = writer.sheets['PT']\n",
    "\n",
    "#insert image\n",
    "worksheet.insert_image(0, 0, './indice.png',{'x_scale': 0.6, 'y_scale': 0.6})\n",
    "\n",
    "# cell format C1\n",
    "cell_format = workbook.add_format({'font_name': 'Tahoma',\n",
    "                                   'font_size':28,\n",
    "                                   'align':'center',\n",
    "                                   'text_wrap': True\n",
    "                                  })\n",
    "# cell weigh C1\n",
    "worksheet.set_column('C1:C1',100)\n",
    "\n",
    "# insert text with format \n",
    "worksheet.write('C1','UNIVERSIDAD DE ANTIOQUIA',cell_format)\n",
    "\n",
    "worksheet.write('C3','VICERRECTORÍA DE INVESTIGACIÓN',cell_format)\n",
    "\n",
    "worksheet.write('C6','VERIFICACIÓN DE INFORMACIÓN PARA OTORGAR AVAL A LOS GRUPOS DE INVESTIGACIÓN  E INVESTIGADORES PARA SU PARTICIPACIÓN EN LA CONVOCATORIA 894 DE 2021 DE MINCIENCIAS',cell_format)\n",
    "\n",
    "worksheet.write('C9','PRESENTACIÓN DEL EJERCICIO',cell_format)\n",
    "\n",
    "# create format for cell C11\n",
    "cell_format_c11 = workbook.add_format({'font_name': 'Tahoma',\n",
    "                                   'font_size':11,\n",
    "                                   'align':'center',\n",
    "                                   'text_wrap': True\n",
    "                                  })\n",
    "# insert text C11 with format\n",
    "worksheet.write('C11','''Los grupos de investigación e investigadores de la Universidad de Antioquia que deseen participar en la Convocatoria Nacional para el reconocimiento y medición de grupos de investigación, desarrollo tecnológico o de innovación y para el reconocimiento de investigadores del Sistema Nacional de Ciencia, Tecnología e Innovación - SNCTI, 894 de 2021, deben presentar la información actualizada en las plataformas CvLAC y GrupLAC validada por el Centro de Investigación en el presente formato, y respaldada en el repositorio digital de evidencias dispuesto para este fin, para la obtención del aval institucional por parte de la Vicerrectoría de Investigación. \n",
    "\n",
    "La información a validar corresponde a los años 2019-2020 y aquella que entra en la ventana de observación y debe ser modificada según el Modelo de medición de grupos. La validación comprende:\n",
    "\n",
    "1. Verificación de la vinculación de los integrantes a la Universidad de Antioquia y al grupo de investigación.  Diligenciar los campos solicitados. \n",
    "\n",
    "2. Verificación de la producción de GNC, DTeI, ASC y FRH, en los campos habilitados en cada hoja de este formato. Las evidencias requeridas para los productos deben ser anexadas al repositorio digital asignado al grupo y se deben enlazar a cada producto.  \n",
    "\n",
    "Este documento debe ser diligenciado en línea.\n",
    "\n",
    "De antemano, la Vicerrectoría de Investigación agradece su participación en este ejercicio, que resulta de vital importancia para llevar a buen término la Convocatoria de Reconocimiento y Medición de Grupos de Investigación\n",
    "''',cell_format_c11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "scheduled-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get objects\n",
    "writer = pd.ExcelWriter('setcols.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Create a Pandas dataframe from the PT sheet.\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Convert the dataframe to an XlsxWriter Excel object.\n",
    "df.to_excel(writer, sheet_name='PT')\n",
    "\n",
    "# Get the xlsxwriter objects from the dataframe writer object.\n",
    "workbook  = writer.book\n",
    "\n",
    "general=workbook.add_format({'text_wrap':True})\n",
    "\n",
    "\n",
    "worksheet = writer.sheets['PT']\n",
    "\n",
    "worksheet.set_column('C:G', 40,general)\n",
    "\n",
    "worksheet.write('C1','HOLA MUNDO HOLA MUNDO HOLA MUNDO',general)\n",
    "worksheet.write('D1','HOLA MUNDO HOLA MUNDO HOLA MUNDO',general)\n",
    "worksheet.write('E1','HOLA MUNDO HOLA MUNDO HOLA MUNDO',general)\n",
    "worksheet.write('F1','HOLA MUNDO HOLA MUNDO HOLA MUNDO',general)\n",
    "worksheet.write('G1','HOLA MUNDO HOLA MUNDO HOLA MUNDO',general)\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-casino",
   "metadata": {},
   "outputs": [],
   "source": [
    "# workbook structure: options\n",
    "    # each worksheet is a categorie\n",
    "        # store group in dict; store each table in dict structure \n",
    "        '''{\n",
    "        \n",
    "            info: table,\n",
    "            members: table,\n",
    "            products:{\n",
    "                        NC:[tables],\n",
    "                        DTI:[tables],\n",
    "                        ASC:[tables],\n",
    "                        FRH:[tables]\n",
    "                        \n",
    "            }\n",
    "        }'''\n",
    "    # each worksheet is a table product or tables of products"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
